{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682cc4d4-ebb1-4e48-b84e-1e62006da233",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61331a7-f1dd-442e-b58a-a5adb337fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ec5c8-c16a-4697-86b5-e22292d3266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:56:37,282] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_symbind_alt@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__nptl_change_stack_perm@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_find_dso_for_object@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_fatal_printf@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_exception_create@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__tunable_get_val@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_preinit@GLIBC_PRIVATE'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# DeepSpeed ZeRO-3\n",
    "import deepspeed\n",
    "from deepspeed.accelerator import get_accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd5b450-0317-4c55-847e-7723042940f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28e8464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free(GB): 3.87164306640625, Global(GB): 23.64971923828125, Free(%): 0.16370778136508748\n"
     ]
    }
   ],
   "source": [
    "(free_memory, global_memory) = torch.cuda.mem_get_info()\n",
    "print(f\"Free(GB): {free_memory/1024/1024/1024}, Global(GB): {global_memory/1024/1024/1024}, Free(%): {free_memory/global_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c439d2-2280-4823-90dd-0867012b907c",
   "metadata": {},
   "source": [
    "# Load Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a715f751-ab65-45a3-ae17-c795b8c78d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Identity</th>\n",
       "      <th>Text</th>\n",
       "      <th>A2-Unambiguous</th>\n",
       "      <th>A4-Tolerances</th>\n",
       "      <th>A5-Sources specified</th>\n",
       "      <th>E1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSS_CONNECTIVITY</td>\n",
       "      <td>SRD_GSS_FUNC_61</td>\n",
       "      <td>The User and Rights Administration HMI shall p...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR2146</td>\n",
       "      <td>The Network Function shall support WiFi 802.11...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR797</td>\n",
       "      <td>The PwrCon software shall monitor the output v...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR3013</td>\n",
       "      <td>When prompted, the TETRA Software shall place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR3198</td>\n",
       "      <td>The TETRA software shall allow users to select...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type         Identity  \\\n",
       "0  GSS_CONNECTIVITY  SRD_GSS_FUNC_61   \n",
       "1        Cobham_ATR     SHLR-ATR2146   \n",
       "2        Cobham_ATR      SHLR-ATR797   \n",
       "3        Cobham_ATR     SHLR-ATR3013   \n",
       "4        Cobham_ATR     SHLR-ATR3198   \n",
       "\n",
       "                                                Text A2-Unambiguous  \\\n",
       "0  The User and Rights Administration HMI shall p...              1   \n",
       "1  The Network Function shall support WiFi 802.11...              1   \n",
       "2  The PwrCon software shall monitor the output v...              1   \n",
       "3  When prompted, the TETRA Software shall place ...              1   \n",
       "4  The TETRA software shall allow users to select...              1   \n",
       "\n",
       "  A4-Tolerances A5-Sources specified  E1  \n",
       "0            na                   na   1  \n",
       "1            na                   na   1  \n",
       "2            na                   na   1  \n",
       "3            na                   na   1  \n",
       "4            na                   na   1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "master_df = pd.read_excel('./DATASETS/Training_Dataset.xlsx')\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bdf637-9e84-4cee-a95c-e5c7bd8075d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The User and Rights Administration HMI shall p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Network Function shall support WiFi 802.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The PwrCon software shall monitor the output v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>When prompted, the TETRA Software shall place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The TETRA software shall allow users to select...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   E1                                               Text\n",
       "0   1  The User and Rights Administration HMI shall p...\n",
       "1   1  The Network Function shall support WiFi 802.11...\n",
       "2   1  The PwrCon software shall monitor the output v...\n",
       "3   1  When prompted, the TETRA Software shall place ...\n",
       "4   1  The TETRA software shall allow users to select..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = master_df[['E1','Text']].copy()\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9296d035-a173-4c8e-81d9-a30a5021dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3255 entries, 0 to 3254\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   E1      3255 non-null   int64 \n",
      " 1   Text    3255 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 51.0+ KB\n"
     ]
    }
   ],
   "source": [
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c738bf70-5be7-4b1e-9d88-1e09f2f497a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2127"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df_label1 = model_df.query('E1 == 1')\n",
    "len(model_df_label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a967f2ce-ce0f-4aa7-8b6e-759011eef775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df_label0 = model_df.query('E1 == 0')\n",
    "len(model_df_label0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fceb9f5-cd13-4779-b8db-57ff9cda386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat([model_df_label1[:1500],model_df_label0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7716b-cbb9-40df-8796-9e2718512f6b",
   "metadata": {},
   "source": [
    "# Data process and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e0084e-559e-46cc-b1a4-892e5e0fcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "train_df, test_df = train_test_split(model_df, test_size=0.1, shuffle=True)\n",
    "\n",
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "test_iter = iter(list(test_df.itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6daca09-ad60-40b6-90f2-e91c79aabb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2365"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6424905-7a3d-4fd5-a22d-f2be29be1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\",\n",
    "    \"tokenizer\",\n",
    "    \"mistralai/Mistral-7B-v0.1\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc4e1ac2-7bff-4984-8bf5-bde7daceef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d1986-12ed-45c3-8a18-bd1ada1e9440",
   "metadata": {},
   "source": [
    "# Dataset iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "880ea8d4-bae1-48bc-996e-d352db324265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "test_iter = iter(list(test_df.itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c4e14f-9c36-4343-ba96-f05c6eb3196e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'The Certificates tab shall display:\\n•\\tThe Expiration date of any certificates loaded onto the CG200P-R')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092203c5-49af-4337-abbd-38851caa6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Get label and text\n",
    "    y, x = list(zip(*batch))\n",
    "\n",
    "    # Create list with indices from tokeniser\n",
    "\n",
    "    encoded_x = tokenizer(x, padding=True, truncation=True)\n",
    "    encoded_x.input_ids = torch.tensor(encoded_x.input_ids).to(device)\n",
    "    encoded_x.attention_mask = torch.tensor(encoded_x.attention_mask).to(device)  \n",
    "    \n",
    "    # Prepare the labels, by subtracting 1 to get them in the range 0-3\n",
    "    return encoded_x, torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a81c8d9-de06-4aed-9ff3-0c5f31985d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'The Certificates tab shall display:\\n•\\tThe Expiration date of any certificates loaded onto the CG200P-R')\n",
      "(1, 'The GAMF Import File HMI shall require that the following metadata is added before accepting the file:\\n-Target Aircraft\\n-File Type\\n-Target Application')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_ids': [[2, 2, 2, 2, 2, 1, 415, 12089, 23575, 7683, 4579, 4249, 28747, 13, 28899, 12, 1014, 16790, 8679, 3608, 302, 707, 5813, 23575, 10773, 5380, 272, 334, 28777, 28750, 28734, 28734, 28753, 28733, 28754], [1, 415, 420, 2854, 28765, 15747, 6184, 382, 5877, 4579, 2699, 369, 272, 2296, 11214, 349, 3886, 1159, 22368, 272, 1729, 28747, 13, 28733, 5332, 5085, 6006, 13, 28733, 2171, 5707, 13, 28733, 5332, 12482]], 'attention_mask': [[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       " tensor([1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "first = next(train_iter)\n",
    "second = next(train_iter)\n",
    "\n",
    "print(first)\n",
    "print(second)\n",
    "\n",
    "collate_batch([first, second])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf18db8-7d58-43c0-ac22-d3e225cb2ed1",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d306ecb-d8ef-4389-ab6c-8fb803124ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212840b389ad4675a4ae8e233f30e01d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\",\n",
    "    \"modelForSequenceClassification\",\n",
    "    \"mistralai/Mistral-7B-v0.1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9fd9daab-93a4-44fb-8193-94c8039002e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForSequenceClassification(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e6724c0-3768-48ee-9176-2518cabbfe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7e98c25-1d44-4a02-997c-8ec0c4e123c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parameter in enumerate(model.parameters()):\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41954657-e0cf-40d1-9143-75e11701c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.model.layers = model.model.layers[:2]\n",
    "model.score = nn.Sequential(\n",
    "    nn.Linear(in_features=4096, out_features=4096),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=4096, out_features=2),\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8afdced-aa5f-4dae-9ef0-7183668838d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForSequenceClassification(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (score): Sequential(\n",
       "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1421e4-51bc-490c-b47d-01d83e0a28cf",
   "metadata": {},
   "source": [
    "# Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a28ea92-68d0-44a7-9a5d-313ee209533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import time\n",
    "\n",
    "def train(model, dataloader, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 5\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    for idx, (data, label) in enumerate(dataloader):         \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "        predicted_label = outputs.logits\n",
    "        loss = criterion(predicted_label, label)\n",
    "        \n",
    "        # Deepspeed model engine, backward pass \n",
    "        model.backward(loss)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Deepspeed model engine, optimizer step\n",
    "        model.step()\n",
    "        \n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    \n",
    "        # Deepspeed model engine, empty cache\n",
    "        model.empty_partition_cache()\n",
    "         \n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "        \n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, label) in enumerate(dataloader):      \n",
    "            outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "            predicted_label = outputs.logits\n",
    "            loss = criterion(predicted_label, label)\n",
    "            \n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count, loss.item() / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b954fb1a",
   "metadata": {},
   "source": [
    "# Deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "46024bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 13:51:41,763] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-09 13:51:41,763] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-09 13:51:41,763] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-09 13:51:42,610] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.1.1.204, master_port=29500\n",
      "[2024-05-09 13:51:42,610] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 13:51:47,275] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-09 13:51:48,374] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/it/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/it/.cache/torch_extensions/py311_cu121/cpu_adam/build.ninja...\n",
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.208834171295166 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 13:51:50,984] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-05-09 13:51:50,984] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-09 13:51:50,989] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-05-09 13:51:50,989] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-05-09 13:51:50,989] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-05-09 13:51:50,989] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-05-09 13:51:51,089] [INFO] [utils.py:779:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-05-09 13:51:51,090] [INFO] [utils.py:780:see_memory_usage] MA 13.78 GB         Max_MA 13.78 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:51,090] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 4.86 GB, percent = 15.9%\n",
      "[2024-05-09 13:51:51,091] [INFO] [stage3.py:130:__init__] Reduce bucket size 200000000\n",
      "[2024-05-09 13:51:51,091] [INFO] [stage3.py:131:__init__] Prefetch bucket size 0\n",
      "[2024-05-09 13:51:51,168] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-05-09 13:51:51,168] [INFO] [utils.py:780:see_memory_usage] MA 13.78 GB         Max_MA 13.78 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:51,169] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 4.86 GB, percent = 15.9%\n",
      "Parameter Offload: Total persistent parameters: 278530 in 68 params\n",
      "[2024-05-09 13:51:55,797] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-05-09 13:51:55,798] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 13.78 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:55,811] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.67 GB, percent = 57.8%\n",
      "[2024-05-09 13:51:55,912] [INFO] [utils.py:779:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-05-09 13:51:55,912] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:55,913] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.67 GB, percent = 57.8%\n",
      "[2024-05-09 13:51:56,044] [INFO] [utils.py:779:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2024-05-09 13:51:56,045] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:56,045] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.7 GB, percent = 57.9%\n",
      "[2024-05-09 13:51:56,127] [INFO] [utils.py:779:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-05-09 13:51:56,127] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:56,127] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.7 GB, percent = 57.9%\n",
      "[2024-05-09 13:51:56,226] [INFO] [utils.py:779:see_memory_usage] After creating fp32 partitions\n",
      "[2024-05-09 13:51:56,226] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:56,227] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.75 GB, percent = 58.1%\n",
      "[2024-05-09 13:51:56,307] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-09 13:51:56,307] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:56,308] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.75 GB, percent = 58.1%\n",
      "[2024-05-09 13:51:56,416] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-09 13:51:56,416] [INFO] [utils.py:780:see_memory_usage] MA 0.5 GB         Max_MA 0.5 GB         CA 14.03 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:56,417] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.82 GB, percent = 58.3%\n",
      "[2024-05-09 13:51:56,417] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-05-09 13:51:56,511] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-09 13:51:56,512] [INFO] [utils.py:780:see_memory_usage] MA 0.87 GB         Max_MA 0.94 GB         CA 14.4 GB         Max_CA 14 GB \n",
      "[2024-05-09 13:51:56,512] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 17.84 GB, percent = 58.4%\n",
      "[2024-05-09 13:51:56,512] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-05-09 13:51:56,513] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2024-05-09 13:51:56,513] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x77c016f0bc10>\n",
      "[2024-05-09 13:51:56,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:51:56,513] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-09 13:51:56,514] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-09 13:51:56,514] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-09 13:51:56,514] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-09 13:51:56,514] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-09 13:51:56,514] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-09 13:51:56,514] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x77c02e4dfe10>\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-09 13:51:56,515] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 13:51:56,516] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-09 13:51:56,516] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-09 13:51:56,516] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-09 13:51:56,516] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-09 13:51:56,516] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-09 13:51:56,516] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-09 13:51:56,516] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-09 13:51:56,517] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-09 13:51:56,517] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-09 13:51:56,517] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-09 13:51:56,517] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-09 13:51:56,517] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-09 13:51:56,517] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-09 13:51:56,517] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   fp16_enabled ................. True\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-09 13:51:56,518] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   loss_scale ................... 0\n",
      "[2024-05-09 13:51:56,519] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-09 13:51:56,520] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-09 13:51:56,520] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-09 13:51:56,520] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-09 13:51:56,520] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-09 13:51:56,520] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-09 13:51:56,520] [INFO] [config.py:1000:print]   optimizer_name ............... adam\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupLR\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n",
      "[2024-05-09 13:51:56,521] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   steps_per_print .............. 10\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   train_batch_size ............. 16\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-09 13:51:56,522] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-09 13:51:56,523] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-09 13:51:56,523] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-09 13:51:56,523] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False\n",
      "[2024-05-09 13:51:56,523] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=0 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-09 13:51:56,523] [INFO] [config.py:1000:print]   zero_enabled ................. True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 13:51:56,523] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-09 13:51:56,523] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\n",
      "[2024-05-09 13:51:56,524] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.001, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"load_from_fp32_weights\": true, \n",
      "        \"gather_16bit_weights_on_model_save\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"stage3_prefetch_bucket_size\": 0\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepSpeedEngine(\n",
       "  (module): MistralForSequenceClassification(\n",
       "    (model): MistralModel(\n",
       "      (embed_tokens): Embedding(32000, 4096)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x MistralDecoderLayer(\n",
       "          (self_attn): MistralAttention(\n",
       "            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "            (rotary_emb): MistralRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): MistralMLP(\n",
       "            (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "            (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): MistralRMSNorm()\n",
       "          (post_attention_layernorm): MistralRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): MistralRMSNorm()\n",
       "    )\n",
       "    (score): Sequential(\n",
       "      (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=4096, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepspeed_config = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "            \"lr\": 0.001,\n",
    "            \"betas\": [\n",
    "                0.8,\n",
    "                0.999\n",
    "            ],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 3e-7,\n",
    "        },\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": 0,\n",
    "            \"warmup_max_lr\": 0.001,\n",
    "            \"warmup_num_steps\": 1000,\n",
    "        },\n",
    "    },\n",
    "    \"train_batch_size\": 16,\n",
    "    # \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    # \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"fp16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"load_from_fp32_weights\": True,\n",
    "        \"gather_16bit_weights_on_model_save\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"stage3_prefetch_bucket_size\": 0,\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"train_batch_size\": 16,\n",
    "}\n",
    "\n",
    "# Initialize DeepSpeed Engine\n",
    "model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    config=deepspeed_config,\n",
    ")\n",
    "model_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afe31842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = get_accelerator().device_name(model_engine.local_rank)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0ddbc-f1e1-4662-a3d2-c09beafd25ed",
   "metadata": {},
   "source": [
    "# Split the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab8adb52-47f3-4740-952f-639d21a70d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "BATCH_SIZE = 16  # batch size for training\n",
    "\n",
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.8)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2abb8af-8fe2-4382-8adf-7d2af9cd8e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today = date.today().isoformat()\n",
    "checkpoint_path = \"./models/mistral\"\n",
    "model_name = \"mistral\"\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20  # epoch\n",
    "# LR = 0.1 # learning rate\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "    \n",
    "best_accu_val = 0.9\n",
    "def train_with_hist(model, checkpoint_path):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_accu = None\n",
    "\n",
    "    loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid = [], [], [], []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        accu_train, loss_train = train(model, train_dataloader, epoch)\n",
    "        accu_val, loss_val = evaluate(model, valid_dataloader)\n",
    "       \n",
    "        print({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss_train\": loss_train,\n",
    "            \"loss_val\": loss_val,\n",
    "            \"accuracy_train\": accu_train,\n",
    "            \"accuracy_val\": accu_val,\n",
    "        })\n",
    "\n",
    "        loss_hist_train.append(loss_train)\n",
    "        loss_hist_valid.append(loss_val)\n",
    "        accuracy_hist_train.append(accu_train)\n",
    "        accuracy_hist_valid.append(accu_val)\n",
    "        \n",
    "        get_accelerator().empty_cache()\n",
    "        \n",
    "        if accu_val > best_accu_val:\n",
    "            best_accu_val = accu_val\n",
    "            torch.save(model, f\"./models/mistral/{today}_mistral_checkpoint.pt\")\n",
    "        \n",
    "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00c4e1f4-9821-42a5-929f-e297c8cd9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e7023b-7888-4afe-906a-76ac56aacab8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.001000, betas=(0.800000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-05-09 13:51:59,561] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2024-05-09 13:52:00,676] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2024-05-09 13:52:01,751] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2024-05-09 13:52:02,843] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2024-05-09 13:52:04,008] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2024-05-09 13:52:05,022] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "[2024-05-09 13:52:06,288] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2024-05-09 13:52:07,437] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2024-05-09 13:52:08,458] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2024-05-09 13:52:09,560] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2024-05-09 13:52:09,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:52:09,561] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=16.17765735733518, CurrSamplesPerSec=16.53964060238731, MemAllocated=1.09GB, MaxMemAllocated=1.85GB\n",
      "[2024-05-09 13:52:10,642] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "[2024-05-09 13:52:11,659] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2024-05-09 13:52:13,188] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2024-05-09 13:52:14,523] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2024-05-09 13:52:15,538] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2024-05-09 13:52:16,670] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "[2024-05-09 13:52:18,052] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2024-05-09 13:52:19,065] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2024-05-09 13:52:21,277] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "[2024-05-09 13:52:21,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=19, lr=[0.0], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:52:21,278] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=15.62338921408384, CurrSamplesPerSec=17.041075756748555, MemAllocated=1.07GB, MaxMemAllocated=2.45GB\n",
      "[2024-05-09 13:52:28,062] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "[2024-05-09 13:52:32,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=20, lr=[0.0003333333333333334], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:52:32,334] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=15.833202575836735, CurrSamplesPerSec=17.669959796277265, MemAllocated=1.03GB, MaxMemAllocated=2.45GB\n",
      "[2024-05-09 13:52:42,539] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "[2024-05-09 13:52:43,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=21, lr=[0.00042625120031760966], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:52:43,937] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=15.701933058542084, CurrSamplesPerSec=12.5526099994613, MemAllocated=1.42GB, MaxMemAllocated=2.45GB\n",
      "[2024-05-09 13:52:54,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=21, lr=[0.00048746599929965206], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:52:54,771] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=15.875852798533842, CurrSamplesPerSec=17.84197606619642, MemAllocated=1.0GB, MaxMemAllocated=2.45GB\n",
      "[2024-05-09 13:53:05,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=21, lr=[0.0005303548690088331], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:53:05,876] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=15.919021678542707, CurrSamplesPerSec=16.23717958489121, MemAllocated=1.12GB, MaxMemAllocated=2.45GB\n",
      "[2024-05-09 13:53:17,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=21, lr=[0.0005633986933428379], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:53:17,491] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=15.832195344817663, CurrSamplesPerSec=16.83228281322346, MemAllocated=1.07GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:53:28,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=21, lr=[0.0005902840038807148], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:53:28,364] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=15.904945846798727, CurrSamplesPerSec=16.70554919395792, MemAllocated=1.11GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:53:39,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=21, lr=[0.0006129496969124185], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:53:39,346] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=15.942712599252028, CurrSamplesPerSec=14.493901162922544, MemAllocated=1.26GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:53:45,872] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "[2024-05-09 13:53:50,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=22, lr=[0.0006306982008968268], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:53:50,465] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=15.949206824683728, CurrSamplesPerSec=17.90093368646775, MemAllocated=1.0GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:54:01,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=22, lr=[0.000648160890716723], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:54:01,814] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=15.917251799903845, CurrSamplesPerSec=16.702347726316784, MemAllocated=1.11GB, MaxMemAllocated=2.62GB\n",
      "{'epoch': 1, 'loss_train': 0.3346937027844516, 'loss_val': 0.0021554571881606767, 'accuracy_train': 0.7515856236786469, 'accuracy_val': 0.7801268498942917}\n",
      "[2024-05-09 13:54:40,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=22, lr=[0.0006637420252308317], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:54:40,459] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=16.015130679546985, CurrSamplesPerSec=16.862053205188605, MemAllocated=1.07GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:54:52,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=22, lr=[0.00067780791849565], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:54:52,232] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=15.93010252482494, CurrSamplesPerSec=11.358529916294799, MemAllocated=1.54GB, MaxMemAllocated=2.62GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 13:55:03,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=22, lr=[0.0006906273357687085], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:55:03,560] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=15.908273123837024, CurrSamplesPerSec=16.83001174684987, MemAllocated=1.07GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:55:14,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=22, lr=[0.0007024033232159562], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:55:14,216] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=15.96287315391339, CurrSamplesPerSec=15.74942801455515, MemAllocated=1.15GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:55:24,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=22, lr=[0.0007132930288004123], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:55:24,603] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=16.03654609378208, CurrSamplesPerSec=18.507357840346998, MemAllocated=0.95GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:55:35,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=22, lr=[0.0007234205717983191], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:55:35,461] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=16.057679574379343, CurrSamplesPerSec=15.955340457684228, MemAllocated=1.17GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:55:46,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=22, lr=[0.0007328856956514742], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:55:46,863] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=16.027932100168076, CurrSamplesPerSec=15.134041254322273, MemAllocated=1.2GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:55:50,465] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
      "[2024-05-09 13:55:58,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=23, lr=[0.0007409054903825278], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:55:58,059] [INFO] [timer.py:260:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=16.017677540142344, CurrSamplesPerSec=15.656496576206155, MemAllocated=1.17GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:56:09,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=23, lr=[0.0007493244221206022], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:56:09,339] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=16.003455532491365, CurrSamplesPerSec=15.166787540468617, MemAllocated=1.19GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:56:20,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=23, lr=[0.0007572805355121663], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:56:20,531] [INFO] [timer.py:260:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=15.997042579135737, CurrSamplesPerSec=14.491578827123185, MemAllocated=1.25GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:56:31,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=23, lr=[0.0007648220753871977], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:56:31,198] [INFO] [timer.py:260:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=16.028708983830754, CurrSamplesPerSec=17.696127222304405, MemAllocated=1.02GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:56:42,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=23, lr=[0.0007719901151523058], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:56:42,846] [INFO] [timer.py:260:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=15.989038286184938, CurrSamplesPerSec=17.12134650405832, MemAllocated=1.06GB, MaxMemAllocated=2.62GB\n",
      "{'epoch': 2, 'loss_train': 0.5205502683025082, 'loss_val': 0.014295388477801268, 'accuracy_train': 0.8181818181818182, 'accuracy_val': 0.828752642706131}\n",
      "[2024-05-09 13:57:21,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=23, lr=[0.0007788199112828433], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:57:21,818] [INFO] [timer.py:260:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=15.997875399612099, CurrSamplesPerSec=11.901068542873833, MemAllocated=1.48GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:57:32,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=23, lr=[0.0007853419523977076], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:57:32,380] [INFO] [timer.py:260:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=16.032976140911778, CurrSamplesPerSec=17.104641601863676, MemAllocated=1.05GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:57:43,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=23, lr=[0.0007915827820033681], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:57:43,755] [INFO] [timer.py:260:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=16.013887265076683, CurrSamplesPerSec=10.979786935190347, MemAllocated=1.58GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:57:54,726] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=23, lr=[0.0007975656510865553], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:57:54,727] [INFO] [timer.py:260:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=16.02209373579701, CurrSamplesPerSec=17.077124459762345, MemAllocated=1.05GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:58:06,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=23, lr=[0.0008033110411104316], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:58:06,102] [INFO] [timer.py:260:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=16.00453523124119, CurrSamplesPerSec=15.766163469056353, MemAllocated=1.15GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:58:16,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=23, lr=[0.0008088370871215251], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:58:16,688] [INFO] [timer.py:260:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=16.032905680988886, CurrSamplesPerSec=17.846459873639287, MemAllocated=1.01GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:58:27,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=23, lr=[0.0008141599230214829], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:58:27,774] [INFO] [timer.py:260:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=16.032401044620226, CurrSamplesPerSec=18.75499596723032, MemAllocated=0.98GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:58:39,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=23, lr=[0.0008192939655779976], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:58:39,068] [INFO] [timer.py:260:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=16.021614748773153, CurrSamplesPerSec=15.553822327888748, MemAllocated=1.19GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:58:49,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=23, lr=[0.0008242521497724042], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:58:49,957] [INFO] [timer.py:260:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=16.031826390504975, CurrSamplesPerSec=16.70623954130875, MemAllocated=1.11GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:59:00,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=23, lr=[0.0008290461251590622], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:59:00,655] [INFO] [timer.py:260:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=16.050723761416332, CurrSamplesPerSec=15.61199522163895, MemAllocated=1.17GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:59:12,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=23, lr=[0.0008336864207392504], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:59:12,207] [INFO] [timer.py:260:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=16.029024538576834, CurrSamplesPerSec=17.685927814049784, MemAllocated=1.01GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 13:59:23,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=23, lr=[0.0008381825842200954], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 13:59:23,772] [INFO] [timer.py:260:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=16.00716847231775, CurrSamplesPerSec=16.940650929845685, MemAllocated=1.06GB, MaxMemAllocated=2.62GB\n",
      "{'epoch': 3, 'loss_train': 0.6357843694739815, 'loss_val': 0.021703224101479915, 'accuracy_train': 0.8472515856236786, 'accuracy_val': 0.7843551797040169}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 13:59:59,437] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
      "[2024-05-09 14:00:01,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=24, lr=[0.0008421130924632814], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:00:01,478] [INFO] [timer.py:260:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=16.047992722789314, CurrSamplesPerSec=17.663504413318787, MemAllocated=1.02GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:00:12,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=24, lr=[0.0008463586995975923], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:00:12,898] [INFO] [timer.py:260:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=16.032678338976442, CurrSamplesPerSec=13.449427512851926, MemAllocated=1.32GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:00:24,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=24, lr=[0.0008504833326576252], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:00:24,005] [INFO] [timer.py:260:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=16.032001745832723, CurrSamplesPerSec=17.847864789032062, MemAllocated=1.01GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:00:35,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=24, lr=[0.0008544936951314702], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:00:35,193] [INFO] [timer.py:260:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=16.027907631400197, CurrSamplesPerSec=12.535395018258912, MemAllocated=1.42GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:00:46,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=24, lr=[0.0008583959483092204], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:00:46,580] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=16.015766847701716, CurrSamplesPerSec=16.675930566720382, MemAllocated=1.11GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:00:57,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=24, lr=[0.0008621957682239185], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:00:57,332] [INFO] [timer.py:260:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=16.02874626793766, CurrSamplesPerSec=17.596403361177078, MemAllocated=1.03GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:01:08,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=24, lr=[0.0008658983953085041], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:01:08,308] [INFO] [timer.py:260:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=16.032955000074583, CurrSamplesPerSec=16.231080711751346, MemAllocated=1.13GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:01:19,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=24, lr=[0.0008695086778590647], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:01:19,574] [INFO] [timer.py:260:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=16.026138556954486, CurrSamplesPerSec=16.245968760054044, MemAllocated=1.14GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:01:30,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=24, lr=[0.0008730311102089144], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:01:30,123] [INFO] [timer.py:260:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=16.045228000445057, CurrSamplesPerSec=18.01801778581098, MemAllocated=1.01GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:01:40,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=24, lr=[0.0008764698663675731], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:01:40,677] [INFO] [timer.py:260:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=16.063504611761232, CurrSamplesPerSec=16.244419350817378, MemAllocated=1.13GB, MaxMemAllocated=2.62GB\n",
      "[2024-05-09 14:01:52,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=24, lr=[0.0008798288297561954], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:01:52,319] [INFO] [timer.py:260:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=16.043078895023346, CurrSamplesPerSec=16.310166234783846, MemAllocated=1.11GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:02:03,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=24, lr=[0.0008831116195707141], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:02:03,356] [INFO] [timer.py:260:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=16.04434646469151, CurrSamplesPerSec=11.91211053120463, MemAllocated=1.48GB, MaxMemAllocated=2.67GB\n",
      "{'epoch': 4, 'loss_train': 0.9806679852669133, 'loss_val': 0.3195031712473573, 'accuracy_train': 0.8551797040169133, 'accuracy_val': 0.7145877378435518}\n",
      "[2024-05-09 14:02:42,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=24, lr=[0.0008863216142214784], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:02:42,890] [INFO] [timer.py:260:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=16.03540240852676, CurrSamplesPerSec=12.35331938261442, MemAllocated=1.43GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:02:53,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=24, lr=[0.00088946197223], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:02:53,790] [INFO] [timer.py:260:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=16.040810164495912, CurrSamplesPerSec=16.708023896040523, MemAllocated=1.12GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:03:05,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=24, lr=[0.0008925356509068312], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:03:05,182] [INFO] [timer.py:260:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=16.031012978666627, CurrSamplesPerSec=16.85621264053989, MemAllocated=1.07GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:03:16,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=24, lr=[0.0008955454230874312], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:03:16,264] [INFO] [timer.py:260:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=16.030762041918038, CurrSamplesPerSec=11.029138059338457, MemAllocated=1.58GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:03:28,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=24, lr=[0.0008984938921633992], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:03:28,079] [INFO] [timer.py:260:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=16.008121228787726, CurrSamplesPerSec=11.363091896984404, MemAllocated=1.54GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:03:38,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=24, lr=[0.0009013835056132664], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:03:38,798] [INFO] [timer.py:260:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=16.019449106328445, CurrSamplesPerSec=17.785304865056254, MemAllocated=1.01GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:03:49,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=24, lr=[0.0009042165672090705], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:03:49,896] [INFO] [timer.py:260:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=16.01944455903011, CurrSamplesPerSec=15.177716175295524, MemAllocated=1.19GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:04:01,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=24, lr=[0.0009069952480512465], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:04:01,097] [INFO] [timer.py:260:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=16.016590394587855, CurrSamplesPerSec=11.948707466617478, MemAllocated=1.47GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:04:11,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=24, lr=[0.0009097215965642567], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:04:11,715] [INFO] [timer.py:260:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=16.030491666787764, CurrSamplesPerSec=17.147332600515835, MemAllocated=1.06GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:04:22,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=24, lr=[0.0009123975475682457], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:04:22,721] [INFO] [timer.py:260:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=16.032933172165105, CurrSamplesPerSec=17.834171552699395, MemAllocated=1.01GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:04:33,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=24, lr=[0.0009150249305273526], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:04:33,575] [INFO] [timer.py:260:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=16.039027507907768, CurrSamplesPerSec=16.318038631440245, MemAllocated=1.11GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:04:45,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=24, lr=[0.0009176054770627572], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:04:45,154] [INFO] [timer.py:260:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=16.025267093463505, CurrSamplesPerSec=13.775203188199638, MemAllocated=1.3GB, MaxMemAllocated=2.67GB\n",
      "{'epoch': 5, 'loss_train': 1.132785072002804, 'loss_val': 0.0, 'accuracy_train': 0.8736786469344608, 'accuracy_val': 0.8583509513742071}\n",
      "[2024-05-09 14:05:24,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=24, lr=[0.0009201408278077375], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:05:24,221] [INFO] [timer.py:260:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=16.034677745491624, CurrSamplesPerSec=12.363134869512464, MemAllocated=1.43GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:05:34,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=24, lr=[0.0009226325386726969], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:05:34,964] [INFO] [timer.py:260:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=16.043697989389873, CurrSamplesPerSec=17.960700433676227, MemAllocated=1.01GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:05:45,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=24, lr=[0.0009250820865800789], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:05:45,932] [INFO] [timer.py:260:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=16.046268627469196, CurrSamplesPerSec=15.205268711395671, MemAllocated=1.18GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:05:57,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=24, lr=[0.0009274908747220953], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:05:57,343] [INFO] [timer.py:260:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=16.03746016722774, CurrSamplesPerSec=11.903426474654815, MemAllocated=1.48GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:06:07,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=24, lr=[0.0009298602373881418], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:06:07,762] [INFO] [timer.py:260:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=16.053923649916, CurrSamplesPerSec=18.560658647640494, MemAllocated=0.99GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:06:18,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=24, lr=[0.0009321914444034765], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:06:18,352] [INFO] [timer.py:260:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=16.065639384199752, CurrSamplesPerSec=16.649811641320614, MemAllocated=1.12GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:06:29,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=24, lr=[0.0009344857052161381], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:06:29,211] [INFO] [timer.py:260:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=16.070425585826943, CurrSamplesPerSec=16.280237334993327, MemAllocated=1.11GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:06:39,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=24, lr=[0.0009367441726650281], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:06:39,832] [INFO] [timer.py:260:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=16.081048696860748, CurrSamplesPerSec=16.714111870632426, MemAllocated=1.11GB, MaxMemAllocated=2.67GB\n",
      "[2024-05-09 14:06:52,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=24, lr=[0.0009389679464585534], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:06:52,147] [INFO] [timer.py:260:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=16.0510554954169, CurrSamplesPerSec=17.24220375075023, MemAllocated=1.04GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:07:03,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=24, lr=[0.0009411580763901004], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:07:03,070] [INFO] [timer.py:260:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=16.05430688368419, CurrSamplesPerSec=16.30566037827537, MemAllocated=1.13GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:07:14,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=24, lr=[0.0009433155653138787], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:07:14,148] [INFO] [timer.py:260:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=16.054135615844487, CurrSamplesPerSec=16.76672003325918, MemAllocated=1.07GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:07:25,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=24, lr=[0.0009454413719022505], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:07:25,665] [INFO] [timer.py:260:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=16.04418934354233, CurrSamplesPerSec=15.708371292167076, MemAllocated=1.15GB, MaxMemAllocated=2.9GB\n",
      "{'epoch': 6, 'loss_train': 1.4229797449978916, 'loss_val': 0.08621828752642706, 'accuracy_train': 0.870507399577167, 'accuracy_val': 0.8477801268498943}\n",
      "[2024-05-09 14:08:04,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=24, lr=[0.0009475364132035208], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:08:04,989] [INFO] [timer.py:260:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=16.03280906867583, CurrSamplesPerSec=11.144032793337047, MemAllocated=1.58GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:08:15,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=24, lr=[0.000949601567017268], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:08:15,496] [INFO] [timer.py:260:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=16.045578621453775, CurrSamplesPerSec=18.260835871190764, MemAllocated=1.02GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:08:26,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=24, lr=[0.0009516376741026186], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:08:26,647] [INFO] [timer.py:260:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=16.043618838374368, CurrSamplesPerSec=17.688281914499655, MemAllocated=1.06GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:08:37,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=24, lr=[0.0009536455402333646], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:08:37,279] [INFO] [timer.py:260:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=16.053014587716575, CurrSamplesPerSec=16.64199975201488, MemAllocated=1.12GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:08:48,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=24, lr=[0.0009556259381124998], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:08:48,490] [INFO] [timer.py:260:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=16.050115833464794, CurrSamplesPerSec=16.950488127258872, MemAllocated=1.05GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:08:59,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=24, lr=[0.0009575796091575563], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:08:59,641] [INFO] [timer.py:260:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=16.048512802840143, CurrSamplesPerSec=18.889926806276033, MemAllocated=0.98GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:09:10,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=24, lr=[0.000959507265167069], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:09:10,595] [INFO] [timer.py:260:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=16.050684164155296, CurrSamplesPerSec=18.25108213805895, MemAllocated=1.01GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:09:20,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=24, lr=[0.0009614095898775347], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:09:20,930] [INFO] [timer.py:260:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=16.065178453145766, CurrSamplesPerSec=16.753162646729958, MemAllocated=1.08GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:09:31,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=24, lr=[0.0009632872404193962], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:09:31,716] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=16.070437735553305, CurrSamplesPerSec=14.539528101888914, MemAllocated=1.24GB, MaxMemAllocated=2.9GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:09:42,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=24, lr=[0.0009651408486798027], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:09:42,841] [INFO] [timer.py:260:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=16.069187468889282, CurrSamplesPerSec=17.77791718015367, MemAllocated=1.01GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:09:53,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=24, lr=[0.0009669710225792231], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:09:53,801] [INFO] [timer.py:260:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=16.07196019280905, CurrSamplesPerSec=16.28592659536104, MemAllocated=1.1GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:10:05,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=24, lr=[0.0009687783472683636], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:10:05,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=16.068566893925667, CurrSamplesPerSec=16.31171631765582, MemAllocated=1.12GB, MaxMemAllocated=2.9GB\n",
      "{'epoch': 7, 'loss_train': 1.0236375461988168, 'loss_val': 0.0386826109936575, 'accuracy_train': 0.8916490486257929, 'accuracy_val': 0.7949260042283298}\n",
      "[2024-05-09 14:10:44,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=24, lr=[0.0009705633862512871], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:10:44,324] [INFO] [timer.py:260:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=16.057837094826375, CurrSamplesPerSec=17.540719675683306, MemAllocated=1.06GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:10:54,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=24, lr=[0.0009723266824401274], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:10:54,676] [INFO] [timer.py:260:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=16.07131130659036, CurrSamplesPerSec=18.416182585367352, MemAllocated=1.01GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:11:05,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=24, lr=[0.0009740687591463388], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:11:05,669] [INFO] [timer.py:260:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=16.072420710880216, CurrSamplesPerSec=15.584329112419, MemAllocated=1.18GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:11:17,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=24, lr=[0.0009757901210130079], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:11:17,614] [INFO] [timer.py:260:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=16.058718230374275, CurrSamplesPerSec=15.292543260673344, MemAllocated=1.08GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:11:29,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=24, lr=[0.0009774912548923844], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:11:29,891] [INFO] [timer.py:260:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=16.03893819026041, CurrSamplesPerSec=13.975647267101403, MemAllocated=1.27GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:11:41,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=24, lr=[0.0009791726306724489], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:11:41,361] [INFO] [timer.py:260:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=16.033044953835205, CurrSamplesPerSec=18.322729032529335, MemAllocated=0.99GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:11:52,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=24, lr=[0.000980834702056027], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:11:52,539] [INFO] [timer.py:260:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=16.032053798415227, CurrSamplesPerSec=16.568461667223815, MemAllocated=1.07GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:12:05,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=24, lr=[0.0009824779072956837], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:12:05,338] [INFO] [timer.py:260:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=16.01158747500319, CurrSamplesPerSec=15.457949565368816, MemAllocated=1.02GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:12:18,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=24, lr=[0.0009841026698873751], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:12:18,572] [INFO] [timer.py:260:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=15.984921834368627, CurrSamplesPerSec=14.2303830075621, MemAllocated=1.06GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:12:31,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=24, lr=[0.0009857093992256044], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:12:31,607] [INFO] [timer.py:260:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=15.961388788840031, CurrSamplesPerSec=14.763854410223422, MemAllocated=1.07GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:12:44,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=24, lr=[0.0009872984912226168], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:12:44,523] [INFO] [timer.py:260:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=15.940127781507325, CurrSamplesPerSec=14.913611150618689, MemAllocated=1.04GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:12:57,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=24, lr=[0.0009888703288939782], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:12:57,583] [INFO] [timer.py:260:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=15.917479151566576, CurrSamplesPerSec=14.7221477193228, MemAllocated=1.04GB, MaxMemAllocated=2.9GB\n",
      "{'epoch': 8, 'loss_train': 2.023357431681917, 'loss_val': 6.300702407546578e-10, 'accuracy_train': 0.8752642706131079, 'accuracy_val': 0.8583509513742071}\n",
      "[2024-05-09 14:13:38,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=24, lr=[0.0009904252829127018], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:13:38,774] [INFO] [timer.py:260:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=15.902052771193874, CurrSamplesPerSec=12.65042812555786, MemAllocated=1.26GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:13:52,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=24, lr=[0.000991963712133931], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:13:52,257] [INFO] [timer.py:260:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=15.873745840732619, CurrSamplesPerSec=14.11115047880106, MemAllocated=1.17GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:14:04,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=24, lr=[0.0009934859640920333], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:14:04,589] [INFO] [timer.py:260:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=15.863603415328004, CurrSamplesPerSec=14.759386477701096, MemAllocated=1.08GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:14:17,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=24, lr=[0.000994992375471831], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:14:17,747] [INFO] [timer.py:260:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=15.84158577655872, CurrSamplesPerSec=14.905929355227125, MemAllocated=1.03GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:14:32,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=24, lr=[0.0009964832725555639], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:14:32,095] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=15.802148775954596, CurrSamplesPerSec=13.196632962998068, MemAllocated=1.12GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:14:46,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=24, lr=[0.0009979589716470705], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:14:46,875] [INFO] [timer.py:260:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=15.757708370082286, CurrSamplesPerSec=10.235514465706556, MemAllocated=1.48GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:15:00,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=24, lr=[0.0009994197794745664], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:15:00,458] [INFO] [timer.py:260:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=15.7317368657582, CurrSamplesPerSec=9.898415843161363, MemAllocated=1.58GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:15:14,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:15:14,459] [INFO] [timer.py:260:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=15.701645636886125, CurrSamplesPerSec=11.98373427409591, MemAllocated=1.27GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:15:27,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:15:27,986] [INFO] [timer.py:260:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=15.678354167690971, CurrSamplesPerSec=13.75328190708846, MemAllocated=1.02GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:15:41,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:15:41,422] [INFO] [timer.py:260:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=15.655879203366373, CurrSamplesPerSec=14.921480024200289, MemAllocated=1.06GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:15:54,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:15:54,231] [INFO] [timer.py:260:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=15.643294726423909, CurrSamplesPerSec=14.826798804379933, MemAllocated=1.01GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:18:45,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:18:45,807] [INFO] [timer.py:260:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=13.653603071808313, CurrSamplesPerSec=9.904765053401919, MemAllocated=1.03GB, MaxMemAllocated=2.9GB\n",
      "{'epoch': 9, 'loss_train': 1.609274931299258, 'loss_val': 0.09559989429175475, 'accuracy_train': 0.8863636363636364, 'accuracy_val': 0.7949260042283298}\n",
      "[2024-05-09 14:19:27,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:19:28,001] [INFO] [timer.py:260:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=13.651933013146532, CurrSamplesPerSec=13.54863681568737, MemAllocated=1.08GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:19:41,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:19:41,793] [INFO] [timer.py:260:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=13.648751484499025, CurrSamplesPerSec=14.063239335824985, MemAllocated=0.99GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:19:56,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:19:56,882] [INFO] [timer.py:260:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=13.632416560165982, CurrSamplesPerSec=10.470036949200193, MemAllocated=1.54GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:20:09,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:20:09,357] [INFO] [timer.py:260:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=13.64081279341439, CurrSamplesPerSec=17.60931328288446, MemAllocated=0.97GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:20:21,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:20:21,918] [INFO] [timer.py:260:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=13.647699813487476, CurrSamplesPerSec=15.511543825764807, MemAllocated=1.05GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:20:33,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:20:33,692] [INFO] [timer.py:260:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=13.659942241530898, CurrSamplesPerSec=16.084577885931544, MemAllocated=1.13GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:20:44,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:20:44,930] [INFO] [timer.py:260:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=13.676743948372486, CurrSamplesPerSec=17.62533847473501, MemAllocated=1.01GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:20:55,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:20:55,564] [INFO] [timer.py:260:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=13.6991947051027, CurrSamplesPerSec=16.247601071667393, MemAllocated=1.12GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:30:27,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:30:27,460] [INFO] [timer.py:260:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=9.696842417798319, CurrSamplesPerSec=15.983522236311481, MemAllocated=1.16GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:30:39,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:30:39,857] [INFO] [timer.py:260:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=9.724498839093897, CurrSamplesPerSec=17.527530127829596, MemAllocated=1.0GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:30:50,366] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:30:50,366] [INFO] [timer.py:260:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=9.760076892725742, CurrSamplesPerSec=16.68693965645856, MemAllocated=1.1GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:31:01,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:31:01,450] [INFO] [timer.py:260:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=9.792708156004625, CurrSamplesPerSec=19.30381514968117, MemAllocated=0.9GB, MaxMemAllocated=2.9GB\n",
      "{'epoch': 10, 'loss_train': 1.6115586959282389, 'loss_val': 0.014972581923890064, 'accuracy_train': 0.8953488372093024, 'accuracy_val': 0.8604651162790697}\n",
      "[2024-05-09 14:31:41,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:31:41,225] [INFO] [timer.py:260:stop] epoch=0/micro_step=1200/global_step=1200, RunningAvgSamplesPerSec=9.824202424317463, CurrSamplesPerSec=17.90401407800637, MemAllocated=1.01GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:31:52,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:31:52,183] [INFO] [timer.py:260:stop] epoch=0/micro_step=1210/global_step=1210, RunningAvgSamplesPerSec=9.856590987817098, CurrSamplesPerSec=15.051332394038935, MemAllocated=1.2GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:32:03,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:32:03,717] [INFO] [timer.py:260:stop] epoch=0/micro_step=1220/global_step=1220, RunningAvgSamplesPerSec=9.885997300660092, CurrSamplesPerSec=15.966979603509701, MemAllocated=1.17GB, MaxMemAllocated=2.9GB\n",
      "[2024-05-09 14:32:15,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:32:15,573] [INFO] [timer.py:260:stop] epoch=0/micro_step=1230/global_step=1230, RunningAvgSamplesPerSec=9.913522441172733, CurrSamplesPerSec=12.812534234636201, MemAllocated=1.42GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:32:26,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:32:26,517] [INFO] [timer.py:260:stop] epoch=0/micro_step=1240/global_step=1240, RunningAvgSamplesPerSec=9.945129855275725, CurrSamplesPerSec=12.187248410146442, MemAllocated=1.47GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:32:37,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:32:37,135] [INFO] [timer.py:260:stop] epoch=0/micro_step=1250/global_step=1250, RunningAvgSamplesPerSec=9.978055099330234, CurrSamplesPerSec=16.091809511568986, MemAllocated=1.17GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:32:47,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:32:47,903] [INFO] [timer.py:260:stop] epoch=0/micro_step=1260/global_step=1260, RunningAvgSamplesPerSec=10.009934067904101, CurrSamplesPerSec=17.441198049442775, MemAllocated=1.07GB, MaxMemAllocated=2.96GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:32:58,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:32:58,701] [INFO] [timer.py:260:stop] epoch=0/micro_step=1270/global_step=1270, RunningAvgSamplesPerSec=10.041326822520412, CurrSamplesPerSec=19.006427628920054, MemAllocated=0.94GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:33:10,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=24, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:33:10,774] [INFO] [timer.py:260:stop] epoch=0/micro_step=1280/global_step=1280, RunningAvgSamplesPerSec=10.066205541105584, CurrSamplesPerSec=15.607344111133798, MemAllocated=1.15GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:33:18,353] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 256, reducing to 128\n",
      "[2024-05-09 14:33:21,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:33:21,492] [INFO] [timer.py:260:stop] epoch=0/micro_step=1290/global_step=1290, RunningAvgSamplesPerSec=10.097522588078059, CurrSamplesPerSec=16.898973101194102, MemAllocated=1.07GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:33:32,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:33:32,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=1300/global_step=1300, RunningAvgSamplesPerSec=10.126947120402843, CurrSamplesPerSec=13.820676213412481, MemAllocated=1.3GB, MaxMemAllocated=2.96GB\n",
      "{'epoch': 11, 'loss_train': 1.7068696168209987, 'loss_val': 0.0, 'accuracy_train': 0.9106765327695561, 'accuracy_val': 0.9027484143763214}\n",
      "[2024-05-09 14:34:10,286] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:34:10,287] [INFO] [timer.py:260:stop] epoch=0/micro_step=1310/global_step=1310, RunningAvgSamplesPerSec=10.158628145980893, CurrSamplesPerSec=17.647915625178328, MemAllocated=1.03GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:34:21,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:34:21,696] [INFO] [timer.py:260:stop] epoch=0/micro_step=1320/global_step=1320, RunningAvgSamplesPerSec=10.185329837009396, CurrSamplesPerSec=16.66401733027544, MemAllocated=1.09GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:34:32,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:34:32,903] [INFO] [timer.py:260:stop] epoch=0/micro_step=1330/global_step=1330, RunningAvgSamplesPerSec=10.212733121724927, CurrSamplesPerSec=16.250784039140086, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:34:44,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:34:44,062] [INFO] [timer.py:260:stop] epoch=0/micro_step=1340/global_step=1340, RunningAvgSamplesPerSec=10.240210245815145, CurrSamplesPerSec=17.135917885109528, MemAllocated=1.06GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:34:55,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:34:55,404] [INFO] [timer.py:260:stop] epoch=0/micro_step=1350/global_step=1350, RunningAvgSamplesPerSec=10.266463066441887, CurrSamplesPerSec=14.569148616298227, MemAllocated=1.24GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:35:06,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:35:06,113] [INFO] [timer.py:260:stop] epoch=0/micro_step=1360/global_step=1360, RunningAvgSamplesPerSec=10.295521176392068, CurrSamplesPerSec=16.832527685976117, MemAllocated=1.08GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:35:17,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:35:17,279] [INFO] [timer.py:260:stop] epoch=0/micro_step=1370/global_step=1370, RunningAvgSamplesPerSec=10.322112547175957, CurrSamplesPerSec=18.330761811604873, MemAllocated=1.02GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:35:28,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:35:28,185] [INFO] [timer.py:260:stop] epoch=0/micro_step=1380/global_step=1380, RunningAvgSamplesPerSec=10.349673036207502, CurrSamplesPerSec=14.866436618086498, MemAllocated=1.27GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:35:39,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:35:39,251] [INFO] [timer.py:260:stop] epoch=0/micro_step=1390/global_step=1390, RunningAvgSamplesPerSec=10.376325859489441, CurrSamplesPerSec=18.417997083699152, MemAllocated=1.02GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:35:50,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:35:50,147] [INFO] [timer.py:260:stop] epoch=0/micro_step=1400/global_step=1400, RunningAvgSamplesPerSec=10.403578912539357, CurrSamplesPerSec=18.30623513667305, MemAllocated=1.01GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:36:00,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:36:00,814] [INFO] [timer.py:260:stop] epoch=0/micro_step=1410/global_step=1410, RunningAvgSamplesPerSec=10.431578343649976, CurrSamplesPerSec=17.667284983843548, MemAllocated=1.02GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:36:12,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:36:12,026] [INFO] [timer.py:260:stop] epoch=0/micro_step=1420/global_step=1420, RunningAvgSamplesPerSec=10.456784526843531, CurrSamplesPerSec=17.437318780421958, MemAllocated=1.07GB, MaxMemAllocated=2.96GB\n",
      "{'epoch': 12, 'loss_train': 1.7266492177643684, 'loss_val': 0.20137420718816068, 'accuracy_train': 0.9085623678646935, 'accuracy_val': 0.8900634249471459}\n",
      "[2024-05-09 14:36:50,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:36:50,131] [INFO] [timer.py:260:stop] epoch=0/micro_step=1430/global_step=1430, RunningAvgSamplesPerSec=10.486622257099205, CurrSamplesPerSec=15.539696712595727, MemAllocated=1.17GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:37:01,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:37:01,655] [INFO] [timer.py:260:stop] epoch=0/micro_step=1440/global_step=1440, RunningAvgSamplesPerSec=10.509894597081518, CurrSamplesPerSec=16.807284231490137, MemAllocated=1.13GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:37:12,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:37:12,099] [INFO] [timer.py:260:stop] epoch=0/micro_step=1450/global_step=1450, RunningAvgSamplesPerSec=10.53801283077578, CurrSamplesPerSec=17.87323398619282, MemAllocated=1.05GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:37:23,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:37:23,654] [INFO] [timer.py:260:stop] epoch=0/micro_step=1460/global_step=1460, RunningAvgSamplesPerSec=10.560664422165175, CurrSamplesPerSec=18.315038180855534, MemAllocated=1.02GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:37:35,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:37:35,014] [INFO] [timer.py:260:stop] epoch=0/micro_step=1470/global_step=1470, RunningAvgSamplesPerSec=10.583942560726452, CurrSamplesPerSec=17.19165082861266, MemAllocated=1.11GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:37:45,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:37:45,306] [INFO] [timer.py:260:stop] epoch=0/micro_step=1480/global_step=1480, RunningAvgSamplesPerSec=10.612034524163908, CurrSamplesPerSec=17.413577786634782, MemAllocated=1.07GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:37:55,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:37:55,646] [INFO] [timer.py:260:stop] epoch=0/micro_step=1490/global_step=1490, RunningAvgSamplesPerSec=10.639632994058962, CurrSamplesPerSec=17.62681527620145, MemAllocated=1.03GB, MaxMemAllocated=2.96GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:38:06,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:38:06,852] [INFO] [timer.py:260:stop] epoch=0/micro_step=1500/global_step=1500, RunningAvgSamplesPerSec=10.662972475380085, CurrSamplesPerSec=11.380606412930188, MemAllocated=1.54GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:38:18,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:38:18,542] [INFO] [timer.py:260:stop] epoch=0/micro_step=1510/global_step=1510, RunningAvgSamplesPerSec=10.683895019464368, CurrSamplesPerSec=10.826931847900248, MemAllocated=1.62GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:38:29,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:38:29,502] [INFO] [timer.py:260:stop] epoch=0/micro_step=1520/global_step=1520, RunningAvgSamplesPerSec=10.707939284841785, CurrSamplesPerSec=17.115613064807576, MemAllocated=1.06GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:38:40,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:38:40,146] [INFO] [timer.py:260:stop] epoch=0/micro_step=1530/global_step=1530, RunningAvgSamplesPerSec=10.733291580478669, CurrSamplesPerSec=16.29232385735733, MemAllocated=1.13GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:38:50,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:38:50,618] [INFO] [timer.py:260:stop] epoch=0/micro_step=1540/global_step=1540, RunningAvgSamplesPerSec=10.759272194976713, CurrSamplesPerSec=16.806400314145513, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n",
      "{'epoch': 13, 'loss_train': 1.6435489936812484, 'loss_val': 0.0, 'accuracy_train': 0.9170190274841438, 'accuracy_val': 0.8942917547568711}\n",
      "[2024-05-09 14:39:29,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:39:29,453] [INFO] [timer.py:260:stop] epoch=0/micro_step=1550/global_step=1550, RunningAvgSamplesPerSec=10.780575060367088, CurrSamplesPerSec=16.678437951233818, MemAllocated=1.08GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:39:40,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:39:40,662] [INFO] [timer.py:260:stop] epoch=0/micro_step=1560/global_step=1560, RunningAvgSamplesPerSec=10.802583498413567, CurrSamplesPerSec=16.68979070384558, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:39:51,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:39:51,821] [INFO] [timer.py:260:stop] epoch=0/micro_step=1570/global_step=1570, RunningAvgSamplesPerSec=10.824632985052405, CurrSamplesPerSec=17.93302166242005, MemAllocated=1.0GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:40:02,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:40:02,424] [INFO] [timer.py:260:stop] epoch=0/micro_step=1580/global_step=1580, RunningAvgSamplesPerSec=10.849158869339114, CurrSamplesPerSec=17.17276487986026, MemAllocated=1.1GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:40:13,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:40:13,041] [INFO] [timer.py:260:stop] epoch=0/micro_step=1590/global_step=1590, RunningAvgSamplesPerSec=10.873442363667609, CurrSamplesPerSec=16.685724003855857, MemAllocated=1.14GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:40:23,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:40:23,856] [INFO] [timer.py:260:stop] epoch=0/micro_step=1600/global_step=1600, RunningAvgSamplesPerSec=10.89660079549315, CurrSamplesPerSec=16.208703817937742, MemAllocated=1.14GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:40:34,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:40:34,776] [INFO] [timer.py:260:stop] epoch=0/micro_step=1610/global_step=1610, RunningAvgSamplesPerSec=10.91909412630899, CurrSamplesPerSec=15.256362608798053, MemAllocated=1.18GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:40:47,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:40:47,011] [INFO] [timer.py:260:stop] epoch=0/micro_step=1620/global_step=1620, RunningAvgSamplesPerSec=10.93545294733982, CurrSamplesPerSec=17.596384905606946, MemAllocated=1.03GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:40:58,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:40:58,190] [INFO] [timer.py:260:stop] epoch=0/micro_step=1630/global_step=1630, RunningAvgSamplesPerSec=10.956459730162106, CurrSamplesPerSec=16.855463274385517, MemAllocated=1.1GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:41:09,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:41:09,074] [INFO] [timer.py:260:stop] epoch=0/micro_step=1640/global_step=1640, RunningAvgSamplesPerSec=10.978585193603154, CurrSamplesPerSec=16.59028906552645, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:41:19,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:41:19,860] [INFO] [timer.py:260:stop] epoch=0/micro_step=1650/global_step=1650, RunningAvgSamplesPerSec=11.001021692783297, CurrSamplesPerSec=16.230425149531158, MemAllocated=1.13GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:41:30,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:41:30,224] [INFO] [timer.py:260:stop] epoch=0/micro_step=1660/global_step=1660, RunningAvgSamplesPerSec=11.025147411038198, CurrSamplesPerSec=17.841473261131437, MemAllocated=1.04GB, MaxMemAllocated=2.96GB\n",
      "{'epoch': 14, 'loss_train': 1.3710346637140116, 'loss_val': 0.0, 'accuracy_train': 0.9207188160676533, 'accuracy_val': 0.8710359408033826}\n",
      "[2024-05-09 14:42:08,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:42:08,413] [INFO] [timer.py:260:stop] epoch=0/micro_step=1670/global_step=1670, RunningAvgSamplesPerSec=11.045810579253859, CurrSamplesPerSec=11.244639646009869, MemAllocated=1.58GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:42:19,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:42:19,016] [INFO] [timer.py:260:stop] epoch=0/micro_step=1680/global_step=1680, RunningAvgSamplesPerSec=11.068523555474854, CurrSamplesPerSec=16.87943363020612, MemAllocated=1.13GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:42:29,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:42:29,622] [INFO] [timer.py:260:stop] epoch=0/micro_step=1690/global_step=1690, RunningAvgSamplesPerSec=11.091136465593626, CurrSamplesPerSec=17.191206027962377, MemAllocated=1.09GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:42:40,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:42:40,818] [INFO] [timer.py:260:stop] epoch=0/micro_step=1700/global_step=1700, RunningAvgSamplesPerSec=11.110826080524712, CurrSamplesPerSec=16.84135207905506, MemAllocated=1.08GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:42:52,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:42:52,046] [INFO] [timer.py:260:stop] epoch=0/micro_step=1710/global_step=1710, RunningAvgSamplesPerSec=11.130130341200875, CurrSamplesPerSec=17.82654907125587, MemAllocated=1.01GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:43:03,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:43:03,182] [INFO] [timer.py:260:stop] epoch=0/micro_step=1720/global_step=1720, RunningAvgSamplesPerSec=11.149814404120622, CurrSamplesPerSec=11.609009685063738, MemAllocated=1.54GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:43:13,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:43:13,489] [INFO] [timer.py:260:stop] epoch=0/micro_step=1730/global_step=1730, RunningAvgSamplesPerSec=11.172979818391225, CurrSamplesPerSec=17.124968453135715, MemAllocated=1.08GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:43:24,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:43:24,619] [INFO] [timer.py:260:stop] epoch=0/micro_step=1740/global_step=1740, RunningAvgSamplesPerSec=11.192378462914244, CurrSamplesPerSec=17.847489806865667, MemAllocated=1.05GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:43:35,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:43:35,266] [INFO] [timer.py:260:stop] epoch=0/micro_step=1750/global_step=1750, RunningAvgSamplesPerSec=11.213702588553968, CurrSamplesPerSec=16.94992300295789, MemAllocated=1.06GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:43:45,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:43:45,857] [INFO] [timer.py:260:stop] epoch=0/micro_step=1760/global_step=1760, RunningAvgSamplesPerSec=11.23508413296599, CurrSamplesPerSec=14.573824915256042, MemAllocated=1.24GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:43:56,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:43:56,831] [INFO] [timer.py:260:stop] epoch=0/micro_step=1770/global_step=1770, RunningAvgSamplesPerSec=11.254764638239159, CurrSamplesPerSec=18.215072868310475, MemAllocated=1.03GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:44:08,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:44:08,228] [INFO] [timer.py:260:stop] epoch=0/micro_step=1780/global_step=1780, RunningAvgSamplesPerSec=11.272235484541417, CurrSamplesPerSec=15.685999815345706, MemAllocated=1.18GB, MaxMemAllocated=2.96GB\n",
      "{'epoch': 15, 'loss_train': 2.0212956941657034, 'loss_val': 0.1569767441860465, 'accuracy_train': 0.9027484143763214, 'accuracy_val': 0.864693446088795}\n",
      "[2024-05-09 14:44:46,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:44:46,387] [INFO] [timer.py:260:stop] epoch=0/micro_step=1790/global_step=1790, RunningAvgSamplesPerSec=11.293664863897465, CurrSamplesPerSec=11.36661204864335, MemAllocated=1.54GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:44:58,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:44:58,201] [INFO] [timer.py:260:stop] epoch=0/micro_step=1800/global_step=1800, RunningAvgSamplesPerSec=11.309088524716154, CurrSamplesPerSec=16.697070063694266, MemAllocated=1.07GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:45:09,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:45:09,665] [INFO] [timer.py:260:stop] epoch=0/micro_step=1810/global_step=1810, RunningAvgSamplesPerSec=11.325986105810367, CurrSamplesPerSec=11.962460304940057, MemAllocated=1.47GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:45:20,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:45:20,977] [INFO] [timer.py:260:stop] epoch=0/micro_step=1820/global_step=1820, RunningAvgSamplesPerSec=11.343276834713292, CurrSamplesPerSec=17.686617663724558, MemAllocated=1.06GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:45:31,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:45:31,263] [INFO] [timer.py:260:stop] epoch=0/micro_step=1830/global_step=1830, RunningAvgSamplesPerSec=11.364885259254464, CurrSamplesPerSec=15.58297569980611, MemAllocated=1.18GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:45:42,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:45:42,494] [INFO] [timer.py:260:stop] epoch=0/micro_step=1840/global_step=1840, RunningAvgSamplesPerSec=11.382378676874493, CurrSamplesPerSec=16.325648560808048, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:45:53,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:45:53,531] [INFO] [timer.py:260:stop] epoch=0/micro_step=1850/global_step=1850, RunningAvgSamplesPerSec=11.400491088668781, CurrSamplesPerSec=16.70873108845121, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:46:04,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:46:04,130] [INFO] [timer.py:260:stop] epoch=0/micro_step=1860/global_step=1860, RunningAvgSamplesPerSec=11.420301666450257, CurrSamplesPerSec=17.307763866626605, MemAllocated=1.04GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:46:15,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:46:15,752] [INFO] [timer.py:260:stop] epoch=0/micro_step=1870/global_step=1870, RunningAvgSamplesPerSec=11.435724873194356, CurrSamplesPerSec=16.62051775857218, MemAllocated=1.09GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:46:26,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:46:26,349] [INFO] [timer.py:260:stop] epoch=0/micro_step=1880/global_step=1880, RunningAvgSamplesPerSec=11.455367860380328, CurrSamplesPerSec=17.853011705878693, MemAllocated=1.04GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:46:37,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:46:37,080] [INFO] [timer.py:260:stop] epoch=0/micro_step=1890/global_step=1890, RunningAvgSamplesPerSec=11.47424703587156, CurrSamplesPerSec=16.80235231906933, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:46:47,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:46:47,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=1900/global_step=1900, RunningAvgSamplesPerSec=11.493086451632989, CurrSamplesPerSec=14.871941793013459, MemAllocated=1.25GB, MaxMemAllocated=2.96GB\n",
      "{'epoch': 16, 'loss_train': 1.478805763146842, 'loss_val': 0.005462969080338266, 'accuracy_train': 0.9249471458773785, 'accuracy_val': 0.9027484143763214}\n",
      "[2024-05-09 14:47:27,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:47:27,161] [INFO] [timer.py:260:stop] epoch=0/micro_step=1910/global_step=1910, RunningAvgSamplesPerSec=11.509938381012269, CurrSamplesPerSec=16.252751886196357, MemAllocated=1.05GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:47:38,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:47:38,513] [INFO] [timer.py:260:stop] epoch=0/micro_step=1920/global_step=1920, RunningAvgSamplesPerSec=11.52560343471468, CurrSamplesPerSec=17.478761031522208, MemAllocated=1.02GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:47:49,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:47:49,409] [INFO] [timer.py:260:stop] epoch=0/micro_step=1930/global_step=1930, RunningAvgSamplesPerSec=11.54330726883986, CurrSamplesPerSec=16.842759598979427, MemAllocated=1.11GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:47:59,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:47:59,870] [INFO] [timer.py:260:stop] epoch=0/micro_step=1940/global_step=1940, RunningAvgSamplesPerSec=11.562681682484522, CurrSamplesPerSec=18.662200936713184, MemAllocated=0.94GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:48:10,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:48:10,647] [INFO] [timer.py:260:stop] epoch=0/micro_step=1950/global_step=1950, RunningAvgSamplesPerSec=11.580604563687634, CurrSamplesPerSec=18.1988659075316, MemAllocated=1.01GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:48:21,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:48:21,991] [INFO] [timer.py:260:stop] epoch=0/micro_step=1960/global_step=1960, RunningAvgSamplesPerSec=11.596380945986251, CurrSamplesPerSec=15.269338303821062, MemAllocated=1.12GB, MaxMemAllocated=2.96GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-09 14:48:32,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:48:32,974] [INFO] [timer.py:260:stop] epoch=0/micro_step=1970/global_step=1970, RunningAvgSamplesPerSec=11.61343250733779, CurrSamplesPerSec=16.721549960182553, MemAllocated=1.03GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:48:45,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:48:45,408] [INFO] [timer.py:260:stop] epoch=0/micro_step=1980/global_step=1980, RunningAvgSamplesPerSec=11.624866972351526, CurrSamplesPerSec=14.416657697725412, MemAllocated=1.25GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:48:55,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:48:55,858] [INFO] [timer.py:260:stop] epoch=0/micro_step=1990/global_step=1990, RunningAvgSamplesPerSec=11.643630878706109, CurrSamplesPerSec=18.582923303092304, MemAllocated=0.97GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:49:07,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:49:07,189] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=11.658561866614184, CurrSamplesPerSec=15.23818243167452, MemAllocated=1.18GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:49:18,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:49:18,311] [INFO] [timer.py:260:stop] epoch=0/micro_step=2010/global_step=2010, RunningAvgSamplesPerSec=11.674252377859789, CurrSamplesPerSec=17.269149304371656, MemAllocated=1.04GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:49:28,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:49:28,951] [INFO] [timer.py:260:stop] epoch=0/micro_step=2020/global_step=2020, RunningAvgSamplesPerSec=11.69186244666335, CurrSamplesPerSec=16.291857138071155, MemAllocated=1.13GB, MaxMemAllocated=2.96GB\n",
      "{'epoch': 17, 'loss_train': 1.7900582075623075, 'loss_val': 0.0, 'accuracy_train': 0.9265327695560254, 'accuracy_val': 0.8837209302325582}\n",
      "[2024-05-09 14:50:08,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:50:08,073] [INFO] [timer.py:260:stop] epoch=0/micro_step=2030/global_step=2030, RunningAvgSamplesPerSec=11.706728440767836, CurrSamplesPerSec=17.759253902881092, MemAllocated=1.06GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:50:18,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:50:18,801] [INFO] [timer.py:260:stop] epoch=0/micro_step=2040/global_step=2040, RunningAvgSamplesPerSec=11.723679178555537, CurrSamplesPerSec=17.713110325432112, MemAllocated=1.02GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:50:29,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:50:29,936] [INFO] [timer.py:260:stop] epoch=0/micro_step=2050/global_step=2050, RunningAvgSamplesPerSec=11.738924618597434, CurrSamplesPerSec=16.53929411927076, MemAllocated=1.11GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:50:41,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:50:41,113] [INFO] [timer.py:260:stop] epoch=0/micro_step=2060/global_step=2060, RunningAvgSamplesPerSec=11.753858853705335, CurrSamplesPerSec=17.656872562634153, MemAllocated=1.03GB, MaxMemAllocated=2.96GB\n",
      "[2024-05-09 14:50:52,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=25, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-09 14:50:52,583] [INFO] [timer.py:260:stop] epoch=0/micro_step=2070/global_step=2070, RunningAvgSamplesPerSec=11.767384508321138, CurrSamplesPerSec=17.637331327516147, MemAllocated=1.03GB, MaxMemAllocated=2.96GB\n"
     ]
    }
   ],
   "source": [
    "hist = train_with_hist(model_engine, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3a2a32-9601-4458-82d3-b169d7e74ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_learning_curves(hist):\n",
    "    x_arr = np.arange(len(hist[0])) + 1\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "    ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "    ax.plot(x_arr, hist[3], '--<', label='Validation acc.')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Accuracy', size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6a0c1b-3a25-4cff-96c6-3266913e208e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curves(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a230c-2886-4676-9a87-6bac8b503bb4",
   "metadata": {},
   "source": [
    "### Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c316c-7bce-4176-b1fa-00dd769a6a1a",
   "metadata": {},
   "source": [
    "Checking the results of the test dataset…\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebc0b680-b1de-4aee-bad4-baf7f50b9f0e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.837\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test, _ = evaluate(model, test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef9b09f4-0640-4e59-aefc-9d2d136d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(model, dataloader):\n",
    "    model.eval()\n",
    "    y_test = np.asarray([])\n",
    "    y_predict = np.asarray([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, label) in enumerate(dataloader):\n",
    "            outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "            predicted_label = outputs.logits\n",
    "                  \n",
    "            y_test = np.concatenate((y_test, np.asarray(label.to(device='cpu', dtype=torch.long))), axis=None)\n",
    "            y_predict = np.concatenate((y_predict, np.asarray((predicted_label.argmax(1).to(device='cpu', dtype=torch.long)))), axis=None)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    sns.heatmap(cm, annot=True, fmt = \"d\")\n",
    "    print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2efeb419-1df7-4e6c-8814-ddf3294b9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.90      0.83       120\n",
      "         1.0       0.90      0.78      0.84       143\n",
      "\n",
      "    accuracy                           0.84       263\n",
      "   macro avg       0.84      0.84      0.84       263\n",
      "weighted avg       0.85      0.84      0.84       263\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a457def5-9ac2-48c9-a2ef-69ead17b1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    with torch.no_grad():\n",
    "        encoded_text = tokenizer(text)\n",
    "        encoded_text.input_ids = torch.tensor(encoded_text.input_ids).to(device).unsqueeze(0)\n",
    "        encoded_text.attention_mask = torch.tensor(encoded_text.attention_mask).to(device).unsqueeze(0)\n",
    "\n",
    "        outputs = model(input_ids=encoded_text.input_ids, attention_mask=encoded_text.attention_mask)\n",
    "        predicted_label = outputs.logits\n",
    "        return predicted_label.argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40c741cb-e91b-406f-a1f7-3c0ff292ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 1\n"
     ]
    }
   ],
   "source": [
    "ex_text_str = 'The ePump Software shall define Fault ID 1 as follows:'\n",
    "\n",
    "print(\"This is a %s\" % predict(ex_text_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61cc330f-c045-4453-a430-d19c5cac6bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"The IO Service shall select the XLR-PW DEV_INFO_DATA file if HPP_XLR_WIRING is grounded (logical 1) and bits AC_TYPE_BIT1 - AC_TYPE_BIT6 do not indicate a CFM engine configuration. NOTE: HPP_XLR_WIRING and bits AC_TYPE_BIT[1-6] are discrete inputs which are received on constant pins between hardware configurations. See 282100-ICD-x for more details.\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "587df1de-6144-420e-a789-7771209aeca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"I shall like waffles\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3f924b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"Bumblebe is red\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0c0e6726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"Bumblebee is red\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5f1d9-eeef-4d53-b3a9-abfac4069485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38fa186d-4971-48b2-8048-ba14511e893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 18:50:33,349] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2380 is about to be saved!\n",
      "[2024-05-08 18:50:33,350] [INFO] [engine.py:3596:save_16bit_model] Saving model weights to ./models/mistral/2024-05-08_mistral.pth, tag: global_step2380\n",
      "[2024-05-08 18:50:33,350] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/mistral/2024-05-08_mistral.pth...\n",
      "[2024-05-08 18:50:47,362] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/mistral/2024-05-08_mistral.pth.\n",
      "[2024-05-08 18:50:47,367] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2380 is ready now!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today().isoformat()\n",
    "model_engine.save_16bit_model(\"./models/mistral/\", f\"{today}_mistral.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9afb70f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.embed_tokens.weight',\n",
       "              tensor([[-0.0000e+00,  0.0000e+00, -0.0000e+00,  ..., -0.0000e+00,\n",
       "                       -0.0000e+00, -0.0000e+00],\n",
       "                      [-4.0588e-03,  1.6499e-04, -4.6997e-03,  ..., -1.8597e-04,\n",
       "                       -9.9945e-04,  4.0531e-05],\n",
       "                      [-1.5640e-03,  9.3460e-04,  1.8692e-04,  ...,  1.1749e-03,\n",
       "                        3.3760e-04,  3.3379e-05],\n",
       "                      ...,\n",
       "                      [ 3.9978e-03, -2.3651e-03,  6.6376e-04,  ..., -8.7357e-04,\n",
       "                        6.4697e-03,  1.5335e-03],\n",
       "                      [ 3.4904e-04,  3.8452e-03,  4.0283e-03,  ...,  6.3705e-04,\n",
       "                        2.5635e-03,  1.1902e-03],\n",
       "                      [-3.9673e-03, -9.7656e-04, -5.3711e-03,  ...,  4.1809e-03,\n",
       "                        4.3392e-05, -2.5024e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.q_proj.weight',\n",
       "              tensor([[ 5.3883e-05,  1.2436e-03, -3.0279e-05,  ...,  4.3945e-03,\n",
       "                        2.8491e-05, -6.1417e-04],\n",
       "                      [-4.6730e-05, -3.7193e-04,  2.3246e-05,  ..., -2.7657e-04,\n",
       "                        5.6028e-06,  3.1281e-04],\n",
       "                      [ 1.8120e-05, -5.1498e-04,  7.2718e-06,  ..., -6.0730e-03,\n",
       "                       -2.5392e-05, -4.1199e-04],\n",
       "                      ...,\n",
       "                      [-2.1935e-05, -1.7242e-03, -2.7537e-05,  ...,  4.3869e-04,\n",
       "                        1.1921e-05,  3.5286e-04],\n",
       "                      [ 7.5102e-06, -1.5640e-03,  1.1623e-05,  ..., -2.0599e-03,\n",
       "                       -2.9922e-05,  7.7057e-04],\n",
       "                      [ 2.2769e-05,  1.9836e-03,  2.4915e-05,  ..., -2.3842e-04,\n",
       "                       -1.1563e-05, -4.9210e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.k_proj.weight',\n",
       "              tensor([[-1.5497e-06, -3.3417e-03,  1.2589e-04,  ..., -1.6357e-02,\n",
       "                        1.8120e-04, -8.8120e-04],\n",
       "                      [ 1.6093e-05,  1.9226e-03, -8.0585e-05,  ...,  7.0190e-03,\n",
       "                       -9.0122e-05,  8.5068e-04],\n",
       "                      [-2.0623e-05,  3.0518e-03, -6.4373e-05,  ...,  1.7944e-02,\n",
       "                       -1.0586e-04,  6.1035e-04],\n",
       "                      ...,\n",
       "                      [-2.4605e-04,  1.9150e-03, -4.2152e-04,  ...,  2.1973e-03,\n",
       "                        3.6812e-04, -2.5368e-04],\n",
       "                      [-1.6975e-04, -4.3945e-03, -2.5940e-04,  ...,  6.5613e-04,\n",
       "                        5.4169e-04, -2.2888e-03],\n",
       "                      [ 2.4796e-04, -1.7700e-03,  4.4823e-04,  ..., -1.9379e-03,\n",
       "                       -4.0245e-04,  8.7357e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.v_proj.weight',\n",
       "              tensor([[-4.1962e-04, -1.6785e-03, -6.5231e-04,  ...,  4.8828e-03,\n",
       "                       -9.5963e-06,  2.1362e-03],\n",
       "                      [-5.1117e-04,  5.1270e-03, -3.8147e-04,  ...,  4.4861e-03,\n",
       "                       -7.4005e-04, -7.0953e-04],\n",
       "                      [ 2.7275e-04,  2.0599e-03, -5.9891e-04,  ..., -7.1335e-04,\n",
       "                       -1.0252e-04, -5.5542e-03],\n",
       "                      ...,\n",
       "                      [ 6.1417e-04,  2.1973e-03, -2.9755e-04,  ...,  3.6774e-03,\n",
       "                       -1.7853e-03, -2.2430e-03],\n",
       "                      [-3.5477e-04, -4.7607e-03, -4.2725e-04,  ...,  2.7924e-03,\n",
       "                        7.0572e-04, -6.2180e-04],\n",
       "                      [-6.2561e-04, -2.8839e-03,  5.1880e-04,  ..., -3.7766e-04,\n",
       "                        6.1798e-04,  1.5182e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.o_proj.weight',\n",
       "              tensor([[ 6.7520e-04,  4.4556e-03,  4.2114e-03,  ..., -1.0910e-03,\n",
       "                       -4.2152e-04,  5.0735e-04],\n",
       "                      [ 9.1553e-05, -2.6093e-03, -8.0872e-04,  ...,  6.9427e-04,\n",
       "                       -6.7139e-04, -1.0071e-03],\n",
       "                      [-1.9684e-03,  2.0599e-03, -3.7909e-05,  ...,  1.0071e-03,\n",
       "                        3.4027e-03, -3.0365e-03],\n",
       "                      ...,\n",
       "                      [ 4.1199e-03, -1.3199e-03,  2.1057e-03,  ..., -3.4027e-03,\n",
       "                        1.3885e-03,  5.0964e-03],\n",
       "                      [ 2.8992e-03, -3.8300e-03, -2.0905e-03,  ..., -1.8501e-04,\n",
       "                       -2.5635e-03, -2.7084e-04],\n",
       "                      [ 4.5471e-03, -2.9449e-03, -2.4261e-03,  ...,  7.1335e-04,\n",
       "                        1.6785e-03, -9.0790e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.gate_proj.weight',\n",
       "              tensor([[-4.2114e-03, -9.9182e-04, -1.5259e-03,  ...,  2.7008e-03,\n",
       "                        3.5248e-03,  3.1738e-03],\n",
       "                      [ 3.5706e-03, -6.3324e-04,  5.9128e-04,  ...,  4.3869e-04,\n",
       "                        3.1586e-03,  1.1673e-03],\n",
       "                      [-6.9046e-04,  1.6556e-03,  1.5488e-03,  ..., -2.9907e-03,\n",
       "                       -1.7166e-03, -1.4038e-03],\n",
       "                      ...,\n",
       "                      [ 1.8616e-03, -5.1880e-04, -2.2736e-03,  ...,  5.7697e-05,\n",
       "                       -3.0518e-03,  9.6512e-04],\n",
       "                      [ 2.3804e-03, -4.3030e-03, -4.6349e-04,  ...,  1.2398e-05,\n",
       "                        4.1389e-04,  4.3335e-03],\n",
       "                      [ 2.9144e-03,  6.7520e-04,  4.5776e-03,  ...,  1.5640e-03,\n",
       "                       -2.0905e-03, -3.7537e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.up_proj.weight',\n",
       "              tensor([[-1.7738e-04, -6.4850e-05, -8.1635e-04,  ...,  5.6152e-03,\n",
       "                        3.6469e-03,  3.4904e-04],\n",
       "                      [-4.1199e-03, -1.3199e-03, -1.7471e-03,  ...,  2.1057e-03,\n",
       "                        1.8311e-03, -1.0443e-04],\n",
       "                      [-2.4986e-04,  1.7624e-03, -1.5030e-03,  ...,  1.5259e-03,\n",
       "                       -6.0272e-04, -5.3406e-04],\n",
       "                      ...,\n",
       "                      [-2.8076e-03, -1.5564e-03,  1.6708e-03,  ..., -3.7384e-03,\n",
       "                       -2.7008e-03,  4.3945e-03],\n",
       "                      [ 2.7771e-03, -2.7618e-03, -5.8594e-03,  ...,  1.5926e-04,\n",
       "                       -6.5002e-03,  3.9673e-03],\n",
       "                      [-1.5488e-03, -1.6708e-03,  1.8024e-04,  ...,  1.4572e-03,\n",
       "                       -5.7373e-03,  7.4387e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.down_proj.weight',\n",
       "              tensor([[-2.6398e-03,  7.9155e-05, -4.1771e-04,  ..., -1.6556e-03,\n",
       "                        3.4790e-03, -8.6975e-04],\n",
       "                      [ 1.2665e-03, -4.7607e-03,  2.5787e-03,  ..., -1.3580e-03,\n",
       "                        1.5106e-03, -3.9978e-03],\n",
       "                      [ 5.6458e-03, -7.9956e-03,  2.3804e-03,  ...,  2.9755e-03,\n",
       "                       -5.0049e-03,  4.0283e-03],\n",
       "                      ...,\n",
       "                      [ 5.6152e-03,  2.2278e-03, -1.2054e-03,  ..., -1.0681e-03,\n",
       "                        5.7068e-03, -1.3809e-03],\n",
       "                      [-2.5024e-03,  2.4109e-03, -1.8539e-03,  ...,  6.6757e-04,\n",
       "                        1.0681e-04,  1.7853e-03],\n",
       "                      [-2.6703e-03, -5.3711e-03, -6.8665e-04,  ..., -4.4861e-03,\n",
       "                        7.0572e-04, -1.9531e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.0.input_layernorm.weight',\n",
       "              tensor([-7.5102e-06, -1.1963e-02,  5.1498e-05,  ...,  3.5400e-02,\n",
       "                      -9.2506e-05,  6.5918e-03], dtype=torch.float16)),\n",
       "             ('model.layers.0.post_attention_layernorm.weight',\n",
       "              tensor([0.4180, 0.4121, 0.3945,  ..., 0.4199, 0.3984, 0.4004],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.q_proj.weight',\n",
       "              tensor([[ 2.1267e-04, -1.8768e-03,  3.3975e-06,  ...,  2.2125e-03,\n",
       "                        4.3678e-04,  2.6093e-03],\n",
       "                      [-1.9741e-04, -9.7656e-04,  4.3631e-05,  ..., -1.7548e-03,\n",
       "                        1.7471e-03, -1.5450e-04],\n",
       "                      [ 2.8229e-03, -2.1515e-03, -2.1100e-05,  ...,  2.8381e-03,\n",
       "                       -3.7079e-03,  9.5749e-04],\n",
       "                      ...,\n",
       "                      [ 1.7624e-03, -3.2501e-03,  2.6822e-06,  ..., -2.8839e-03,\n",
       "                       -6.5002e-03, -1.6861e-03],\n",
       "                      [ 2.0599e-03,  5.9204e-03, -1.1921e-07,  ..., -1.4591e-04,\n",
       "                        4.5776e-03,  1.0147e-03],\n",
       "                      [ 5.4932e-03, -2.5940e-03, -2.8610e-06,  ..., -2.9602e-03,\n",
       "                       -6.8359e-03, -2.3193e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.k_proj.weight',\n",
       "              tensor([[-2.3346e-03,  1.0620e-02, -3.6716e-05,  ..., -3.4027e-03,\n",
       "                        4.4823e-04,  2.4109e-03],\n",
       "                      [ 7.0572e-05,  4.1504e-03,  1.0967e-04,  ...,  4.7493e-04,\n",
       "                       -6.7749e-03,  2.7008e-03],\n",
       "                      [-2.1667e-03,  3.5248e-03,  1.2684e-04,  ..., -1.6022e-03,\n",
       "                        1.8005e-03, -4.5013e-04],\n",
       "                      ...,\n",
       "                      [ 2.1515e-03, -1.9531e-03, -7.9274e-06,  ..., -2.0752e-03,\n",
       "                       -1.6861e-03, -1.8005e-03],\n",
       "                      [ 1.8082e-03,  2.4109e-03,  5.9366e-05,  ..., -1.5793e-03,\n",
       "                        2.2430e-03,  3.6469e-03],\n",
       "                      [ 6.9275e-03, -1.0757e-03, -5.2452e-06,  ...,  2.7466e-03,\n",
       "                        1.7776e-03, -9.6893e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.v_proj.weight',\n",
       "              tensor([[-5.1575e-03, -2.9297e-03, -2.0599e-04,  ...,  7.5150e-04,\n",
       "                        4.9591e-04, -1.9836e-03],\n",
       "                      [ 3.6621e-03, -4.1389e-04,  3.0041e-05,  ..., -6.5308e-03,\n",
       "                        4.8256e-04, -3.4180e-03],\n",
       "                      [-7.3624e-04,  1.4114e-03,  1.7262e-04,  ...,  2.3041e-03,\n",
       "                       -2.9564e-04,  2.9449e-03],\n",
       "                      ...,\n",
       "                      [-2.3499e-03, -4.0894e-03,  2.2888e-04,  ..., -3.0060e-03,\n",
       "                        4.4861e-03, -1.1673e-03],\n",
       "                      [ 6.0272e-04, -4.0283e-03, -1.2875e-04,  ...,  1.7285e-05,\n",
       "                       -1.3428e-03,  3.6011e-03],\n",
       "                      [ 1.4210e-04,  1.8387e-03,  2.4986e-04,  ...,  7.0953e-04,\n",
       "                       -1.4114e-03,  1.1520e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0004, -0.0004, -0.0019,  ..., -0.0013,  0.0008,  0.0007],\n",
       "                      [-0.0039, -0.0007, -0.0010,  ..., -0.0030,  0.0016,  0.0016],\n",
       "                      [ 0.0011, -0.0004,  0.0014,  ...,  0.0027, -0.0033, -0.0003],\n",
       "                      ...,\n",
       "                      [-0.0009, -0.0029, -0.0022,  ...,  0.0016, -0.0021,  0.0018],\n",
       "                      [ 0.0003,  0.0013,  0.0005,  ...,  0.0086,  0.0006, -0.0023],\n",
       "                      [-0.0003,  0.0027,  0.0002,  ...,  0.0050, -0.0031, -0.0019]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.gate_proj.weight',\n",
       "              tensor([[-4.4556e-03,  6.0425e-03,  1.9836e-03,  ..., -5.9204e-03,\n",
       "                       -3.5248e-03,  1.3809e-03],\n",
       "                      [-3.8300e-03, -1.4496e-04,  4.7913e-03,  ...,  2.2278e-03,\n",
       "                       -1.9073e-03, -5.8289e-03],\n",
       "                      [ 5.7373e-03, -3.2654e-03,  1.6632e-03,  ...,  5.7983e-04,\n",
       "                        5.6458e-03,  1.2436e-03],\n",
       "                      ...,\n",
       "                      [ 8.6594e-04,  5.8899e-03, -4.5776e-03,  ...,  9.6130e-04,\n",
       "                        1.7624e-03,  5.6458e-04],\n",
       "                      [ 1.7395e-03, -2.2125e-03, -1.2436e-03,  ...,  1.7047e-05,\n",
       "                        7.2479e-04, -3.2501e-03],\n",
       "                      [ 2.0447e-03, -2.2030e-04, -1.3657e-03,  ..., -1.4420e-03,\n",
       "                       -1.2302e-04, -2.1057e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.up_proj.weight',\n",
       "              tensor([[-6.4697e-03,  5.2795e-03,  1.5411e-03,  ..., -1.2741e-03,\n",
       "                        2.1667e-03, -1.8463e-03],\n",
       "                      [ 2.4109e-03,  5.0735e-04,  6.4697e-03,  ...,  1.6937e-03,\n",
       "                        9.3384e-03,  8.4686e-04],\n",
       "                      [-1.1873e-04,  6.9046e-04,  1.6174e-03,  ..., -2.3041e-03,\n",
       "                       -2.5787e-03,  6.2256e-03],\n",
       "                      ...,\n",
       "                      [ 1.4343e-03,  1.7166e-03,  2.3499e-03,  ..., -2.6398e-03,\n",
       "                       -6.5994e-04, -1.6022e-03],\n",
       "                      [ 2.7466e-03,  1.9989e-03, -8.8215e-05,  ...,  1.3046e-03,\n",
       "                       -2.9144e-03, -7.2479e-04],\n",
       "                      [ 1.4782e-04, -5.2643e-04,  4.6921e-04,  ...,  2.6093e-03,\n",
       "                        1.3657e-03,  1.9684e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.down_proj.weight',\n",
       "              tensor([[-1.0452e-03, -3.8300e-03,  4.8637e-05,  ...,  9.3079e-04,\n",
       "                        8.6060e-03, -9.1934e-04],\n",
       "                      [-2.1820e-03,  2.3499e-03, -8.0109e-04,  ...,  5.4016e-03,\n",
       "                       -5.1575e-03,  5.6839e-04],\n",
       "                      [ 4.6997e-03, -2.5787e-03,  5.9891e-04,  ...,  5.7373e-03,\n",
       "                        3.8300e-03,  7.7820e-03],\n",
       "                      ...,\n",
       "                      [-2.2736e-03,  2.1973e-03,  2.7618e-03,  ..., -5.7068e-03,\n",
       "                       -9.6893e-04,  1.6556e-03],\n",
       "                      [-9.4986e-04, -2.4872e-03, -5.5847e-03,  ..., -1.4191e-03,\n",
       "                       -6.4468e-04, -1.6479e-03],\n",
       "                      [-2.7161e-03, -1.9989e-03, -4.1504e-03,  ..., -2.8229e-04,\n",
       "                        2.6855e-03,  3.9673e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.1.input_layernorm.weight',\n",
       "              tensor([1.6504e-01, 1.0596e-01, 1.6093e-06,  ..., 1.7188e-01, 1.0498e-01,\n",
       "                      6.8359e-02], dtype=torch.float16)),\n",
       "             ('model.layers.1.post_attention_layernorm.weight',\n",
       "              tensor([0.6406, 0.6328, 0.6484,  ..., 0.6289, 0.6523, 0.6445],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.q_proj.weight',\n",
       "              tensor([[-3.4027e-03, -1.4954e-03, -2.6703e-03,  ...,  2.3041e-03,\n",
       "                        6.3171e-03, -7.8583e-04],\n",
       "                      [-1.5030e-03,  1.6174e-03, -4.8256e-04,  ..., -2.2411e-04,\n",
       "                        3.0212e-03,  1.5488e-03],\n",
       "                      [ 3.0670e-03, -2.3956e-03,  4.1771e-04,  ...,  1.5182e-03,\n",
       "                        9.3079e-04,  6.7444e-03],\n",
       "                      ...,\n",
       "                      [ 2.3956e-03, -1.1139e-03,  1.4404e-02,  ...,  1.0132e-02,\n",
       "                        6.8054e-03, -7.5378e-03],\n",
       "                      [-6.1646e-03, -1.3657e-03, -1.5869e-03,  ..., -6.8054e-03,\n",
       "                       -2.9297e-03, -4.5166e-03],\n",
       "                      [ 2.0447e-03,  3.8300e-03,  1.3885e-03,  ...,  1.7319e-03,\n",
       "                        7.1526e-05, -1.1475e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.k_proj.weight',\n",
       "              tensor([[ 3.2196e-03, -9.0408e-04,  5.0659e-03,  ..., -4.6158e-04,\n",
       "                       -3.2425e-04, -4.7112e-04],\n",
       "                      [ 1.7014e-03,  5.1575e-03, -4.3945e-03,  ...,  2.3804e-03,\n",
       "                       -3.9062e-03,  3.2043e-03],\n",
       "                      [-2.6855e-03,  3.1853e-04,  3.2959e-03,  ..., -1.6174e-03,\n",
       "                        3.6621e-03, -2.2583e-03],\n",
       "                      ...,\n",
       "                      [ 1.4343e-03, -8.1062e-05,  1.3550e-02,  ...,  1.9684e-03,\n",
       "                        7.5340e-05, -3.5095e-03],\n",
       "                      [ 5.7983e-04,  6.0654e-04,  8.7280e-03,  ..., -6.3477e-03,\n",
       "                       -8.0566e-03,  1.3809e-03],\n",
       "                      [ 6.8970e-03, -2.4567e-03,  5.7983e-03,  ..., -8.4839e-03,\n",
       "                       -4.3640e-03, -1.3855e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.v_proj.weight',\n",
       "              tensor([[-2.5940e-04,  2.3556e-04, -1.7395e-03,  ...,  1.9379e-03,\n",
       "                        6.8665e-03, -3.9673e-03],\n",
       "                      [-1.3657e-03, -1.5163e-04,  1.0681e-03,  ...,  2.5787e-03,\n",
       "                        2.1973e-03, -5.3406e-03],\n",
       "                      [-9.2697e-04,  6.0272e-04, -8.1062e-05,  ...,  2.8992e-03,\n",
       "                        2.8076e-03,  1.8387e-03],\n",
       "                      ...,\n",
       "                      [ 3.6774e-03,  1.8024e-04, -2.7008e-03,  ..., -1.3504e-03,\n",
       "                        1.5488e-03, -1.0071e-03],\n",
       "                      [-2.2583e-03,  2.3804e-03, -5.6458e-04,  ..., -8.8501e-04,\n",
       "                        7.4768e-04, -5.2643e-04],\n",
       "                      [ 3.1891e-03, -2.3651e-03,  1.7853e-03,  ...,  2.9449e-03,\n",
       "                        3.6316e-03, -1.8921e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.o_proj.weight',\n",
       "              tensor([[ 3.9673e-03,  1.0757e-03, -2.1839e-04,  ...,  5.0049e-03,\n",
       "                        1.4648e-03,  1.0910e-03],\n",
       "                      [-1.0910e-03,  6.4468e-04,  2.8229e-03,  ...,  2.5558e-04,\n",
       "                        9.1076e-05,  3.4027e-03],\n",
       "                      [-1.7166e-03,  8.1635e-04,  2.3651e-03,  ..., -5.9509e-03,\n",
       "                        1.6403e-03, -2.1362e-03],\n",
       "                      ...,\n",
       "                      [-1.9836e-04,  1.2741e-03,  3.7842e-03,  ...,  7.0190e-04,\n",
       "                        2.9144e-03,  1.1921e-04],\n",
       "                      [-3.2234e-04,  2.0885e-04, -9.2506e-05,  ..., -2.0905e-03,\n",
       "                       -1.6022e-03, -3.1281e-03],\n",
       "                      [-1.0681e-03, -3.2806e-03, -3.0975e-03,  ...,  3.5095e-03,\n",
       "                        4.5967e-04,  2.5482e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.gate_proj.weight',\n",
       "              tensor([[-5.7678e-03,  1.0910e-03,  1.4725e-03,  ..., -2.5787e-03,\n",
       "                       -1.5717e-03,  1.8539e-03],\n",
       "                      [-1.8501e-04,  4.5166e-03,  5.4626e-03,  ...,  2.2583e-03,\n",
       "                       -1.2436e-03, -2.8839e-03],\n",
       "                      [-1.1063e-03,  1.6556e-03, -4.1809e-03,  ..., -4.3945e-03,\n",
       "                       -1.9073e-04, -6.8054e-03],\n",
       "                      ...,\n",
       "                      [-6.5804e-05,  2.9144e-03, -3.4790e-03,  ..., -1.3885e-03,\n",
       "                       -6.2943e-04,  1.6708e-03],\n",
       "                      [-2.7924e-03,  3.5095e-03,  4.6082e-03,  ...,  1.8692e-03,\n",
       "                       -7.7820e-03,  3.7231e-03],\n",
       "                      [ 2.8076e-03,  5.0354e-03, -1.1253e-04,  ...,  8.7738e-04,\n",
       "                       -1.7471e-03, -6.2256e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.up_proj.weight',\n",
       "              tensor([[ 2.6131e-04,  3.7231e-03, -5.2490e-03,  ..., -2.3460e-04,\n",
       "                       -8.5068e-04, -3.2806e-03],\n",
       "                      [-2.4719e-03, -9.7046e-03, -1.8082e-03,  ...,  5.5313e-04,\n",
       "                       -2.7771e-03, -3.2806e-03],\n",
       "                      [-4.3945e-03,  3.3569e-04, -5.8365e-04,  ..., -4.1580e-04,\n",
       "                       -4.4584e-05, -2.9755e-03],\n",
       "                      ...,\n",
       "                      [ 5.2490e-03, -2.4109e-03, -4.0588e-03,  ..., -2.1973e-03,\n",
       "                       -2.0294e-03, -3.5248e-03],\n",
       "                      [ 3.9673e-03, -6.8359e-03, -2.2278e-03,  ...,  1.5335e-03,\n",
       "                       -4.5776e-03,  6.7444e-03],\n",
       "                      [ 2.8992e-03,  5.2795e-03,  5.1575e-03,  ...,  5.5542e-03,\n",
       "                       -8.0490e-04, -1.0223e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0020,  0.0007, -0.0008,  ...,  0.0047,  0.0016,  0.0031],\n",
       "                      [-0.0067, -0.0076, -0.0015,  ..., -0.0011, -0.0028,  0.0049],\n",
       "                      [ 0.0003, -0.0019, -0.0023,  ..., -0.0027, -0.0004, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0012,  0.0031, -0.0009,  ...,  0.0008,  0.0028,  0.0032],\n",
       "                      [-0.0016, -0.0012,  0.0022,  ...,  0.0022, -0.0035,  0.0029],\n",
       "                      [-0.0027, -0.0060,  0.0028,  ..., -0.0007,  0.0003, -0.0013]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.input_layernorm.weight',\n",
       "              tensor([0.8555, 0.6953, 0.6484,  ..., 0.8711, 0.7578, 0.8320],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.post_attention_layernorm.weight',\n",
       "              tensor([0.8281, 0.8125, 0.8438,  ..., 0.8125, 0.8359, 0.8203],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0008, -0.0011,  0.0020,  ...,  0.0011, -0.0020, -0.0034],\n",
       "                      [-0.0039, -0.0006,  0.0003,  ...,  0.0021,  0.0011, -0.0011],\n",
       "                      [-0.0027,  0.0006,  0.0055,  ...,  0.0050,  0.0007, -0.0037],\n",
       "                      ...,\n",
       "                      [ 0.0009,  0.0020,  0.0033,  ..., -0.0007,  0.0060,  0.0029],\n",
       "                      [ 0.0037, -0.0074, -0.0047,  ...,  0.0042, -0.0029, -0.0036],\n",
       "                      [ 0.0118, -0.0026,  0.0128,  ..., -0.0046, -0.0026, -0.0022]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0020, -0.0021, -0.0022,  ..., -0.0061,  0.0035,  0.0003],\n",
       "                      [-0.0069, -0.0027,  0.0021,  ..., -0.0014,  0.0027, -0.0093],\n",
       "                      [-0.0020,  0.0029,  0.0146,  ...,  0.0031, -0.0014, -0.0110],\n",
       "                      ...,\n",
       "                      [-0.0193,  0.0001, -0.0036,  ...,  0.0098,  0.0025, -0.0020],\n",
       "                      [ 0.0045,  0.0019, -0.0003,  ..., -0.0027,  0.0031, -0.0062],\n",
       "                      [-0.0029,  0.0015, -0.0042,  ..., -0.0003,  0.0001,  0.0010]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0024,  0.0036, -0.0051,  ...,  0.0002,  0.0104,  0.0037],\n",
       "                      [-0.0024,  0.0003,  0.0025,  ..., -0.0009, -0.0016, -0.0036],\n",
       "                      [ 0.0011, -0.0017,  0.0004,  ...,  0.0018,  0.0009, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0017, -0.0002,  0.0030,  ...,  0.0003, -0.0026,  0.0006],\n",
       "                      [-0.0007, -0.0040,  0.0032,  ...,  0.0006,  0.0013, -0.0009],\n",
       "                      [ 0.0002,  0.0012,  0.0035,  ..., -0.0014,  0.0022,  0.0037]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.o_proj.weight',\n",
       "              tensor([[-5.1880e-03,  2.8839e-03, -2.2507e-04,  ..., -2.0905e-03,\n",
       "                       -1.9226e-03,  3.2501e-03],\n",
       "                      [ 5.3711e-03,  5.9509e-03,  9.9182e-04,  ..., -3.7537e-03,\n",
       "                       -4.1504e-03,  4.3335e-03],\n",
       "                      [ 3.1128e-03,  1.2512e-03, -7.6771e-05,  ...,  6.2256e-03,\n",
       "                        4.3030e-03,  4.8828e-04],\n",
       "                      ...,\n",
       "                      [ 1.7929e-03, -3.7079e-03, -9.8419e-04,  ...,  3.1128e-03,\n",
       "                        2.2984e-04, -5.6152e-03],\n",
       "                      [ 2.2888e-03,  5.4626e-03, -7.2956e-05,  ..., -1.4725e-03,\n",
       "                        2.6398e-03, -1.1368e-03],\n",
       "                      [ 8.5449e-03,  6.5002e-03, -1.9989e-03,  ...,  1.9379e-03,\n",
       "                       -1.5564e-03,  1.4496e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0021,  0.0013, -0.0045,  ...,  0.0010,  0.0015,  0.0030],\n",
       "                      [ 0.0019,  0.0050,  0.0026,  ...,  0.0069,  0.0022,  0.0007],\n",
       "                      [-0.0001,  0.0042,  0.0047,  ..., -0.0062,  0.0023,  0.0023],\n",
       "                      ...,\n",
       "                      [-0.0013,  0.0007,  0.0014,  ...,  0.0016,  0.0061,  0.0006],\n",
       "                      [-0.0006, -0.0008, -0.0016,  ..., -0.0011,  0.0019, -0.0012],\n",
       "                      [-0.0042,  0.0025, -0.0013,  ..., -0.0013, -0.0010, -0.0025]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0063,  0.0030, -0.0012,  ...,  0.0014, -0.0008,  0.0013],\n",
       "                      [ 0.0016, -0.0036,  0.0002,  ...,  0.0045,  0.0006,  0.0016],\n",
       "                      [ 0.0045,  0.0017, -0.0026,  ...,  0.0053, -0.0030, -0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0019,  0.0045, -0.0006,  ...,  0.0029, -0.0037,  0.0015],\n",
       "                      [ 0.0038,  0.0040, -0.0013,  ...,  0.0040,  0.0028,  0.0041],\n",
       "                      [-0.0033, -0.0031, -0.0003,  ..., -0.0015,  0.0022,  0.0001]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0010,  0.0051,  0.0018,  ..., -0.0025, -0.0038, -0.0035],\n",
       "                      [ 0.0027,  0.0021,  0.0056,  ...,  0.0013, -0.0010,  0.0017],\n",
       "                      [ 0.0009,  0.0018,  0.0040,  ...,  0.0002, -0.0019, -0.0052],\n",
       "                      ...,\n",
       "                      [-0.0006,  0.0037, -0.0064,  ...,  0.0048,  0.0008, -0.0052],\n",
       "                      [ 0.0025, -0.0003,  0.0035,  ...,  0.0006,  0.0016, -0.0012],\n",
       "                      [ 0.0022, -0.0005, -0.0028,  ..., -0.0012, -0.0034,  0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.input_layernorm.weight',\n",
       "              tensor([0.8906, 0.6562, 0.5977,  ..., 0.8945, 0.6953, 0.7109],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.post_attention_layernorm.weight',\n",
       "              tensor([0.9805, 0.9922, 1.0156,  ..., 0.9883, 0.9805, 0.9883],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.q_proj.weight',\n",
       "              tensor([[-2.2278e-03,  3.1281e-03,  6.7749e-03,  ...,  3.2806e-03,\n",
       "                        1.7090e-03,  3.8452e-03],\n",
       "                      [ 4.7874e-04,  1.7834e-04,  1.2817e-03,  ..., -8.8120e-04,\n",
       "                       -6.5613e-03, -4.3030e-03],\n",
       "                      [-2.5024e-03,  2.3804e-03,  8.6212e-04,  ..., -7.3624e-04,\n",
       "                       -4.1809e-03, -3.9368e-03],\n",
       "                      ...,\n",
       "                      [ 1.7853e-03,  1.3962e-03, -2.5787e-03,  ..., -3.9482e-04,\n",
       "                       -1.5869e-03, -4.1199e-04],\n",
       "                      [ 1.5259e-03, -5.9128e-04, -4.1008e-04,  ..., -2.1362e-03,\n",
       "                       -5.2185e-03, -1.1063e-03],\n",
       "                      [-2.3365e-04, -4.2114e-03,  9.2983e-05,  ...,  5.7983e-03,\n",
       "                       -2.1667e-03, -2.2278e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.k_proj.weight',\n",
       "              tensor([[-2.3193e-03,  4.3640e-03,  3.6774e-03,  ...,  3.7079e-03,\n",
       "                        1.4725e-03,  5.1575e-03],\n",
       "                      [ 2.9144e-03, -6.7749e-03, -1.2131e-03,  ...,  3.0518e-03,\n",
       "                        2.5940e-03, -2.9449e-03],\n",
       "                      [ 6.1340e-03, -1.4572e-03,  2.2888e-03,  ...,  5.1880e-04,\n",
       "                       -6.5231e-04,  5.7983e-03],\n",
       "                      ...,\n",
       "                      [-1.3885e-03, -3.6716e-05,  1.1444e-03,  ...,  3.2997e-04,\n",
       "                       -6.8283e-04, -2.1267e-04],\n",
       "                      [ 5.5542e-03, -6.8359e-03,  8.3618e-03,  ..., -1.3046e-03,\n",
       "                        1.1978e-03, -7.8735e-03],\n",
       "                      [-9.8877e-03, -2.3804e-03, -9.3384e-03,  ..., -7.4158e-03,\n",
       "                       -9.0332e-03,  4.4250e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.v_proj.weight',\n",
       "              tensor([[-2.9755e-04, -1.0147e-03, -1.5030e-03,  ...,  6.2943e-04,\n",
       "                       -4.9973e-04, -8.2779e-04],\n",
       "                      [ 4.5166e-03,  4.3640e-03, -3.8910e-04,  ...,  1.5640e-03,\n",
       "                       -8.6308e-05,  4.7874e-04],\n",
       "                      [ 2.1210e-03, -7.2479e-04, -2.3499e-03,  ...,  6.3324e-04,\n",
       "                       -3.1128e-03, -1.0605e-03],\n",
       "                      ...,\n",
       "                      [-1.8845e-03, -3.4790e-03,  1.9150e-03,  ...,  2.3346e-03,\n",
       "                        5.9891e-04,  1.9226e-03],\n",
       "                      [ 2.1362e-03,  1.9150e-03, -2.0294e-03,  ..., -2.2278e-03,\n",
       "                        2.4414e-03, -4.0283e-03],\n",
       "                      [-4.1389e-04,  1.4572e-03, -6.0272e-04,  ...,  1.3199e-03,\n",
       "                        2.5787e-03, -2.1973e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.3885e-03, -8.2016e-04,  2.7008e-03,  ...,  5.5237e-03,\n",
       "                       -1.2436e-03,  5.3024e-04],\n",
       "                      [ 1.8387e-03,  5.0354e-04, -5.2490e-03,  ...,  7.4387e-04,\n",
       "                       -4.5776e-03, -3.1662e-04],\n",
       "                      [-8.4686e-04, -4.7607e-03,  2.5635e-03,  ...,  2.3804e-03,\n",
       "                        3.9864e-04,  1.2360e-03],\n",
       "                      ...,\n",
       "                      [ 8.6212e-04, -1.4801e-03, -3.1738e-03,  ..., -3.0212e-03,\n",
       "                        4.3335e-03, -1.2696e-05],\n",
       "                      [-3.7842e-03,  1.5335e-03,  6.0120e-03,  ..., -1.8997e-03,\n",
       "                       -2.4719e-03, -1.7471e-03],\n",
       "                      [ 4.0283e-03,  2.4872e-03, -1.7853e-03,  ..., -2.1362e-03,\n",
       "                        3.1891e-03, -3.0365e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0005,  0.0026,  0.0035,  ..., -0.0005, -0.0012,  0.0044],\n",
       "                      [ 0.0039, -0.0005, -0.0012,  ..., -0.0006,  0.0005, -0.0042],\n",
       "                      [-0.0022, -0.0006,  0.0058,  ..., -0.0003, -0.0026,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0052, -0.0037,  0.0036,  ...,  0.0071,  0.0011,  0.0057],\n",
       "                      [-0.0014,  0.0040, -0.0004,  ...,  0.0030,  0.0031,  0.0013],\n",
       "                      [-0.0009,  0.0022, -0.0020,  ..., -0.0008,  0.0038, -0.0010]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.up_proj.weight',\n",
       "              tensor([[-3.9062e-03,  1.9684e-03, -1.9150e-03,  ..., -1.6308e-04,\n",
       "                       -9.3384e-03, -3.7079e-03],\n",
       "                      [ 3.6011e-03,  5.7983e-03, -2.0905e-03,  ...,  8.7738e-04,\n",
       "                        3.9339e-05, -9.8419e-04],\n",
       "                      [ 2.1667e-03,  1.6479e-03,  1.8311e-03,  ...,  1.2741e-03,\n",
       "                       -1.4038e-03, -4.4556e-03],\n",
       "                      ...,\n",
       "                      [ 1.7319e-03, -9.4986e-04, -1.2741e-03,  ..., -1.5182e-03,\n",
       "                        1.7548e-03,  2.1820e-03],\n",
       "                      [ 3.7537e-03,  1.3199e-03, -4.2534e-04,  ...,  1.0529e-03,\n",
       "                       -3.2501e-03,  5.0964e-03],\n",
       "                      [ 2.1515e-03, -2.4872e-03, -1.9550e-04,  ..., -3.5400e-03,\n",
       "                        3.4485e-03, -1.6632e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.down_proj.weight',\n",
       "              tensor([[-2.4109e-03,  4.4556e-03,  2.7618e-03,  ...,  1.4267e-03,\n",
       "                        3.5048e-05, -1.8845e-03],\n",
       "                      [-4.2152e-04, -9.9182e-04, -5.0735e-04,  ...,  7.0572e-04,\n",
       "                        2.5482e-03, -7.2098e-04],\n",
       "                      [-6.3171e-03, -1.7166e-03,  7.0801e-03,  ...,  1.6327e-03,\n",
       "                       -1.0452e-03, -2.3499e-03],\n",
       "                      ...,\n",
       "                      [-3.3569e-03,  2.3365e-05, -8.6594e-04,  ..., -5.1117e-04,\n",
       "                        1.6022e-04, -1.2283e-03],\n",
       "                      [-7.9346e-03, -2.6398e-03,  2.8839e-03,  ...,  1.4954e-03,\n",
       "                       -5.3101e-03, -4.3488e-04],\n",
       "                      [-2.8839e-03,  9.9182e-04, -1.3733e-03,  ...,  2.8076e-03,\n",
       "                        1.9836e-03,  1.7242e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.input_layernorm.weight',\n",
       "              tensor([1.1406, 0.8984, 0.8984,  ..., 1.1562, 0.9531, 0.8359],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.post_attention_layernorm.weight',\n",
       "              tensor([1.0391, 1.0625, 1.0781,  ..., 1.0391, 1.0547, 1.0625],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.q_proj.weight',\n",
       "              tensor([[-6.4850e-05, -5.1270e-03, -1.9989e-03,  ...,  3.8528e-04,\n",
       "                        1.6556e-03,  5.6028e-05],\n",
       "                      [ 2.8687e-03,  1.5259e-03, -3.5095e-03,  ..., -3.1891e-03,\n",
       "                        5.3787e-04, -2.2125e-03],\n",
       "                      [-3.2234e-04,  4.8828e-03,  2.1667e-03,  ..., -4.0283e-03,\n",
       "                        3.4332e-03,  1.7738e-04],\n",
       "                      ...,\n",
       "                      [ 1.7319e-03,  6.6833e-03, -5.1880e-03,  ..., -5.1575e-03,\n",
       "                        4.2725e-03, -9.4604e-04],\n",
       "                      [ 1.0910e-03,  2.9297e-03, -2.9755e-03,  ...,  4.9438e-03,\n",
       "                       -1.7471e-03,  1.0605e-03],\n",
       "                      [-4.5395e-04,  5.7220e-04, -5.1575e-03,  ...,  3.4180e-03,\n",
       "                        1.1978e-03, -1.6251e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0038, -0.0008,  0.0030,  ...,  0.0052, -0.0022, -0.0009],\n",
       "                      [ 0.0049,  0.0030, -0.0004,  ...,  0.0007,  0.0024, -0.0060],\n",
       "                      [-0.0035, -0.0010,  0.0021,  ...,  0.0061, -0.0087, -0.0088],\n",
       "                      ...,\n",
       "                      [ 0.0032, -0.0091,  0.0082,  ..., -0.0045,  0.0027, -0.0059],\n",
       "                      [ 0.0036,  0.0090, -0.0080,  ...,  0.0010, -0.0007,  0.0063],\n",
       "                      [ 0.0033,  0.0079, -0.0032,  ...,  0.0045, -0.0038,  0.0037]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0012, -0.0001, -0.0024,  ...,  0.0021, -0.0059, -0.0006],\n",
       "                      [ 0.0023, -0.0038,  0.0015,  ..., -0.0005, -0.0005, -0.0034],\n",
       "                      [ 0.0052,  0.0029, -0.0007,  ...,  0.0002,  0.0007, -0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0027, -0.0041,  0.0018,  ...,  0.0021, -0.0025,  0.0002],\n",
       "                      [-0.0029,  0.0003,  0.0017,  ..., -0.0006,  0.0021,  0.0025],\n",
       "                      [ 0.0027, -0.0029, -0.0007,  ..., -0.0010,  0.0006,  0.0003]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.o_proj.weight',\n",
       "              tensor([[ 3.0975e-03,  1.0223e-03, -6.8283e-04,  ...,  5.6839e-04,\n",
       "                        2.9755e-03,  3.9062e-03],\n",
       "                      [ 6.0654e-04, -6.1417e-04,  2.4414e-03,  ...,  1.7014e-03,\n",
       "                       -3.8757e-03,  3.9368e-03],\n",
       "                      [-2.7618e-03, -1.1292e-03,  2.5940e-03,  ..., -2.8229e-03,\n",
       "                       -1.2994e-05,  3.7689e-03],\n",
       "                      ...,\n",
       "                      [ 6.3324e-04, -1.4648e-03, -3.1281e-03,  ..., -2.5635e-03,\n",
       "                       -3.0212e-03, -5.3406e-03],\n",
       "                      [ 2.0142e-03,  3.0899e-04, -9.5367e-04,  ...,  1.6785e-03,\n",
       "                        1.1826e-03,  2.6245e-03],\n",
       "                      [ 4.6082e-03,  7.1411e-03,  5.5542e-03,  ..., -2.3346e-03,\n",
       "                        2.1057e-03,  3.9673e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.gate_proj.weight',\n",
       "              tensor([[ 1.1873e-04, -5.0049e-03, -2.8229e-03,  ..., -4.7913e-03,\n",
       "                        3.7994e-03, -1.7624e-03],\n",
       "                      [ 3.4485e-03,  3.6621e-03, -5.7602e-04,  ...,  2.3804e-03,\n",
       "                       -9.3842e-04,  3.7842e-03],\n",
       "                      [-8.9722e-03, -3.2654e-03,  1.0559e-02,  ...,  1.1902e-03,\n",
       "                       -1.6098e-03,  4.1504e-03],\n",
       "                      ...,\n",
       "                      [-1.7014e-03,  6.5613e-04,  5.3787e-04,  ...,  5.7068e-03,\n",
       "                        6.2180e-04, -2.0218e-04],\n",
       "                      [ 6.6280e-05, -1.1673e-03,  2.5787e-03,  ..., -2.5177e-03,\n",
       "                        8.9645e-04,  3.7384e-03],\n",
       "                      [-4.6082e-03, -5.9204e-03,  7.2861e-04,  ...,  8.2016e-04,\n",
       "                       -7.0953e-04, -3.4180e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.up_proj.weight',\n",
       "              tensor([[-1.4267e-03, -2.1210e-03,  8.2016e-04,  ..., -8.2779e-04,\n",
       "                        4.2725e-03,  4.0627e-04],\n",
       "                      [ 1.0223e-03,  6.5918e-03, -2.6855e-03,  ..., -1.3351e-03,\n",
       "                       -3.6316e-03,  1.5869e-03],\n",
       "                      [-4.2343e-04, -2.1362e-03, -5.2214e-05,  ...,  2.8229e-03,\n",
       "                       -1.1368e-03,  1.4038e-03],\n",
       "                      ...,\n",
       "                      [ 2.4261e-03,  4.1389e-04,  2.1820e-03,  ...,  5.9814e-03,\n",
       "                       -6.2180e-04,  7.9346e-04],\n",
       "                      [-1.6174e-03,  5.3406e-03,  7.1411e-03,  ..., -4.5166e-03,\n",
       "                        8.5449e-04, -2.3346e-03],\n",
       "                      [ 2.3346e-03, -2.0695e-04,  2.5635e-03,  ..., -1.9073e-03,\n",
       "                       -2.1667e-03, -8.3923e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.down_proj.weight',\n",
       "              tensor([[ 1.8234e-03, -1.6861e-03,  1.5259e-03,  ..., -2.4109e-03,\n",
       "                       -1.2817e-03, -3.9062e-03],\n",
       "                      [ 2.3804e-03, -1.4648e-03, -5.1880e-04,  ..., -2.1515e-03,\n",
       "                        9.0332e-03,  7.2098e-04],\n",
       "                      [-3.3264e-03, -2.9907e-03,  4.8828e-03,  ...,  2.2888e-03,\n",
       "                        3.9978e-03,  1.7929e-03],\n",
       "                      ...,\n",
       "                      [ 4.3640e-03, -2.4719e-03,  2.3937e-04,  ..., -1.5488e-03,\n",
       "                       -3.5706e-03,  4.8828e-03],\n",
       "                      [ 3.6469e-03,  4.3640e-03,  1.8921e-03,  ..., -2.0905e-03,\n",
       "                        1.2589e-04,  2.2278e-03],\n",
       "                      [ 1.5488e-03, -3.2349e-03,  5.6028e-06,  ...,  1.5869e-03,\n",
       "                       -3.2043e-03, -2.2125e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.5.input_layernorm.weight',\n",
       "              tensor([1.5000, 1.1016, 1.1016,  ..., 1.3594, 1.2031, 1.1953],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.post_attention_layernorm.weight',\n",
       "              tensor([1.1797, 1.2344, 1.2266,  ..., 1.2109, 1.2031, 1.2109],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0010,  0.0095, -0.0002,  ..., -0.0064, -0.0039, -0.0045],\n",
       "                      [-0.0045,  0.0022,  0.0060,  ...,  0.0011, -0.0007,  0.0014],\n",
       "                      [ 0.0035, -0.0087, -0.0045,  ...,  0.0023,  0.0023,  0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0035, -0.0001,  0.0014,  ...,  0.0031, -0.0015,  0.0022],\n",
       "                      [-0.0006, -0.0033,  0.0024,  ...,  0.0069, -0.0017, -0.0050],\n",
       "                      [ 0.0040,  0.0019, -0.0067,  ..., -0.0090,  0.0052,  0.0068]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.4648e-03,  1.4114e-03,  2.7924e-03,  ..., -5.5237e-03,\n",
       "                       -3.2806e-03,  4.8828e-03],\n",
       "                      [ 1.0986e-02, -1.9226e-03,  2.7466e-03,  ...,  6.7711e-05,\n",
       "                       -1.6861e-03, -1.1444e-03],\n",
       "                      [-4.0283e-03, -2.9755e-04,  1.5106e-03,  ..., -3.3569e-04,\n",
       "                       -1.8387e-03, -1.3046e-03],\n",
       "                      ...,\n",
       "                      [ 2.1820e-03,  4.8828e-03, -4.0894e-03,  ...,  5.0049e-03,\n",
       "                        4.4250e-03, -3.4943e-03],\n",
       "                      [ 1.8539e-03, -9.7656e-03, -4.6387e-03,  ..., -8.6594e-04,\n",
       "                       -1.5411e-03,  2.3956e-03],\n",
       "                      [ 4.2725e-03, -1.5793e-03, -4.7607e-03,  ...,  1.3123e-02,\n",
       "                       -1.4114e-03, -1.2268e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0009, -0.0041,  0.0012,  ...,  0.0006,  0.0047, -0.0008],\n",
       "                      [ 0.0009, -0.0003, -0.0014,  ...,  0.0021,  0.0003, -0.0029],\n",
       "                      [-0.0016,  0.0013, -0.0019,  ...,  0.0008,  0.0027,  0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0013,  0.0004,  0.0056,  ...,  0.0020, -0.0047,  0.0004],\n",
       "                      [-0.0036,  0.0054, -0.0022,  ...,  0.0004, -0.0035, -0.0002],\n",
       "                      [ 0.0015, -0.0045, -0.0008,  ...,  0.0017, -0.0037, -0.0020]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.o_proj.weight',\n",
       "              tensor([[-4.6082e-03, -1.2207e-03, -6.5994e-04,  ..., -3.4094e-05,\n",
       "                       -6.3171e-03,  4.7874e-04],\n",
       "                      [ 3.0060e-03, -2.6093e-03,  1.8005e-03,  ..., -1.6632e-03,\n",
       "                       -2.4605e-04,  1.4191e-03],\n",
       "                      [-1.1520e-03, -3.1128e-03, -1.9455e-03,  ...,  1.3504e-03,\n",
       "                       -1.2207e-04, -6.7520e-04],\n",
       "                      ...,\n",
       "                      [-6.0272e-04,  2.4109e-03, -6.1646e-03,  ...,  7.6675e-04,\n",
       "                        3.9062e-03,  9.1553e-04],\n",
       "                      [ 1.2875e-04,  4.6349e-04, -3.3264e-03,  ..., -6.2866e-03,\n",
       "                       -9.9945e-04, -2.5024e-03],\n",
       "                      [-7.9727e-04,  7.3853e-03,  8.3923e-04,  ..., -9.9182e-04,\n",
       "                       -9.9945e-04, -2.0294e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0001,  0.0041,  0.0012,  ..., -0.0053,  0.0022, -0.0002],\n",
       "                      [-0.0006, -0.0010, -0.0006,  ..., -0.0050, -0.0054,  0.0006],\n",
       "                      [ 0.0014,  0.0015, -0.0047,  ...,  0.0081,  0.0009,  0.0022],\n",
       "                      ...,\n",
       "                      [-0.0009, -0.0040, -0.0030,  ..., -0.0044,  0.0072, -0.0017],\n",
       "                      [-0.0037, -0.0024,  0.0048,  ..., -0.0024,  0.0061, -0.0007],\n",
       "                      [-0.0008, -0.0033,  0.0059,  ...,  0.0001, -0.0050, -0.0015]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.up_proj.weight',\n",
       "              tensor([[ 5.3787e-04, -3.3569e-03,  4.8218e-03,  ..., -4.2152e-04,\n",
       "                        6.6833e-03, -2.1820e-03],\n",
       "                      [-1.3428e-03,  2.7618e-03, -4.9744e-03,  ..., -2.0752e-03,\n",
       "                        2.1577e-05,  5.6076e-04],\n",
       "                      [ 2.1515e-03, -1.1978e-03,  6.4697e-03,  ...,  7.0572e-04,\n",
       "                        7.4387e-04,  1.2512e-03],\n",
       "                      ...,\n",
       "                      [ 1.7643e-04, -2.8381e-03,  2.4414e-03,  ..., -1.1520e-03,\n",
       "                       -1.9226e-03, -1.4954e-03],\n",
       "                      [-2.1057e-03, -8.2397e-04,  2.8839e-03,  ..., -2.8687e-03,\n",
       "                        1.7166e-03, -2.2583e-03],\n",
       "                      [-3.0975e-03,  4.0588e-03, -6.7749e-03,  ..., -3.9062e-03,\n",
       "                        5.6458e-04,  1.8234e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.down_proj.weight',\n",
       "              tensor([[ 2.0447e-03,  1.0605e-03,  3.2806e-03,  ..., -2.8076e-03,\n",
       "                       -2.5330e-03, -1.7643e-04],\n",
       "                      [-8.8692e-05,  2.3651e-03,  4.9438e-03,  ...,  2.5330e-03,\n",
       "                       -4.6921e-04,  4.0894e-03],\n",
       "                      [ 3.3264e-03, -1.2894e-03,  4.2725e-03,  ...,  3.5095e-03,\n",
       "                        2.7161e-03,  2.0447e-03],\n",
       "                      ...,\n",
       "                      [ 2.9945e-04, -4.6997e-03,  1.1673e-03,  ...,  4.7913e-03,\n",
       "                       -1.2436e-03, -3.6011e-03],\n",
       "                      [ 4.8828e-03,  2.1820e-03,  4.4556e-03,  ..., -1.2512e-03,\n",
       "                       -6.5231e-04,  5.0659e-03],\n",
       "                      [-3.0823e-03, -9.3460e-05, -1.2040e-05,  ..., -1.7357e-04,\n",
       "                        1.0681e-03, -1.5640e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.6.input_layernorm.weight',\n",
       "              tensor([1.2266, 1.0625, 1.0312,  ..., 1.1641, 1.1562, 1.0703],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.post_attention_layernorm.weight',\n",
       "              tensor([1.2578, 1.3359, 1.3438,  ..., 1.2734, 1.2969, 1.2891],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0018,  0.0004, -0.0003,  ...,  0.0011, -0.0024,  0.0013],\n",
       "                      [-0.0034, -0.0047,  0.0037,  ..., -0.0022,  0.0029, -0.0056],\n",
       "                      [ 0.0022,  0.0033,  0.0022,  ...,  0.0011,  0.0022, -0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0114, -0.0064,  0.0032,  ...,  0.0015,  0.0006, -0.0080],\n",
       "                      [-0.0132,  0.0010,  0.0055,  ..., -0.0031, -0.0065, -0.0028],\n",
       "                      [ 0.0027, -0.0114, -0.0020,  ..., -0.0016,  0.0116, -0.0061]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0010, -0.0004,  0.0031,  ..., -0.0020, -0.0036, -0.0025],\n",
       "                      [ 0.0028,  0.0014, -0.0028,  ..., -0.0017,  0.0027,  0.0025],\n",
       "                      [ 0.0011, -0.0026, -0.0028,  ...,  0.0003,  0.0023, -0.0001],\n",
       "                      ...,\n",
       "                      [-0.0054, -0.0056, -0.0023,  ..., -0.0028,  0.0062, -0.0081],\n",
       "                      [ 0.0111,  0.0007, -0.0063,  ..., -0.0098, -0.0054,  0.0075],\n",
       "                      [-0.0112,  0.0005,  0.0088,  ...,  0.0130,  0.0107,  0.0110]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.v_proj.weight',\n",
       "              tensor([[-2.4567e-03,  2.7924e-03, -1.7834e-04,  ..., -1.2665e-03,\n",
       "                       -8.5831e-04, -1.7700e-03],\n",
       "                      [ 9.0790e-04,  5.4169e-04,  2.9564e-04,  ..., -2.4319e-04,\n",
       "                       -8.6212e-04,  5.9128e-04],\n",
       "                      [-7.0095e-05,  6.5565e-06,  1.2054e-03,  ...,  7.1716e-04,\n",
       "                       -1.4648e-03, -1.6632e-03],\n",
       "                      ...,\n",
       "                      [-3.2806e-04, -8.6060e-03,  1.1215e-03,  ...,  4.2419e-03,\n",
       "                        4.5776e-03,  1.2589e-03],\n",
       "                      [ 1.8463e-03,  5.7678e-03, -1.0376e-03,  ...,  4.1389e-04,\n",
       "                       -1.0347e-04, -3.7231e-03],\n",
       "                      [ 4.5166e-03, -1.6937e-03, -9.1934e-04,  ...,  1.1139e-03,\n",
       "                        2.3041e-03,  1.8463e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.o_proj.weight',\n",
       "              tensor([[-5.0354e-04,  4.1008e-04, -1.4305e-04,  ..., -5.7459e-05,\n",
       "                        4.1809e-03, -2.1362e-03],\n",
       "                      [ 5.4016e-03, -5.4932e-04,  1.1826e-04,  ..., -3.1281e-03,\n",
       "                        1.6937e-03,  2.6398e-03],\n",
       "                      [ 9.9945e-04,  1.3657e-03,  2.3346e-03,  ..., -3.4637e-03,\n",
       "                        1.5640e-03, -3.2616e-04],\n",
       "                      ...,\n",
       "                      [-1.8921e-03,  1.4572e-03, -3.2043e-04,  ...,  1.1597e-03,\n",
       "                       -3.1891e-03, -5.7602e-04],\n",
       "                      [-1.4954e-03, -5.6763e-03,  5.9891e-04,  ...,  3.6621e-03,\n",
       "                       -5.5237e-03, -6.9046e-04],\n",
       "                      [-9.6512e-04,  4.1199e-04, -2.6855e-03,  ...,  1.9531e-03,\n",
       "                       -1.7853e-03, -2.1839e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.gate_proj.weight',\n",
       "              tensor([[-2.2125e-03, -4.3640e-03, -7.8735e-03,  ...,  1.0681e-03,\n",
       "                        2.9802e-05, -4.7302e-04],\n",
       "                      [-7.0572e-04, -4.8218e-03,  6.1417e-04,  ..., -3.4027e-03,\n",
       "                       -4.5471e-03,  1.1292e-03],\n",
       "                      [ 4.3640e-03,  3.9062e-03, -1.1169e-02,  ...,  1.4572e-03,\n",
       "                        2.8992e-04,  2.6550e-03],\n",
       "                      ...,\n",
       "                      [ 4.6082e-03, -5.2795e-03,  3.5553e-03,  ...,  2.6398e-03,\n",
       "                        1.0757e-03,  2.7008e-03],\n",
       "                      [ 2.9144e-03, -3.8605e-03,  1.8921e-03,  ...,  5.7068e-03,\n",
       "                       -2.1973e-03, -2.4414e-03],\n",
       "                      [ 1.2817e-03, -2.9449e-03, -1.4267e-03,  ..., -9.4604e-04,\n",
       "                       -7.0801e-03, -1.7929e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.up_proj.weight',\n",
       "              tensor([[-2.4261e-03,  4.1008e-04,  2.0885e-04,  ...,  3.7231e-03,\n",
       "                       -1.9455e-03, -1.5717e-03],\n",
       "                      [ 4.8218e-03,  2.6398e-03,  3.2654e-03,  ..., -1.1902e-03,\n",
       "                        8.2493e-05, -1.4114e-03],\n",
       "                      [-3.8757e-03,  7.3853e-03, -1.1292e-03,  ...,  2.2583e-03,\n",
       "                        1.8539e-03,  1.8082e-03],\n",
       "                      ...,\n",
       "                      [ 5.4321e-03,  5.7220e-04,  4.9438e-03,  ...,  9.7752e-05,\n",
       "                       -6.4392e-03, -1.2360e-03],\n",
       "                      [-4.7302e-03,  3.2349e-03, -4.7493e-04,  ..., -3.5858e-03,\n",
       "                       -6.7520e-04, -7.5531e-04],\n",
       "                      [ 2.1820e-03,  1.4877e-03, -1.6479e-03,  ...,  1.0681e-04,\n",
       "                        4.3335e-03,  7.1335e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.down_proj.weight',\n",
       "              tensor([[-0.0009,  0.0039, -0.0024,  ...,  0.0057, -0.0013,  0.0043],\n",
       "                      [-0.0014, -0.0058, -0.0018,  ...,  0.0022, -0.0002,  0.0001],\n",
       "                      [ 0.0005,  0.0023,  0.0012,  ...,  0.0067,  0.0020, -0.0017],\n",
       "                      ...,\n",
       "                      [ 0.0058, -0.0021, -0.0025,  ...,  0.0018, -0.0012, -0.0007],\n",
       "                      [ 0.0045, -0.0006, -0.0006,  ..., -0.0033, -0.0009, -0.0037],\n",
       "                      [-0.0044,  0.0082, -0.0017,  ..., -0.0063, -0.0004, -0.0022]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.input_layernorm.weight',\n",
       "              tensor([1.3438, 1.2188, 1.2891,  ..., 1.2812, 1.3359, 1.2188],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.post_attention_layernorm.weight',\n",
       "              tensor([1.3672, 1.5234, 1.4922,  ..., 1.3594, 1.3906, 1.3984],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0028, -0.0022, -0.0039,  ...,  0.0004, -0.0029, -0.0006],\n",
       "                      [ 0.0008,  0.0015,  0.0009,  ...,  0.0036,  0.0004,  0.0025],\n",
       "                      [ 0.0007, -0.0022, -0.0007,  ..., -0.0018, -0.0020, -0.0024],\n",
       "                      ...,\n",
       "                      [-0.0018,  0.0022, -0.0015,  ...,  0.0015,  0.0006,  0.0051],\n",
       "                      [ 0.0049,  0.0023,  0.0002,  ..., -0.0046, -0.0015, -0.0063],\n",
       "                      [-0.0004,  0.0015,  0.0038,  ...,  0.0034, -0.0019,  0.0009]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.1826e-03,  4.2915e-04, -7.0572e-04,  ..., -4.3335e-03,\n",
       "                        3.2501e-03,  1.9073e-03],\n",
       "                      [ 6.5327e-05,  1.0834e-03, -3.0518e-03,  ...,  5.4121e-05,\n",
       "                       -4.0894e-03, -2.3499e-03],\n",
       "                      [-1.4191e-03, -3.7193e-04,  2.1667e-03,  ..., -1.8311e-03,\n",
       "                        3.2806e-03,  4.0283e-03],\n",
       "                      ...,\n",
       "                      [ 4.7607e-03,  2.3193e-03, -3.0518e-03,  ...,  2.6703e-03,\n",
       "                       -1.0147e-03,  2.6703e-04],\n",
       "                      [-1.0910e-03,  6.5002e-03,  3.6774e-03,  ...,  2.0447e-03,\n",
       "                       -2.8534e-03, -5.3406e-03],\n",
       "                      [-8.2397e-04, -2.2583e-03,  3.5553e-03,  ...,  2.8992e-03,\n",
       "                        1.9455e-03,  4.4556e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.v_proj.weight',\n",
       "              tensor([[-3.2806e-03,  1.9989e-03,  1.1749e-03,  ..., -2.6398e-03,\n",
       "                       -4.8828e-04, -3.9062e-03],\n",
       "                      [ 3.2425e-05, -1.7319e-03, -2.1820e-03,  ..., -3.0365e-03,\n",
       "                       -2.0447e-03,  1.4877e-03],\n",
       "                      [-3.0212e-03, -3.6812e-04,  2.2793e-04,  ..., -2.4986e-04,\n",
       "                       -1.2360e-03, -2.7275e-04],\n",
       "                      ...,\n",
       "                      [ 1.5335e-03, -1.5640e-03, -1.6785e-04,  ...,  5.6458e-04,\n",
       "                        8.0109e-04, -2.8687e-03],\n",
       "                      [ 8.5831e-04,  1.5259e-04, -1.7762e-05,  ...,  1.1520e-03,\n",
       "                       -1.4038e-03,  3.0823e-03],\n",
       "                      [-1.9989e-03,  4.6692e-03,  1.6174e-03,  ..., -2.5940e-03,\n",
       "                       -2.8992e-04, -2.5330e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.o_proj.weight',\n",
       "              tensor([[-4.6730e-04, -1.0300e-03, -2.9449e-03,  ...,  2.8229e-03,\n",
       "                       -7.2098e-04,  1.2894e-03],\n",
       "                      [-4.3335e-03,  7.2861e-04,  8.8882e-04,  ..., -1.4038e-03,\n",
       "                       -2.1973e-03,  9.4986e-04],\n",
       "                      [-2.8839e-03,  4.2534e-04, -1.5869e-03,  ..., -1.2970e-03,\n",
       "                        4.0436e-04,  2.0599e-03],\n",
       "                      ...,\n",
       "                      [-4.6921e-04,  4.2343e-04,  3.4904e-04,  ...,  1.5564e-03,\n",
       "                        5.1022e-05,  2.1667e-03],\n",
       "                      [-2.6245e-03, -4.3106e-04,  1.9531e-03,  ..., -2.3804e-03,\n",
       "                        2.9564e-04, -5.2490e-03],\n",
       "                      [ 8.2016e-04, -1.9073e-03,  1.1597e-03,  ...,  9.3842e-04,\n",
       "                       -1.7014e-03,  1.9989e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0008,  0.0030,  0.0014,  ...,  0.0060,  0.0020, -0.0018],\n",
       "                      [ 0.0047, -0.0018,  0.0009,  ..., -0.0064, -0.0025, -0.0031],\n",
       "                      [-0.0042, -0.0009,  0.0028,  ..., -0.0040,  0.0012, -0.0033],\n",
       "                      ...,\n",
       "                      [-0.0031,  0.0018, -0.0042,  ..., -0.0028, -0.0004, -0.0025],\n",
       "                      [-0.0003, -0.0017, -0.0026,  ...,  0.0008,  0.0002,  0.0049],\n",
       "                      [ 0.0031, -0.0019,  0.0010,  ..., -0.0025, -0.0046, -0.0069]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0030, -0.0003,  0.0029,  ..., -0.0010,  0.0022, -0.0014],\n",
       "                      [ 0.0007, -0.0049,  0.0012,  ...,  0.0034, -0.0029, -0.0020],\n",
       "                      [-0.0056, -0.0011,  0.0003,  ...,  0.0013,  0.0021,  0.0008],\n",
       "                      ...,\n",
       "                      [-0.0047,  0.0028,  0.0042,  ...,  0.0051,  0.0045, -0.0005],\n",
       "                      [-0.0050,  0.0025,  0.0020,  ..., -0.0002, -0.0027,  0.0021],\n",
       "                      [-0.0005,  0.0026, -0.0018,  ..., -0.0025, -0.0009,  0.0083]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.down_proj.weight',\n",
       "              tensor([[-0.0017, -0.0014, -0.0023,  ..., -0.0042, -0.0020, -0.0022],\n",
       "                      [-0.0040, -0.0008, -0.0038,  ...,  0.0032, -0.0003,  0.0042],\n",
       "                      [ 0.0054,  0.0019,  0.0014,  ...,  0.0035, -0.0039, -0.0072],\n",
       "                      ...,\n",
       "                      [-0.0021,  0.0053, -0.0009,  ...,  0.0046, -0.0008, -0.0028],\n",
       "                      [-0.0019,  0.0043,  0.0022,  ..., -0.0014, -0.0016, -0.0024],\n",
       "                      [-0.0034, -0.0008,  0.0022,  ..., -0.0014, -0.0017,  0.0060]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.input_layernorm.weight',\n",
       "              tensor([1.3438, 1.2969, 1.4609,  ..., 1.2188, 1.4297, 1.2422],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.post_attention_layernorm.weight',\n",
       "              tensor([1.4062, 1.6328, 1.6250,  ..., 1.3906, 1.4609, 1.5000],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.0757e-03,  2.4719e-03, -2.3556e-04,  ..., -2.9602e-03,\n",
       "                       -6.3171e-03, -7.7209e-03],\n",
       "                      [-5.2643e-04, -3.2501e-03,  3.0823e-03,  ..., -3.9673e-03,\n",
       "                        4.0283e-03, -3.9978e-03],\n",
       "                      [-1.1063e-03,  5.6152e-03, -2.6703e-04,  ..., -2.0142e-03,\n",
       "                       -3.8300e-03,  8.8692e-05],\n",
       "                      ...,\n",
       "                      [-1.2970e-03, -7.4463e-03, -5.0354e-03,  ...,  3.8147e-04,\n",
       "                        9.0332e-03, -8.4839e-03],\n",
       "                      [ 4.7913e-03,  1.6357e-02, -3.7994e-03,  ...,  4.8828e-03,\n",
       "                        2.4261e-03, -3.6011e-03],\n",
       "                      [ 5.8289e-03, -3.5248e-03, -3.1128e-03,  ...,  1.9531e-03,\n",
       "                       -9.5215e-03,  5.5695e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.3885e-03,  8.9264e-04,  1.0223e-03,  ...,  2.3804e-03,\n",
       "                       -2.3651e-03,  3.9062e-03],\n",
       "                      [-3.3112e-03,  4.7684e-04, -6.8283e-04,  ..., -6.5613e-04,\n",
       "                        5.3406e-03,  1.7834e-04],\n",
       "                      [ 1.0300e-03,  4.7874e-04,  1.7166e-03,  ...,  3.8719e-04,\n",
       "                       -8.5235e-06,  1.6479e-03],\n",
       "                      ...,\n",
       "                      [-5.4626e-03,  5.1575e-03, -1.9531e-03,  ...,  6.3171e-03,\n",
       "                        3.2425e-04, -6.3782e-03],\n",
       "                      [-1.9684e-03,  9.8267e-03,  2.2278e-03,  ...,  1.0071e-02,\n",
       "                        2.6321e-04, -3.4332e-03],\n",
       "                      [-3.3264e-03, -8.5449e-03,  3.6926e-03,  ...,  4.2725e-03,\n",
       "                       -1.4648e-02,  1.7624e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.v_proj.weight',\n",
       "              tensor([[-5.0659e-03, -1.5182e-03,  2.2278e-03,  ..., -1.6937e-03,\n",
       "                       -2.3499e-03,  1.4267e-03],\n",
       "                      [-7.5531e-04, -2.2769e-05, -3.0060e-03,  ..., -2.3193e-03,\n",
       "                        1.4877e-03, -1.5640e-03],\n",
       "                      [ 2.4567e-03,  2.9144e-03, -2.0599e-03,  ...,  1.3885e-03,\n",
       "                       -1.0986e-03,  9.0408e-04],\n",
       "                      ...,\n",
       "                      [-7.5531e-04, -7.5817e-05,  2.2736e-03,  ...,  2.5749e-04,\n",
       "                        4.0054e-04, -2.7618e-03],\n",
       "                      [ 1.4572e-03, -3.1433e-03,  1.2589e-03,  ...,  2.8610e-04,\n",
       "                       -1.7548e-03,  1.7471e-03],\n",
       "                      [-3.1128e-03,  3.3264e-03,  2.4796e-04,  ...,  2.8038e-04,\n",
       "                        3.9816e-05,  6.2943e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.o_proj.weight',\n",
       "              tensor([[-5.1575e-03,  9.0408e-04,  2.9602e-03,  ...,  4.3869e-04,\n",
       "                        1.0147e-03,  3.3569e-04],\n",
       "                      [-2.1667e-03, -4.5013e-04,  2.8534e-03,  ...,  1.1978e-03,\n",
       "                        8.7357e-04,  2.7008e-03],\n",
       "                      [ 4.7913e-03,  1.8463e-03,  2.6703e-03,  ...,  3.9673e-03,\n",
       "                       -4.1008e-04,  1.4954e-03],\n",
       "                      ...,\n",
       "                      [-3.6163e-03, -2.4872e-03, -1.1683e-04,  ...,  4.4250e-03,\n",
       "                       -1.3351e-03,  4.2152e-04],\n",
       "                      [-2.3193e-03,  7.8201e-04, -1.1597e-03,  ...,  1.8845e-03,\n",
       "                        1.0071e-03, -4.6692e-03],\n",
       "                      [ 7.4387e-05, -3.3417e-03,  2.7466e-03,  ..., -1.4725e-03,\n",
       "                       -2.4567e-03, -7.5531e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.gate_proj.weight',\n",
       "              tensor([[-2.3651e-03, -1.6928e-05,  3.8757e-03,  ...,  1.7090e-03,\n",
       "                       -6.6280e-05,  3.8147e-05],\n",
       "                      [-2.8534e-03,  5.9509e-04, -1.2665e-03,  ...,  5.0964e-03,\n",
       "                       -5.9509e-04, -2.0294e-03],\n",
       "                      [-5.9128e-04,  3.0212e-03,  2.1057e-03,  ..., -3.0670e-03,\n",
       "                       -9.0942e-03,  1.8387e-03],\n",
       "                      ...,\n",
       "                      [ 8.1177e-03,  1.0315e-02, -1.2207e-03,  ..., -1.4496e-03,\n",
       "                        3.2806e-03, -4.7913e-03],\n",
       "                      [ 2.7008e-03, -8.3923e-04, -2.0599e-03,  ..., -3.7079e-03,\n",
       "                        3.2806e-03,  2.0752e-03],\n",
       "                      [-6.5613e-04,  3.9978e-03, -3.1738e-03,  ...,  2.4261e-03,\n",
       "                       -1.2131e-03,  5.7068e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.up_proj.weight',\n",
       "              tensor([[-1.4572e-03, -1.8387e-03,  8.5068e-04,  ...,  3.1586e-03,\n",
       "                        2.2793e-04,  2.2125e-03],\n",
       "                      [-5.9814e-03, -5.0964e-03, -8.6212e-04,  ...,  4.9133e-03,\n",
       "                       -3.3379e-04,  1.4954e-03],\n",
       "                      [ 1.8616e-03, -2.0504e-05, -3.8757e-03,  ...,  1.2360e-03,\n",
       "                       -2.2125e-04, -1.4038e-03],\n",
       "                      ...,\n",
       "                      [-2.1267e-04, -1.2146e-02, -1.4877e-03,  ..., -4.5776e-03,\n",
       "                       -3.2349e-03, -4.0588e-03],\n",
       "                      [-1.6022e-03,  2.9564e-04,  1.8311e-04,  ...,  2.8038e-04,\n",
       "                       -4.8065e-04,  1.7242e-03],\n",
       "                      [ 2.8076e-03, -5.9891e-04, -3.0670e-03,  ..., -2.9449e-03,\n",
       "                       -4.1504e-03, -3.8605e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.down_proj.weight',\n",
       "              tensor([[ 4.2725e-03, -1.3809e-03,  1.6479e-03,  ..., -3.7384e-03,\n",
       "                        2.0599e-03,  7.2937e-03],\n",
       "                      [-8.7738e-04, -5.9204e-03,  3.6621e-04,  ..., -8.3618e-03,\n",
       "                        7.3624e-04,  1.5182e-03],\n",
       "                      [-4.5166e-03, -3.6163e-03, -3.0212e-03,  ...,  4.7607e-03,\n",
       "                       -1.6022e-03,  4.2114e-03],\n",
       "                      ...,\n",
       "                      [-3.6812e-04, -1.4343e-03,  5.1270e-03,  ...,  1.5106e-03,\n",
       "                        5.3406e-03,  6.5994e-04],\n",
       "                      [ 2.7924e-03,  3.1090e-04, -2.5177e-03,  ..., -5.3644e-05,\n",
       "                       -2.2430e-03, -4.4556e-03],\n",
       "                      [-2.3956e-03, -5.3406e-04, -4.5471e-03,  ..., -2.6398e-03,\n",
       "                        4.0293e-05,  2.0313e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.9.input_layernorm.weight',\n",
       "              tensor([1.6250, 1.6797, 1.8281,  ..., 1.4141, 1.7188, 1.6250],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.post_attention_layernorm.weight',\n",
       "              tensor([1.4375, 1.7578, 1.7266,  ..., 1.3672, 1.4609, 1.5312],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.q_proj.weight',\n",
       "              tensor([[-1.4954e-03,  8.4686e-04, -1.8234e-03,  ..., -1.8234e-03,\n",
       "                       -5.9891e-04,  7.9632e-05],\n",
       "                      [-2.1515e-03, -1.0910e-03,  1.4687e-04,  ..., -2.4872e-03,\n",
       "                       -1.2894e-03,  2.3346e-03],\n",
       "                      [ 6.2561e-04, -4.5166e-03, -2.4567e-03,  ..., -2.1515e-03,\n",
       "                        4.3869e-05,  8.6670e-03],\n",
       "                      ...,\n",
       "                      [-5.4016e-03, -2.1553e-04, -2.7008e-03,  ..., -6.9046e-04,\n",
       "                       -4.6692e-03,  5.4321e-03],\n",
       "                      [-1.3199e-03, -3.9368e-03, -2.7008e-03,  ...,  1.2207e-02,\n",
       "                       -1.1292e-03, -3.1281e-03],\n",
       "                      [ 1.9150e-03,  1.7166e-03, -4.1199e-03,  ...,  5.7678e-03,\n",
       "                        2.7924e-03, -8.6975e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.k_proj.weight',\n",
       "              tensor([[ 3.7079e-03, -3.3112e-03, -2.4567e-03,  ...,  9.4604e-04,\n",
       "                       -4.3869e-04, -5.7983e-04],\n",
       "                      [-2.7771e-03, -4.8828e-03, -7.3242e-03,  ...,  3.3875e-03,\n",
       "                        4.7302e-03, -1.8311e-03],\n",
       "                      [-8.0109e-04,  1.4114e-03, -1.9150e-03,  ..., -1.0864e-02,\n",
       "                       -1.8463e-03, -5.1022e-05],\n",
       "                      ...,\n",
       "                      [-7.4463e-03, -1.2589e-03, -2.9602e-03,  ...,  3.2501e-03,\n",
       "                        5.0354e-03, -8.7891e-03],\n",
       "                      [-6.1340e-03, -7.0496e-03,  6.3477e-03,  ..., -2.9907e-03,\n",
       "                       -9.5215e-03,  3.4637e-03],\n",
       "                      [ 3.7994e-03,  1.2695e-02,  9.0942e-03,  ..., -3.3112e-03,\n",
       "                       -2.5330e-03, -8.3008e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0084, -0.0062, -0.0006,  ..., -0.0012, -0.0027,  0.0025],\n",
       "                      [ 0.0014,  0.0022, -0.0013,  ...,  0.0049,  0.0087, -0.0016],\n",
       "                      [ 0.0004,  0.0037,  0.0036,  ...,  0.0001,  0.0026, -0.0016],\n",
       "                      ...,\n",
       "                      [-0.0012,  0.0037, -0.0048,  ...,  0.0006,  0.0004,  0.0003],\n",
       "                      [-0.0008, -0.0025, -0.0001,  ...,  0.0007, -0.0003,  0.0019],\n",
       "                      [-0.0018,  0.0005,  0.0039,  ..., -0.0050,  0.0021, -0.0023]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.o_proj.weight',\n",
       "              tensor([[ 4.0894e-03, -1.8787e-04,  8.2016e-04,  ..., -2.4414e-03,\n",
       "                        1.3962e-03, -1.3657e-03],\n",
       "                      [ 3.0670e-03, -2.9449e-03, -3.1586e-03,  ..., -5.3883e-05,\n",
       "                       -1.2875e-04, -2.8687e-03],\n",
       "                      [ 2.7161e-03, -8.2016e-04, -2.7466e-04,  ..., -4.1809e-03,\n",
       "                        6.0425e-03,  2.4567e-03],\n",
       "                      ...,\n",
       "                      [ 3.0975e-03,  2.0123e-04,  2.9945e-04,  ..., -1.2970e-03,\n",
       "                        1.6632e-03,  4.7493e-04],\n",
       "                      [ 4.3030e-03,  5.0783e-05, -1.6174e-03,  ..., -1.2131e-03,\n",
       "                       -2.8534e-03, -1.7700e-03],\n",
       "                      [-6.0654e-04,  1.7090e-03,  9.2697e-04,  ...,  2.2583e-03,\n",
       "                        2.3193e-03,  1.0071e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.gate_proj.weight',\n",
       "              tensor([[ 1.0529e-03,  9.4604e-04, -3.8452e-03,  ...,  2.1973e-03,\n",
       "                       -1.9150e-03,  1.8787e-04],\n",
       "                      [ 3.8147e-03,  1.9379e-03, -6.0120e-03,  ..., -5.1575e-03,\n",
       "                       -3.5553e-03,  7.9727e-04],\n",
       "                      [ 5.2185e-03,  1.6689e-05,  4.8828e-03,  ...,  3.5248e-03,\n",
       "                        6.5613e-03,  2.2583e-03],\n",
       "                      ...,\n",
       "                      [ 2.0752e-03, -1.2665e-03,  3.0670e-03,  ..., -1.7700e-03,\n",
       "                       -1.3809e-03, -2.6093e-03],\n",
       "                      [-7.0953e-04,  7.2098e-04, -2.6703e-03,  ..., -1.1063e-03,\n",
       "                        4.1809e-03, -2.0294e-03],\n",
       "                      [-2.0266e-06,  6.2561e-04,  2.9373e-04,  ...,  4.3106e-04,\n",
       "                        1.0986e-03, -6.5308e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.up_proj.weight',\n",
       "              tensor([[ 9.1934e-04,  8.5449e-03, -3.3569e-03,  ...,  7.6599e-03,\n",
       "                        2.5330e-03, -1.8997e-03],\n",
       "                      [-2.3499e-03,  4.3106e-04,  2.5024e-03,  ..., -4.0894e-03,\n",
       "                       -1.1139e-03, -3.8147e-03],\n",
       "                      [ 3.6469e-03,  5.8899e-03, -5.5847e-03,  ...,  1.0757e-03,\n",
       "                        9.8419e-04,  1.6861e-03],\n",
       "                      ...,\n",
       "                      [ 2.9297e-03, -3.5858e-03,  3.2654e-03,  ..., -2.2125e-03,\n",
       "                        1.6479e-03,  2.5177e-03],\n",
       "                      [-3.7079e-03,  1.0529e-03, -6.0425e-03,  ...,  1.2970e-03,\n",
       "                       -8.5831e-04, -9.9182e-04],\n",
       "                      [ 2.4872e-03,  9.1553e-04,  1.1015e-04,  ...,  7.4768e-04,\n",
       "                       -6.7711e-05,  1.4801e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.down_proj.weight',\n",
       "              tensor([[-1.6556e-03,  4.4556e-03,  1.7853e-03,  ..., -4.9591e-04,\n",
       "                        9.1171e-04,  6.4087e-03],\n",
       "                      [-3.0365e-03,  3.4485e-03,  2.3346e-03,  ..., -1.9150e-03,\n",
       "                        3.7575e-04, -3.1891e-03],\n",
       "                      [ 2.2278e-03,  1.1978e-03, -4.0588e-03,  ...,  1.5717e-03,\n",
       "                       -2.2125e-03,  4.1504e-03],\n",
       "                      ...,\n",
       "                      [-2.3193e-03, -3.5400e-03,  3.0365e-03,  ...,  5.4932e-04,\n",
       "                        5.0545e-05, -6.1035e-03],\n",
       "                      [ 1.3809e-03, -2.2125e-03, -1.8387e-03,  ..., -1.8921e-03,\n",
       "                       -5.3406e-03, -1.0223e-03],\n",
       "                      [ 1.3809e-03, -2.9144e-03,  1.0223e-03,  ..., -4.8523e-03,\n",
       "                        9.4223e-04, -2.1667e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.10.input_layernorm.weight',\n",
       "              tensor([1.4141, 1.6641, 1.8750,  ..., 1.2031, 1.4609, 1.5703],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.post_attention_layernorm.weight',\n",
       "              tensor([1.4297, 1.8125, 1.8125,  ..., 1.3438, 1.4609, 1.5547],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0020, -0.0020,  0.0006,  ..., -0.0009,  0.0007, -0.0005],\n",
       "                      [-0.0017, -0.0042, -0.0003,  ..., -0.0022,  0.0015,  0.0035],\n",
       "                      [-0.0009, -0.0023,  0.0011,  ..., -0.0006,  0.0003,  0.0024],\n",
       "                      ...,\n",
       "                      [-0.0025,  0.0041,  0.0022,  ...,  0.0022, -0.0028, -0.0062],\n",
       "                      [-0.0002, -0.0072, -0.0024,  ...,  0.0013,  0.0013, -0.0030],\n",
       "                      [-0.0020, -0.0020, -0.0015,  ..., -0.0008, -0.0011, -0.0031]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.k_proj.weight',\n",
       "              tensor([[ 8.8501e-04, -1.2894e-03,  3.2959e-03,  ...,  1.1749e-03,\n",
       "                        6.7444e-03,  4.0894e-03],\n",
       "                      [-3.0365e-03, -2.6245e-03,  1.0376e-03,  ..., -3.3264e-03,\n",
       "                       -1.9302e-03, -1.8005e-03],\n",
       "                      [-8.5068e-04,  8.7891e-03, -1.8768e-03,  ...,  1.7700e-03,\n",
       "                       -3.1891e-03, -1.1139e-03],\n",
       "                      ...,\n",
       "                      [ 6.7749e-03, -6.5613e-03, -1.3504e-03,  ...,  5.4321e-03,\n",
       "                        6.3477e-03, -1.1292e-03],\n",
       "                      [ 3.9339e-05,  1.1475e-02, -6.5002e-03,  ..., -1.8997e-03,\n",
       "                       -1.8692e-03,  2.6245e-03],\n",
       "                      [ 1.3580e-03,  8.4229e-03,  1.2817e-02,  ..., -2.8839e-03,\n",
       "                        9.2163e-03, -5.0354e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0005, -0.0048, -0.0017,  ..., -0.0024, -0.0030, -0.0008],\n",
       "                      [-0.0055, -0.0033,  0.0105,  ..., -0.0023,  0.0016, -0.0035],\n",
       "                      [ 0.0028, -0.0014,  0.0045,  ..., -0.0019, -0.0042, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0006,  0.0022,  0.0003,  ..., -0.0041, -0.0021,  0.0001],\n",
       "                      [ 0.0033,  0.0020, -0.0005,  ...,  0.0021, -0.0006, -0.0004],\n",
       "                      [ 0.0044, -0.0019,  0.0019,  ..., -0.0026, -0.0016, -0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.o_proj.weight',\n",
       "              tensor([[-8.0872e-04, -1.1902e-03,  2.1820e-03,  ..., -9.2697e-04,\n",
       "                        1.0757e-03, -1.7166e-03],\n",
       "                      [ 1.1139e-03,  2.2583e-03,  2.9602e-03,  ..., -2.9602e-03,\n",
       "                       -2.3041e-03,  3.9368e-03],\n",
       "                      [-2.8992e-03, -4.7607e-03, -3.4637e-03,  ...,  6.7234e-05,\n",
       "                       -7.4005e-04,  1.4591e-04],\n",
       "                      ...,\n",
       "                      [ 1.2894e-03,  3.0365e-03, -4.7913e-03,  ..., -1.6022e-04,\n",
       "                       -3.8147e-03,  1.7853e-03],\n",
       "                      [-3.3569e-03,  1.8845e-03, -2.2888e-03,  ...,  7.8964e-04,\n",
       "                        6.8665e-04,  4.9133e-03],\n",
       "                      [ 2.8038e-04, -4.5166e-03, -1.9455e-04,  ..., -1.1597e-03,\n",
       "                       -1.1673e-03,  8.9264e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.gate_proj.weight',\n",
       "              tensor([[-3.6621e-03,  1.3733e-03, -2.8229e-03,  ...,  2.1362e-04,\n",
       "                        4.1008e-04,  4.6158e-04],\n",
       "                      [-6.1951e-03,  6.2866e-03, -1.9684e-03,  ..., -4.2725e-03,\n",
       "                        2.8229e-03,  5.4321e-03],\n",
       "                      [ 2.9564e-04, -7.9346e-04,  5.3406e-03,  ..., -4.1962e-04,\n",
       "                       -2.3746e-04, -1.0452e-03],\n",
       "                      ...,\n",
       "                      [ 6.6376e-04, -2.0142e-03,  1.8311e-03,  ...,  1.0681e-03,\n",
       "                        2.9907e-03, -1.4877e-03],\n",
       "                      [ 2.7618e-03, -2.0752e-03,  3.8147e-03,  ...,  6.0425e-03,\n",
       "                        1.8692e-03,  3.8147e-03],\n",
       "                      [ 1.4038e-03, -7.2002e-05, -7.4768e-04,  ...,  4.0588e-03,\n",
       "                       -5.2261e-04, -2.4567e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.up_proj.weight',\n",
       "              tensor([[ 3.0670e-03, -3.0975e-03, -2.8534e-03,  ...,  1.6098e-03,\n",
       "                       -5.0354e-04, -3.8147e-03],\n",
       "                      [ 3.6316e-03, -5.3711e-03, -4.2114e-03,  ...,  4.9133e-03,\n",
       "                        8.2397e-04,  1.1921e-06],\n",
       "                      [-1.2589e-03,  9.4604e-04,  3.3379e-04,  ..., -4.5471e-03,\n",
       "                        7.3547e-03,  1.4343e-03],\n",
       "                      ...,\n",
       "                      [ 1.3733e-03,  2.8534e-03,  5.3406e-04,  ...,  3.1281e-03,\n",
       "                        2.4033e-04,  3.9673e-04],\n",
       "                      [-3.5286e-04, -6.7139e-03, -1.8158e-03,  ..., -1.8692e-04,\n",
       "                       -1.7700e-03,  3.0212e-03],\n",
       "                      [ 2.5330e-03, -7.0801e-03,  3.4637e-03,  ...,  4.1809e-03,\n",
       "                       -5.5542e-03, -3.6621e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.down_proj.weight',\n",
       "              tensor([[ 3.1433e-03,  2.5330e-03, -1.5411e-03,  ...,  5.1575e-03,\n",
       "                        1.3199e-03, -1.4191e-03],\n",
       "                      [ 9.6893e-04, -5.9204e-03,  1.9989e-03,  ...,  4.0283e-03,\n",
       "                        4.4823e-04, -2.5787e-03],\n",
       "                      [ 2.2736e-03,  3.5286e-04,  2.1577e-05,  ...,  1.6022e-03,\n",
       "                        1.3428e-03,  6.7520e-04],\n",
       "                      ...,\n",
       "                      [ 6.0320e-05,  2.6703e-03,  1.8024e-04,  ...,  1.5793e-03,\n",
       "                       -5.1117e-04,  5.1498e-04],\n",
       "                      [ 4.6921e-04, -2.6093e-03,  4.7302e-03,  ..., -6.8054e-03,\n",
       "                        8.2779e-04,  2.3804e-03],\n",
       "                      [-3.2501e-03, -2.2583e-03,  3.0365e-03,  ...,  1.6251e-03,\n",
       "                       -2.0905e-03, -3.7842e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.11.input_layernorm.weight',\n",
       "              tensor([1.6016, 2.1250, 2.1094,  ..., 1.3125, 1.6875, 1.7266],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.post_attention_layernorm.weight',\n",
       "              tensor([1.4922, 2.0625, 1.8906,  ..., 1.3750, 1.4922, 1.6016],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0044, -0.0026, -0.0031,  ...,  0.0002,  0.0015, -0.0006],\n",
       "                      [ 0.0041, -0.0078,  0.0010,  ..., -0.0027, -0.0036, -0.0025],\n",
       "                      [-0.0018, -0.0071, -0.0044,  ...,  0.0012, -0.0018,  0.0017],\n",
       "                      ...,\n",
       "                      [-0.0067,  0.0005,  0.0034,  ..., -0.0014, -0.0036,  0.0024],\n",
       "                      [-0.0053,  0.0034, -0.0008,  ...,  0.0052,  0.0056,  0.0017],\n",
       "                      [-0.0002,  0.0035,  0.0023,  ..., -0.0011, -0.0046,  0.0049]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.k_proj.weight',\n",
       "              tensor([[-1.3504e-03, -3.2349e-03, -2.0623e-05,  ..., -2.6512e-04,\n",
       "                       -1.0729e-04,  5.6152e-03],\n",
       "                      [-2.0905e-03,  7.0572e-04,  6.8283e-04,  ...,  4.6158e-04,\n",
       "                       -1.6632e-03, -4.8828e-03],\n",
       "                      [-1.4305e-04, -2.2125e-03,  3.0518e-04,  ..., -1.2741e-03,\n",
       "                        9.9945e-04, -5.4169e-04],\n",
       "                      ...,\n",
       "                      [-3.0060e-03, -2.9907e-03,  2.1362e-03,  ...,  3.8147e-04,\n",
       "                        8.4686e-04, -4.0283e-03],\n",
       "                      [-2.2888e-04, -1.1368e-03,  8.6975e-04,  ..., -8.4686e-04,\n",
       "                        9.6893e-04, -2.0752e-03],\n",
       "                      [-6.9580e-03,  1.6479e-03, -5.3406e-03,  ...,  1.3199e-03,\n",
       "                        1.4572e-03, -5.0659e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.v_proj.weight',\n",
       "              tensor([[ 1.3351e-03, -1.1368e-03, -2.1362e-03,  ...,  4.8828e-04,\n",
       "                       -3.4714e-04,  1.8463e-03],\n",
       "                      [-1.2589e-04,  3.5248e-03, -2.1362e-03,  ..., -1.7929e-03,\n",
       "                       -2.0294e-03, -6.0272e-04],\n",
       "                      [ 4.8065e-04,  2.0294e-03, -1.8997e-03,  ..., -3.8910e-03,\n",
       "                       -1.7929e-03,  2.4872e-03],\n",
       "                      ...,\n",
       "                      [ 1.7624e-03, -4.7607e-03, -2.1667e-03,  ..., -8.8882e-04,\n",
       "                       -2.4261e-03, -4.5166e-03],\n",
       "                      [ 3.6774e-03, -3.3264e-03, -1.4420e-03,  ..., -1.3828e-04,\n",
       "                       -4.2915e-06, -7.9346e-04],\n",
       "                      [ 2.5635e-03, -1.1292e-03, -7.6294e-03,  ...,  2.1210e-03,\n",
       "                       -1.7471e-03, -1.4496e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.o_proj.weight',\n",
       "              tensor([[-2.6855e-03,  1.0376e-03, -1.4267e-03,  ...,  2.1515e-03,\n",
       "                        3.0365e-03,  1.1673e-03],\n",
       "                      [ 3.4180e-03,  3.8338e-04, -4.2419e-03,  ..., -1.6632e-03,\n",
       "                       -1.8158e-03, -9.4414e-05],\n",
       "                      [ 4.4632e-04, -1.0300e-03, -1.0452e-03,  ..., -6.0730e-03,\n",
       "                       -1.3123e-03, -3.1586e-03],\n",
       "                      ...,\n",
       "                      [-9.1934e-04,  1.7624e-03, -1.3428e-03,  ...,  1.2131e-03,\n",
       "                       -1.7624e-03, -1.9989e-03],\n",
       "                      [ 4.2343e-04, -3.0823e-03, -1.6556e-03,  ...,  1.9789e-05,\n",
       "                        4.3335e-03,  2.6703e-03],\n",
       "                      [ 1.7624e-03,  2.7466e-03, -1.1826e-03,  ..., -6.3324e-04,\n",
       "                       -5.2795e-03,  5.7983e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.gate_proj.weight',\n",
       "              tensor([[-2.3956e-03, -1.6785e-03,  1.7929e-03,  ..., -8.8882e-04,\n",
       "                        1.2436e-03,  6.2943e-04],\n",
       "                      [ 3.0212e-03, -3.1948e-05, -4.9133e-03,  ..., -1.6937e-03,\n",
       "                        2.0313e-04, -2.5635e-03],\n",
       "                      [ 5.3406e-03,  1.2665e-03,  5.3711e-03,  ..., -2.1210e-03,\n",
       "                       -3.3722e-03, -2.3041e-03],\n",
       "                      ...,\n",
       "                      [ 3.4485e-03,  1.8387e-03, -1.8311e-03,  ...,  8.7357e-04,\n",
       "                        4.9438e-03, -9.1553e-05],\n",
       "                      [-5.9509e-03, -3.5858e-03,  5.9204e-03,  ...,  2.8076e-03,\n",
       "                        4.0894e-03,  4.8256e-04],\n",
       "                      [ 3.2654e-03,  4.5471e-03,  4.7493e-04,  ...,  3.0756e-05,\n",
       "                        1.4019e-04,  2.2736e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.up_proj.weight',\n",
       "              tensor([[ 2.3041e-03, -3.2806e-03,  2.0447e-03,  ..., -2.2888e-03,\n",
       "                       -9.2697e-04, -3.1090e-04],\n",
       "                      [-1.7242e-03,  2.7466e-03, -3.7079e-03,  ..., -1.9989e-03,\n",
       "                        2.6093e-03,  5.7220e-06],\n",
       "                      [ 1.0529e-03,  1.8005e-03, -3.6774e-03,  ..., -6.8283e-04,\n",
       "                       -1.9836e-03, -1.1368e-03],\n",
       "                      ...,\n",
       "                      [ 4.6082e-03,  2.1820e-03, -5.0735e-04,  ..., -8.8215e-05,\n",
       "                       -2.0142e-03,  4.9744e-03],\n",
       "                      [ 2.8381e-03,  2.5330e-03, -2.7313e-03,  ...,  3.7231e-03,\n",
       "                       -1.0967e-04, -1.4343e-03],\n",
       "                      [ 6.8665e-03,  7.5684e-03, -8.1787e-03,  ...,  4.7607e-03,\n",
       "                       -4.8828e-03,  8.8501e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.down_proj.weight',\n",
       "              tensor([[ 3.0212e-03, -1.1597e-03,  3.2234e-04,  ...,  2.4719e-03,\n",
       "                        3.3569e-03,  5.4016e-03],\n",
       "                      [-7.6675e-04,  2.1210e-03,  4.2419e-03,  ..., -1.9302e-03,\n",
       "                       -3.7537e-03,  3.5553e-03],\n",
       "                      [-7.3910e-05,  2.3499e-03,  7.4158e-03,  ..., -1.0669e-05,\n",
       "                        2.8992e-03, -3.2654e-03],\n",
       "                      ...,\n",
       "                      [ 2.3499e-03, -2.6131e-04, -2.2793e-04,  ...,  2.1362e-03,\n",
       "                        3.7994e-03,  1.6708e-03],\n",
       "                      [ 4.4861e-03, -1.9836e-03, -7.3242e-04,  ...,  1.5030e-03,\n",
       "                        2.7924e-03, -1.7242e-03],\n",
       "                      [ 5.4550e-04,  3.6774e-03, -3.7689e-03,  ...,  3.3112e-03,\n",
       "                       -4.9438e-03,  2.0752e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.12.input_layernorm.weight',\n",
       "              tensor([1.7422, 2.2812, 2.3594,  ..., 1.4531, 1.7891, 1.9531],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.post_attention_layernorm.weight',\n",
       "              tensor([1.6172, 2.3594, 2.1094,  ..., 1.4375, 1.6328, 1.7500],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.q_proj.weight',\n",
       "              tensor([[-2.7657e-04,  6.6833e-03,  3.7842e-03,  ...,  1.6880e-04,\n",
       "                        1.2207e-03, -5.6839e-04],\n",
       "                      [-9.9182e-04, -5.9509e-03,  3.6469e-03,  ...,  2.9297e-03,\n",
       "                        3.0365e-03, -1.0071e-03],\n",
       "                      [ 4.3335e-03,  6.9580e-03,  3.2654e-03,  ...,  1.2207e-03,\n",
       "                       -1.9531e-03, -7.6771e-05],\n",
       "                      ...,\n",
       "                      [ 1.9836e-03,  4.5166e-03,  2.4719e-03,  ...,  1.0204e-04,\n",
       "                       -3.0518e-04,  4.3030e-03],\n",
       "                      [ 7.1335e-04, -1.3638e-04,  3.2806e-03,  ..., -4.7874e-04,\n",
       "                        1.1368e-03, -2.5177e-03],\n",
       "                      [-4.6387e-03,  1.3351e-03,  7.7438e-04,  ...,  2.0294e-03,\n",
       "                       -8.9645e-04, -4.3030e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0036, -0.0073, -0.0039,  ..., -0.0015, -0.0067,  0.0029],\n",
       "                      [ 0.0006, -0.0036,  0.0017,  ..., -0.0039,  0.0076,  0.0022],\n",
       "                      [ 0.0007, -0.0015,  0.0020,  ...,  0.0017,  0.0050,  0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0086, -0.0029, -0.0035,  ..., -0.0008, -0.0013, -0.0061],\n",
       "                      [-0.0056, -0.0056, -0.0032,  ...,  0.0001,  0.0028,  0.0031],\n",
       "                      [ 0.0038, -0.0064, -0.0034,  ..., -0.0051,  0.0005,  0.0025]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0008, -0.0010,  0.0006,  ...,  0.0006, -0.0020,  0.0001],\n",
       "                      [ 0.0017, -0.0039, -0.0041,  ...,  0.0003, -0.0010,  0.0009],\n",
       "                      [ 0.0001,  0.0012,  0.0060,  ...,  0.0022, -0.0003, -0.0045],\n",
       "                      ...,\n",
       "                      [-0.0002, -0.0012, -0.0009,  ...,  0.0024,  0.0024,  0.0005],\n",
       "                      [-0.0006,  0.0017, -0.0009,  ..., -0.0047, -0.0016, -0.0024],\n",
       "                      [ 0.0045,  0.0048, -0.0020,  ..., -0.0040, -0.0014,  0.0003]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.o_proj.weight',\n",
       "              tensor([[ 5.7602e-04, -2.1696e-05,  4.6692e-03,  ..., -3.6621e-03,\n",
       "                       -1.4496e-03, -1.9455e-03],\n",
       "                      [ 3.5400e-03, -1.9684e-03, -1.6327e-03,  ..., -1.6708e-03,\n",
       "                       -3.0670e-03, -1.2302e-04],\n",
       "                      [ 1.1139e-03, -1.4019e-04,  3.9062e-03,  ..., -3.3722e-03,\n",
       "                        1.4648e-03,  7.2098e-04],\n",
       "                      ...,\n",
       "                      [ 3.1586e-03, -2.1057e-03,  2.7008e-03,  ...,  4.3030e-03,\n",
       "                       -2.8610e-04, -9.9945e-04],\n",
       "                      [-1.8158e-03, -2.8534e-03, -3.6469e-03,  ...,  1.2360e-03,\n",
       "                       -5.6458e-04,  4.1199e-03],\n",
       "                      [-1.6861e-03, -1.7471e-03, -5.0964e-03,  ...,  7.9727e-04,\n",
       "                       -4.7913e-03,  1.5640e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.gate_proj.weight',\n",
       "              tensor([[-2.9907e-03, -1.0834e-03,  2.1667e-03,  ...,  3.4637e-03,\n",
       "                        9.8419e-04,  1.2283e-03],\n",
       "                      [-5.8594e-03,  6.6757e-04, -2.0905e-03,  ..., -3.1586e-03,\n",
       "                        4.6730e-04, -2.2430e-03],\n",
       "                      [-8.8501e-03,  4.8523e-03,  3.9673e-03,  ..., -1.3580e-03,\n",
       "                        4.0588e-03,  3.7537e-03],\n",
       "                      ...,\n",
       "                      [-7.0572e-04, -5.6458e-03,  4.5967e-04,  ..., -3.9673e-03,\n",
       "                        1.2817e-03, -2.7618e-03],\n",
       "                      [ 2.3956e-03, -4.7922e-05, -3.5667e-04,  ...,  2.8839e-03,\n",
       "                       -2.0599e-03,  7.6294e-04],\n",
       "                      [-4.9438e-03,  1.2360e-03, -3.0823e-03,  ..., -2.7466e-03,\n",
       "                       -3.0975e-03,  5.4016e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.up_proj.weight',\n",
       "              tensor([[-0.0023,  0.0003,  0.0003,  ...,  0.0022, -0.0024,  0.0003],\n",
       "                      [ 0.0022,  0.0031,  0.0010,  ..., -0.0021,  0.0044,  0.0009],\n",
       "                      [-0.0042, -0.0018,  0.0060,  ...,  0.0007,  0.0044, -0.0047],\n",
       "                      ...,\n",
       "                      [ 0.0020, -0.0090, -0.0002,  ...,  0.0020, -0.0002,  0.0010],\n",
       "                      [-0.0008,  0.0034, -0.0002,  ..., -0.0021,  0.0008, -0.0019],\n",
       "                      [-0.0032, -0.0022, -0.0013,  ...,  0.0010,  0.0016,  0.0052]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.down_proj.weight',\n",
       "              tensor([[-6.9427e-04,  9.6130e-04,  1.1206e-04,  ..., -1.5106e-03,\n",
       "                       -2.4414e-03,  2.7771e-03],\n",
       "                      [ 5.5313e-04, -4.4441e-04,  1.3123e-03,  ..., -5.5237e-03,\n",
       "                        1.9836e-03, -1.4572e-03],\n",
       "                      [ 1.2589e-03, -2.6245e-03,  3.3569e-03,  ..., -3.7003e-04,\n",
       "                        3.0518e-03, -4.2114e-03],\n",
       "                      ...,\n",
       "                      [-1.0529e-03, -2.7008e-03, -3.4485e-03,  ..., -5.3406e-04,\n",
       "                        2.4719e-03,  1.2207e-03],\n",
       "                      [ 1.8539e-03,  4.7607e-03,  3.9062e-03,  ..., -2.3346e-03,\n",
       "                       -2.6245e-03, -4.1199e-03],\n",
       "                      [-1.3657e-03,  1.6861e-03, -1.1978e-03,  ..., -1.3657e-03,\n",
       "                        1.4420e-03, -1.8477e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.13.input_layernorm.weight',\n",
       "              tensor([1.7969, 2.7031, 2.6250,  ..., 1.4219, 1.7969, 2.0781],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.post_attention_layernorm.weight',\n",
       "              tensor([1.7344, 2.7188, 2.2344,  ..., 1.5391, 1.7109, 1.8281],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.q_proj.weight',\n",
       "              tensor([[-4.7913e-03,  4.3030e-03, -2.1667e-03,  ..., -4.0894e-03,\n",
       "                        1.7643e-04,  4.1504e-03],\n",
       "                      [ 5.2691e-05,  8.8501e-03,  9.5749e-04,  ..., -2.6321e-04,\n",
       "                        1.0376e-03, -5.8746e-04],\n",
       "                      [ 4.2725e-03, -5.4016e-03,  4.6997e-03,  ...,  1.2436e-03,\n",
       "                        7.4005e-04, -1.7242e-03],\n",
       "                      ...,\n",
       "                      [ 5.2643e-04,  7.1716e-04,  9.6436e-03,  ...,  5.9509e-03,\n",
       "                        1.0010e-02, -4.4861e-03],\n",
       "                      [ 8.3618e-03,  5.3406e-03, -9.7656e-03,  ...,  1.3809e-03,\n",
       "                       -1.2146e-02, -1.0910e-03],\n",
       "                      [-2.9907e-03, -7.5073e-03,  5.1117e-04,  ..., -1.2970e-03,\n",
       "                       -1.6861e-03,  1.7242e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.k_proj.weight',\n",
       "              tensor([[ 6.2561e-04, -1.2283e-03, -8.9645e-04,  ..., -2.7466e-03,\n",
       "                       -3.7766e-04, -1.9455e-03],\n",
       "                      [-1.4954e-03,  8.0109e-04, -3.4714e-04,  ..., -3.1891e-03,\n",
       "                        2.0752e-03,  1.7929e-03],\n",
       "                      [-1.2665e-03, -5.6839e-04, -4.2915e-04,  ...,  1.0529e-03,\n",
       "                       -1.6403e-03,  1.1368e-03],\n",
       "                      ...,\n",
       "                      [ 6.0730e-03,  1.8158e-03,  2.9449e-03,  ...,  2.2888e-03,\n",
       "                       -3.6011e-03, -1.4877e-03],\n",
       "                      [-1.8768e-03, -3.9978e-03,  9.5749e-04,  ..., -4.6692e-03,\n",
       "                       -1.4465e-02,  3.2196e-03],\n",
       "                      [-4.1809e-03,  2.2888e-05,  4.8828e-03,  ..., -1.7166e-03,\n",
       "                        5.8289e-03, -1.4496e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.v_proj.weight',\n",
       "              tensor([[ 8.9645e-04, -2.1515e-03,  1.7166e-03,  ..., -3.9673e-03,\n",
       "                       -2.0905e-03,  1.5640e-03],\n",
       "                      [-3.2043e-03, -7.1049e-05,  3.4637e-03,  ...,  2.3651e-03,\n",
       "                        2.6398e-03,  1.0147e-03],\n",
       "                      [-4.2915e-04, -1.9226e-03, -7.0953e-04,  ...,  4.0894e-03,\n",
       "                       -1.0910e-03,  1.1978e-03],\n",
       "                      ...,\n",
       "                      [ 5.4359e-05,  1.8311e-03, -1.3657e-03,  ..., -1.3504e-03,\n",
       "                       -2.3193e-03, -1.6937e-03],\n",
       "                      [ 3.7231e-03, -3.7689e-03,  3.6316e-03,  ...,  1.2131e-03,\n",
       "                        6.4468e-04, -2.4414e-03],\n",
       "                      [ 5.6839e-04,  6.2943e-04, -9.1553e-04,  ...,  4.8218e-03,\n",
       "                        1.4420e-03, -2.7466e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.o_proj.weight',\n",
       "              tensor([[-1.8387e-03, -1.0681e-03,  3.1471e-05,  ...,  1.9226e-03,\n",
       "                        5.6839e-04,  2.4719e-03],\n",
       "                      [-1.4725e-03, -2.8076e-03, -7.2479e-04,  ..., -5.0964e-03,\n",
       "                       -4.7445e-05, -2.8381e-03],\n",
       "                      [-2.3041e-03, -1.8997e-03,  3.1586e-03,  ...,  9.0790e-04,\n",
       "                        4.1962e-04, -2.2888e-03],\n",
       "                      ...,\n",
       "                      [-1.4954e-03,  1.4420e-03, -6.9809e-04,  ...,  5.7678e-03,\n",
       "                        3.5706e-03,  2.2888e-03],\n",
       "                      [ 2.9449e-03,  1.5030e-03,  2.2278e-03,  ...,  9.0408e-04,\n",
       "                        3.0365e-03,  1.2894e-03],\n",
       "                      [ 2.6855e-03, -5.2261e-04, -2.7313e-03,  ..., -8.6975e-04,\n",
       "                       -7.7820e-04, -2.0885e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.gate_proj.weight',\n",
       "              tensor([[-5.0659e-03, -3.4790e-03, -2.8076e-03,  ...,  4.5471e-03,\n",
       "                        5.4321e-03,  5.7678e-03],\n",
       "                      [ 3.3379e-05,  1.5182e-03, -5.0964e-03,  ..., -4.5166e-03,\n",
       "                        4.8523e-03, -3.0975e-03],\n",
       "                      [-2.7924e-03,  2.3956e-03, -4.2419e-03,  ..., -2.7275e-04,\n",
       "                        1.7319e-03, -3.8300e-03],\n",
       "                      ...,\n",
       "                      [-1.8616e-03,  1.3885e-03,  3.8452e-03,  ...,  8.7891e-03,\n",
       "                        7.7209e-03,  2.4567e-03],\n",
       "                      [ 1.3123e-03, -3.2997e-04,  1.1826e-03,  ...,  2.5940e-04,\n",
       "                       -1.8845e-03, -3.9978e-03],\n",
       "                      [-4.5166e-03, -8.1635e-04, -2.7771e-03,  ..., -2.3174e-04,\n",
       "                        1.5488e-03,  1.0757e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.up_proj.weight',\n",
       "              tensor([[-1.2302e-04,  3.2806e-03, -9.5749e-04,  ...,  3.8300e-03,\n",
       "                        2.6245e-03,  2.3651e-03],\n",
       "                      [ 3.0518e-03, -7.0190e-03, -3.0060e-03,  ...,  1.1902e-03,\n",
       "                        2.9144e-03, -8.0490e-04],\n",
       "                      [ 5.7220e-06, -2.7924e-03,  9.6512e-04,  ..., -1.0757e-03,\n",
       "                       -3.3875e-03,  1.9150e-03],\n",
       "                      ...,\n",
       "                      [ 2.8839e-03,  3.1891e-03,  2.2278e-03,  ...,  1.2741e-03,\n",
       "                        8.7357e-04,  3.2806e-03],\n",
       "                      [-2.0599e-03,  2.5177e-03,  4.2114e-03,  ...,  9.0408e-04,\n",
       "                       -3.9368e-03, -5.0049e-03],\n",
       "                      [ 4.3030e-03, -1.4954e-03,  2.7924e-03,  ..., -3.9673e-03,\n",
       "                        1.8616e-03, -3.4637e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.down_proj.weight',\n",
       "              tensor([[-0.0004,  0.0049,  0.0010,  ..., -0.0006,  0.0012,  0.0015],\n",
       "                      [-0.0021, -0.0030, -0.0007,  ...,  0.0035, -0.0010,  0.0045],\n",
       "                      [ 0.0032, -0.0063,  0.0003,  ...,  0.0004, -0.0077,  0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0025,  0.0027,  0.0026,  ..., -0.0042,  0.0032, -0.0036],\n",
       "                      [ 0.0035,  0.0006, -0.0019,  ...,  0.0017, -0.0020, -0.0013],\n",
       "                      [-0.0022,  0.0007, -0.0019,  ..., -0.0006, -0.0018,  0.0010]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.input_layernorm.weight',\n",
       "              tensor([1.9766, 2.9375, 2.7656,  ..., 1.5391, 1.8984, 2.1875],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.post_attention_layernorm.weight',\n",
       "              tensor([1.7891, 3.1094, 2.2969,  ..., 1.6328, 1.7734, 1.8750],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.q_proj.weight',\n",
       "              tensor([[-1.0529e-03,  2.4567e-03, -4.0054e-05,  ...,  8.6212e-04,\n",
       "                       -3.1128e-03, -2.1667e-03],\n",
       "                      [ 2.0313e-04, -3.7994e-03,  1.1139e-03,  ...,  2.1515e-03,\n",
       "                        1.2512e-03,  9.8419e-04],\n",
       "                      [-3.6469e-03,  7.9346e-03, -1.0777e-04,  ...,  1.7471e-03,\n",
       "                       -2.1515e-03,  1.2817e-03],\n",
       "                      ...,\n",
       "                      [-6.2866e-03,  3.8300e-03,  5.5237e-03,  ..., -4.5471e-03,\n",
       "                        4.9133e-03,  1.0605e-03],\n",
       "                      [ 9.1934e-04,  1.8120e-05,  4.6692e-03,  ...,  4.2725e-03,\n",
       "                       -8.6670e-03,  1.7700e-03],\n",
       "                      [ 2.1362e-03,  2.9907e-03,  2.2888e-03,  ..., -3.8300e-03,\n",
       "                        1.9150e-03, -5.3101e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.k_proj.weight',\n",
       "              tensor([[ 2.1362e-03, -5.0049e-03,  4.0627e-04,  ...,  4.6015e-05,\n",
       "                        2.0599e-03,  4.0245e-04],\n",
       "                      [-2.1362e-03,  4.6692e-03, -3.6049e-04,  ..., -3.8452e-03,\n",
       "                       -1.9531e-03,  2.3651e-03],\n",
       "                      [ 1.4191e-03, -6.0425e-03, -5.6763e-03,  ...,  2.9907e-03,\n",
       "                        6.2180e-04,  1.1749e-03],\n",
       "                      ...,\n",
       "                      [-7.2632e-03, -7.6294e-04,  9.9487e-03,  ...,  5.1270e-03,\n",
       "                        3.3264e-03, -5.0659e-03],\n",
       "                      [ 4.2725e-03, -3.1948e-05,  3.0060e-03,  ..., -5.7983e-03,\n",
       "                       -3.2349e-03, -2.1935e-04],\n",
       "                      [-1.6174e-03, -2.6093e-03,  8.1787e-03,  ...,  4.3640e-03,\n",
       "                        1.9287e-02,  8.5449e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.v_proj.weight',\n",
       "              tensor([[ 4.3640e-03, -1.8921e-03, -1.7548e-03,  ..., -4.3030e-03,\n",
       "                        4.8828e-04,  2.6550e-03],\n",
       "                      [ 8.2016e-05,  1.1139e-03,  2.5482e-03,  ..., -1.2512e-03,\n",
       "                       -1.1969e-04,  3.5706e-03],\n",
       "                      [-5.6839e-04, -4.7607e-03,  3.0518e-03,  ...,  3.2196e-03,\n",
       "                        4.4861e-03, -5.5695e-04],\n",
       "                      ...,\n",
       "                      [ 1.6499e-04, -4.9591e-04, -6.0654e-04,  ..., -3.6163e-03,\n",
       "                       -1.9455e-03, -1.8082e-03],\n",
       "                      [ 1.6098e-03, -1.7643e-04,  8.3160e-04,  ..., -2.4567e-03,\n",
       "                        4.5776e-03,  1.3504e-03],\n",
       "                      [ 3.1586e-03, -1.0452e-03, -9.6130e-04,  ...,  3.9864e-04,\n",
       "                       -5.7220e-06, -1.9836e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0034, -0.0033,  0.0003,  ..., -0.0021,  0.0039,  0.0025],\n",
       "                      [-0.0013,  0.0015, -0.0015,  ...,  0.0039,  0.0041,  0.0018],\n",
       "                      [ 0.0009,  0.0032,  0.0025,  ..., -0.0021,  0.0040,  0.0038],\n",
       "                      ...,\n",
       "                      [-0.0032, -0.0011,  0.0032,  ..., -0.0045, -0.0004, -0.0040],\n",
       "                      [ 0.0001,  0.0003,  0.0010,  ...,  0.0001, -0.0045,  0.0019],\n",
       "                      [ 0.0028,  0.0023, -0.0011,  ..., -0.0024, -0.0017,  0.0017]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0001, -0.0025, -0.0017,  ...,  0.0040,  0.0028,  0.0011],\n",
       "                      [-0.0026,  0.0018, -0.0007,  ...,  0.0004, -0.0005,  0.0057],\n",
       "                      [-0.0011, -0.0027, -0.0004,  ..., -0.0001,  0.0017,  0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0028, -0.0004,  0.0011,  ..., -0.0031, -0.0009, -0.0018],\n",
       "                      [ 0.0010,  0.0002, -0.0002,  ..., -0.0015, -0.0028, -0.0030],\n",
       "                      [-0.0034,  0.0033, -0.0038,  ...,  0.0005,  0.0006,  0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.up_proj.weight',\n",
       "              tensor([[-1.2054e-03,  4.3640e-03, -1.6022e-03,  ..., -5.5847e-03,\n",
       "                       -4.0283e-03,  1.4572e-03],\n",
       "                      [-2.3842e-06,  1.8616e-03, -3.0518e-03,  ..., -4.1199e-03,\n",
       "                        3.4637e-03, -3.9291e-04],\n",
       "                      [ 3.0212e-03, -3.7384e-03, -1.9989e-03,  ..., -2.0294e-03,\n",
       "                       -7.1335e-04, -2.5177e-03],\n",
       "                      ...,\n",
       "                      [-6.1798e-04,  3.1471e-04, -1.1520e-03,  ...,  2.0752e-03,\n",
       "                       -5.3711e-03,  6.0654e-04],\n",
       "                      [-1.3065e-04, -5.1575e-03, -9.9182e-04,  ..., -2.8381e-03,\n",
       "                       -4.8637e-04,  1.0452e-03],\n",
       "                      [-2.1210e-03,  2.6398e-03, -2.7161e-03,  ...,  2.2736e-03,\n",
       "                        8.4839e-03, -4.1504e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.down_proj.weight',\n",
       "              tensor([[ 7.6294e-04,  1.4572e-03,  2.1515e-03,  ..., -2.6703e-03,\n",
       "                       -3.0975e-03, -4.8523e-03],\n",
       "                      [-6.7139e-04,  1.7357e-04,  8.3923e-04,  ..., -1.7776e-03,\n",
       "                       -2.0146e-05,  8.3923e-04],\n",
       "                      [ 1.4267e-03, -2.1057e-03, -1.6022e-03,  ..., -3.9978e-03,\n",
       "                       -3.7842e-03, -1.2741e-03],\n",
       "                      ...,\n",
       "                      [ 8.0872e-04, -1.9379e-03, -1.2360e-03,  ..., -2.0294e-03,\n",
       "                       -2.2278e-03,  5.0964e-03],\n",
       "                      [ 4.0894e-03, -1.0443e-04, -7.5150e-04,  ..., -4.3640e-03,\n",
       "                        7.1335e-04, -2.3956e-03],\n",
       "                      [ 7.3242e-04, -5.8746e-04,  5.2691e-05,  ...,  8.1635e-04,\n",
       "                        1.3542e-04, -3.7193e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.15.input_layernorm.weight',\n",
       "              tensor([2.2031, 3.2656, 3.0156,  ..., 1.8750, 2.2188, 2.4219],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.post_attention_layernorm.weight',\n",
       "              tensor([1.9609, 3.8438, 2.4219,  ..., 1.8438, 1.9609, 2.0156],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.q_proj.weight',\n",
       "              tensor([[-2.0905e-03,  4.5471e-03, -2.6512e-04,  ...,  2.7313e-03,\n",
       "                        1.3733e-03, -3.9291e-04],\n",
       "                      [-1.0681e-04, -3.6163e-03, -2.7008e-03,  ...,  2.3193e-03,\n",
       "                        2.4109e-03, -1.3962e-03],\n",
       "                      [ 3.0708e-04,  3.6621e-03,  4.1389e-04,  ...,  3.7003e-04,\n",
       "                       -1.9684e-03,  1.1215e-03],\n",
       "                      ...,\n",
       "                      [ 2.7618e-03, -5.6076e-04, -5.3406e-05,  ..., -4.6692e-03,\n",
       "                        5.2795e-03, -1.0490e-04],\n",
       "                      [-1.7242e-03,  1.3809e-03,  1.3885e-03,  ..., -2.1744e-04,\n",
       "                        3.9978e-03,  4.1504e-03],\n",
       "                      [ 1.8539e-03,  1.5259e-03, -2.7313e-03,  ..., -1.7319e-03,\n",
       "                       -3.1128e-03, -6.8665e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0001,  0.0030,  0.0030,  ...,  0.0029,  0.0007, -0.0019],\n",
       "                      [-0.0025, -0.0002, -0.0004,  ..., -0.0035,  0.0016,  0.0020],\n",
       "                      [ 0.0015,  0.0015,  0.0025,  ...,  0.0011,  0.0030, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0005, -0.0006, -0.0014,  ..., -0.0007, -0.0018,  0.0010],\n",
       "                      [-0.0012,  0.0024, -0.0018,  ...,  0.0031, -0.0043, -0.0029],\n",
       "                      [ 0.0008,  0.0070, -0.0059,  ..., -0.0118,  0.0026, -0.0063]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0003,  0.0006, -0.0045,  ..., -0.0034,  0.0009, -0.0008],\n",
       "                      [-0.0003, -0.0003,  0.0016,  ..., -0.0030, -0.0029,  0.0017],\n",
       "                      [ 0.0028, -0.0020, -0.0013,  ..., -0.0029, -0.0025, -0.0016],\n",
       "                      ...,\n",
       "                      [ 0.0009, -0.0008, -0.0019,  ...,  0.0014,  0.0012, -0.0041],\n",
       "                      [ 0.0002,  0.0011, -0.0010,  ..., -0.0025,  0.0003,  0.0033],\n",
       "                      [-0.0022,  0.0012,  0.0004,  ..., -0.0018,  0.0058,  0.0026]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0029, -0.0024,  0.0032,  ...,  0.0022, -0.0032,  0.0031],\n",
       "                      [ 0.0013,  0.0009,  0.0027,  ..., -0.0019, -0.0005, -0.0013],\n",
       "                      [-0.0051, -0.0011,  0.0001,  ..., -0.0002, -0.0006,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0034,  0.0001, -0.0002,  ..., -0.0024,  0.0012,  0.0034],\n",
       "                      [ 0.0015, -0.0003, -0.0035,  ...,  0.0011,  0.0035,  0.0030],\n",
       "                      [-0.0029, -0.0014, -0.0031,  ..., -0.0031, -0.0013,  0.0032]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.gate_proj.weight',\n",
       "              tensor([[-2.4261e-03, -4.2725e-03,  1.2817e-03,  ..., -5.8289e-03,\n",
       "                       -9.4891e-05, -4.5776e-03],\n",
       "                      [ 3.6011e-03,  2.4719e-03,  6.3705e-04,  ...,  2.9144e-03,\n",
       "                        5.0659e-03, -3.8300e-03],\n",
       "                      [ 2.2736e-03, -7.7057e-04, -6.0120e-03,  ...,  2.5024e-03,\n",
       "                       -2.4872e-03,  6.5918e-03],\n",
       "                      ...,\n",
       "                      [ 1.5335e-03,  2.6855e-03, -1.7853e-03,  ...,  1.7014e-03,\n",
       "                        5.2185e-03, -6.5002e-03],\n",
       "                      [ 3.2043e-04,  6.0654e-04, -2.4109e-03,  ..., -5.5847e-03,\n",
       "                       -2.8839e-03, -1.4973e-04],\n",
       "                      [-3.9482e-04, -2.0447e-03, -4.3106e-04,  ..., -9.6321e-05,\n",
       "                        5.7983e-04,  2.4261e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.up_proj.weight',\n",
       "              tensor([[ 1.3962e-03, -4.6692e-03, -2.7466e-03,  ..., -2.5940e-03,\n",
       "                       -8.2779e-04, -4.5776e-03],\n",
       "                      [ 3.0518e-04,  4.0588e-03,  7.2956e-05,  ...,  2.7466e-03,\n",
       "                       -2.8229e-03,  1.6556e-03],\n",
       "                      [-2.6703e-03,  7.8201e-04, -1.2436e-03,  ..., -1.9455e-03,\n",
       "                       -6.3171e-03,  2.2430e-03],\n",
       "                      ...,\n",
       "                      [-2.6550e-03, -1.4496e-03, -2.5024e-03,  ..., -1.8234e-03,\n",
       "                       -2.1820e-03, -2.8992e-03],\n",
       "                      [-3.6163e-03,  2.7313e-03, -1.6556e-03,  ...,  7.1716e-04,\n",
       "                        4.7302e-03, -5.0964e-03],\n",
       "                      [-8.7280e-03, -1.2283e-03,  3.8910e-03,  ...,  1.3046e-03,\n",
       "                       -1.8845e-03, -8.1062e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.down_proj.weight',\n",
       "              tensor([[ 2.6398e-03, -2.8076e-03, -4.3945e-03,  ..., -2.3041e-03,\n",
       "                       -6.5308e-03, -9.5215e-03],\n",
       "                      [-7.3242e-03,  6.0120e-03,  9.1553e-04,  ..., -4.3030e-03,\n",
       "                        9.4986e-04,  1.6098e-03],\n",
       "                      [-4.3640e-03, -1.0605e-03, -9.2983e-05,  ..., -2.5177e-03,\n",
       "                        9.7275e-04,  3.7842e-03],\n",
       "                      ...,\n",
       "                      [ 5.3406e-03,  1.1139e-03,  1.5335e-03,  ..., -2.4719e-03,\n",
       "                        6.3705e-04, -5.1880e-03],\n",
       "                      [-1.6098e-03, -1.7853e-03, -8.4229e-03,  ..., -2.1820e-03,\n",
       "                        2.3127e-05, -7.8964e-04],\n",
       "                      [-4.9744e-03,  2.3651e-03,  3.2043e-03,  ..., -4.8218e-03,\n",
       "                       -1.9836e-03, -1.9455e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.16.input_layernorm.weight',\n",
       "              tensor([2.3594, 3.6875, 2.9688,  ..., 1.9531, 2.2812, 2.4688],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.post_attention_layernorm.weight',\n",
       "              tensor([2.1406, 4.5000, 2.5156,  ..., 1.9922, 2.1250, 2.1719],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.q_proj.weight',\n",
       "              tensor([[-2.8992e-03, -8.3008e-03,  3.5553e-03,  ..., -2.1515e-03,\n",
       "                       -5.1575e-03, -1.6327e-03],\n",
       "                      [ 5.0545e-05, -4.1199e-03,  1.8215e-04,  ...,  4.5471e-03,\n",
       "                       -5.0735e-04,  2.0447e-03],\n",
       "                      [-2.0752e-03,  6.0730e-03, -4.4556e-03,  ..., -1.3428e-03,\n",
       "                       -1.4114e-03, -3.4943e-03],\n",
       "                      ...,\n",
       "                      [ 3.8910e-04, -1.0498e-02,  1.9989e-03,  ..., -3.8147e-03,\n",
       "                       -2.1553e-04,  2.6464e-05],\n",
       "                      [ 7.4005e-04,  5.6076e-04, -1.3123e-03,  ...,  4.5471e-03,\n",
       "                       -1.2436e-03, -3.2806e-03],\n",
       "                      [ 4.7913e-03,  7.9956e-03,  9.2316e-04,  ..., -5.6763e-03,\n",
       "                        5.1270e-03, -2.0447e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.2970e-03,  4.0588e-03, -4.2725e-03,  ..., -3.4027e-03,\n",
       "                        2.6550e-03, -4.0293e-05],\n",
       "                      [ 3.5286e-04,  4.0894e-03,  2.7466e-03,  ..., -5.0049e-03,\n",
       "                        1.7319e-03, -3.9062e-03],\n",
       "                      [ 2.1362e-03, -5.8899e-03, -9.4414e-05,  ..., -2.5940e-03,\n",
       "                        7.8583e-04, -8.8882e-04],\n",
       "                      ...,\n",
       "                      [ 6.2866e-03, -9.3994e-03,  2.5940e-03,  ..., -2.6226e-05,\n",
       "                       -2.6093e-03,  6.2256e-03],\n",
       "                      [-5.1575e-03, -1.5945e-03,  6.5918e-03,  ...,  1.0437e-02,\n",
       "                       -5.9814e-03,  2.5787e-03],\n",
       "                      [ 4.3945e-03,  8.7280e-03, -4.1771e-04,  ..., -1.2436e-03,\n",
       "                        8.3160e-04, -7.9346e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0015,  0.0048, -0.0007,  ..., -0.0055,  0.0024,  0.0048],\n",
       "                      [-0.0005, -0.0014,  0.0002,  ..., -0.0018,  0.0061,  0.0036],\n",
       "                      [-0.0023, -0.0009,  0.0005,  ..., -0.0075,  0.0006, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0010,  0.0031,  0.0011,  ..., -0.0051,  0.0021,  0.0009],\n",
       "                      [-0.0047,  0.0021,  0.0015,  ..., -0.0026, -0.0001,  0.0001],\n",
       "                      [-0.0042,  0.0003,  0.0017,  ..., -0.0041,  0.0053,  0.0010]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0005, -0.0009, -0.0034,  ...,  0.0044,  0.0012,  0.0023],\n",
       "                      [ 0.0016, -0.0014,  0.0003,  ..., -0.0016, -0.0025, -0.0022],\n",
       "                      [ 0.0007, -0.0020,  0.0022,  ..., -0.0026,  0.0038,  0.0038],\n",
       "                      ...,\n",
       "                      [-0.0022, -0.0044, -0.0022,  ...,  0.0059,  0.0035, -0.0031],\n",
       "                      [ 0.0025,  0.0005, -0.0041,  ...,  0.0023,  0.0008, -0.0033],\n",
       "                      [ 0.0046,  0.0032, -0.0026,  ..., -0.0037, -0.0026, -0.0050]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0048,  0.0041,  0.0004,  ...,  0.0026, -0.0031,  0.0040],\n",
       "                      [-0.0017,  0.0006,  0.0006,  ..., -0.0008, -0.0032,  0.0022],\n",
       "                      [-0.0036, -0.0020,  0.0018,  ...,  0.0045,  0.0013, -0.0010],\n",
       "                      ...,\n",
       "                      [ 0.0009,  0.0005,  0.0003,  ..., -0.0015,  0.0014,  0.0009],\n",
       "                      [ 0.0051,  0.0034, -0.0026,  ...,  0.0091, -0.0043, -0.0035],\n",
       "                      [-0.0056,  0.0016,  0.0017,  ..., -0.0031, -0.0003,  0.0004]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.up_proj.weight',\n",
       "              tensor([[-0.0020,  0.0028,  0.0023,  ..., -0.0005,  0.0049, -0.0041],\n",
       "                      [ 0.0040,  0.0085, -0.0044,  ...,  0.0023,  0.0059, -0.0034],\n",
       "                      [-0.0014, -0.0031,  0.0039,  ...,  0.0019,  0.0003,  0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0013, -0.0009, -0.0071,  ...,  0.0020, -0.0019,  0.0002],\n",
       "                      [-0.0028,  0.0016,  0.0044,  ..., -0.0032, -0.0002, -0.0036],\n",
       "                      [-0.0008,  0.0018, -0.0001,  ..., -0.0062, -0.0054, -0.0016]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0033, -0.0025,  0.0023,  ...,  0.0020, -0.0032, -0.0030],\n",
       "                      [ 0.0032, -0.0016,  0.0079,  ...,  0.0007, -0.0026,  0.0013],\n",
       "                      [ 0.0011, -0.0012, -0.0015,  ..., -0.0001,  0.0048, -0.0011],\n",
       "                      ...,\n",
       "                      [-0.0008, -0.0013, -0.0014,  ..., -0.0043, -0.0052, -0.0016],\n",
       "                      [-0.0009, -0.0025, -0.0016,  ..., -0.0007,  0.0017, -0.0012],\n",
       "                      [-0.0026,  0.0046, -0.0023,  ...,  0.0019, -0.0022,  0.0026]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.input_layernorm.weight',\n",
       "              tensor([2.0781, 3.5312, 2.3906,  ..., 1.8125, 1.9297, 2.1406],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.post_attention_layernorm.weight',\n",
       "              tensor([2.3125, 4.4375, 2.6719,  ..., 2.1562, 2.3125, 2.3281],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.q_proj.weight',\n",
       "              tensor([[ 3.4485e-03, -1.3123e-03,  9.6130e-04,  ...,  2.3346e-03,\n",
       "                        2.3651e-03, -3.0041e-05],\n",
       "                      [ 2.0599e-03,  2.5482e-03, -6.7711e-05,  ..., -9.5367e-04,\n",
       "                        1.0071e-03,  1.4496e-03],\n",
       "                      [ 2.5940e-03, -1.6098e-03, -6.4087e-04,  ...,  3.1586e-03,\n",
       "                        1.3113e-06, -2.4109e-03],\n",
       "                      ...,\n",
       "                      [-3.4027e-03,  6.7139e-03, -7.0190e-03,  ...,  1.5717e-03,\n",
       "                       -7.5531e-04,  4.6015e-05],\n",
       "                      [ 2.9755e-03,  1.4343e-03,  2.1744e-04,  ..., -4.2114e-03,\n",
       "                       -5.2490e-03, -1.9379e-03],\n",
       "                      [-1.6785e-04,  5.7678e-03,  6.9275e-03,  ...,  6.6528e-03,\n",
       "                        1.8768e-03,  1.7166e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.k_proj.weight',\n",
       "              tensor([[-1.4572e-03,  7.5150e-04, -7.2956e-05,  ..., -8.8811e-06,\n",
       "                        7.0953e-04, -1.8311e-03],\n",
       "                      [-3.3188e-04,  8.1539e-05, -4.0283e-03,  ...,  2.0695e-04,\n",
       "                       -3.7231e-03,  1.2970e-03],\n",
       "                      [-9.1171e-04,  1.3657e-03,  4.7302e-04,  ..., -3.9101e-04,\n",
       "                       -1.0910e-03,  4.1809e-03],\n",
       "                      ...,\n",
       "                      [-3.4943e-03, -1.2054e-03, -3.0823e-03,  ..., -8.5449e-04,\n",
       "                        5.8899e-03, -9.9487e-03],\n",
       "                      [ 3.1853e-04,  1.8692e-04,  5.7983e-03,  ..., -2.3956e-03,\n",
       "                       -6.1646e-03, -1.0757e-03],\n",
       "                      [ 6.1646e-03, -3.1738e-03,  3.6011e-03,  ...,  9.8877e-03,\n",
       "                       -1.0620e-02, -1.3199e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0002,  0.0011,  0.0010,  ...,  0.0042, -0.0009,  0.0023],\n",
       "                      [ 0.0005, -0.0007,  0.0015,  ...,  0.0008,  0.0015,  0.0003],\n",
       "                      [-0.0031,  0.0013,  0.0006,  ...,  0.0016,  0.0015,  0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0014,  0.0027,  0.0028,  ..., -0.0018,  0.0013, -0.0030],\n",
       "                      [ 0.0029,  0.0007,  0.0039,  ..., -0.0023, -0.0030,  0.0010],\n",
       "                      [-0.0019, -0.0014,  0.0005,  ...,  0.0019,  0.0004, -0.0002]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.o_proj.weight',\n",
       "              tensor([[-6.1951e-03,  3.1738e-03,  3.0060e-03,  ..., -2.5024e-03,\n",
       "                        9.7656e-04, -3.1433e-03],\n",
       "                      [ 1.3580e-03, -4.5586e-04, -2.0294e-03,  ...,  5.2452e-05,\n",
       "                        1.4038e-03,  5.5847e-03],\n",
       "                      [-8.9264e-04,  2.0905e-03, -5.6839e-04,  ...,  2.2583e-03,\n",
       "                        2.3499e-03, -4.4556e-03],\n",
       "                      ...,\n",
       "                      [ 6.5918e-03, -1.8463e-03, -1.7166e-04,  ..., -9.2697e-04,\n",
       "                       -1.6022e-03,  1.2207e-03],\n",
       "                      [ 1.8539e-03,  5.1880e-04, -2.6093e-03,  ...,  2.5034e-05,\n",
       "                        1.5106e-03, -2.8534e-03],\n",
       "                      [-1.6708e-03, -3.1662e-04, -9.4604e-03,  ..., -4.6387e-03,\n",
       "                        2.5940e-03, -2.5482e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.gate_proj.weight',\n",
       "              tensor([[-9.4223e-04,  2.3499e-03,  2.9602e-03,  ...,  4.9438e-03,\n",
       "                        1.9684e-03, -8.3542e-04],\n",
       "                      [ 3.4142e-04, -2.1172e-04, -2.5940e-04,  ..., -9.4986e-04,\n",
       "                        1.9150e-03, -2.0447e-03],\n",
       "                      [ 5.8289e-03,  2.9945e-04,  2.5482e-03,  ...,  1.2207e-03,\n",
       "                       -3.0060e-03,  1.7776e-03],\n",
       "                      ...,\n",
       "                      [-2.8381e-03, -2.2430e-03,  1.5564e-03,  ...,  1.0620e-02,\n",
       "                        3.3569e-03, -3.0823e-03],\n",
       "                      [ 6.4850e-04, -6.2561e-04,  1.9989e-03,  ..., -7.4863e-05,\n",
       "                        6.7139e-04, -1.4801e-03],\n",
       "                      [-9.7656e-04,  8.4229e-03,  1.8692e-03,  ..., -2.3041e-03,\n",
       "                        4.1199e-03,  1.9531e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.up_proj.weight',\n",
       "              tensor([[-5.8365e-04,  1.7166e-03,  4.2725e-03,  ...,  6.1035e-03,\n",
       "                        2.5177e-03, -8.2397e-04],\n",
       "                      [-7.4387e-05,  5.2261e-04,  1.3809e-03,  ...,  1.9741e-04,\n",
       "                        2.3346e-03,  9.2697e-04],\n",
       "                      [-5.7068e-03,  1.8616e-03, -2.1057e-03,  ...,  3.2654e-03,\n",
       "                        4.5013e-04, -4.7607e-03],\n",
       "                      ...,\n",
       "                      [ 9.8419e-04, -5.8289e-03,  3.3379e-04,  ..., -1.3199e-03,\n",
       "                       -1.5793e-03,  1.1292e-03],\n",
       "                      [ 2.4872e-03, -5.8746e-04,  2.9297e-03,  ..., -6.2180e-04,\n",
       "                       -5.4550e-04, -2.8229e-03],\n",
       "                      [ 8.6975e-04, -2.0142e-03, -1.8234e-03,  ..., -1.0529e-03,\n",
       "                       -3.7231e-03,  1.6174e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.down_proj.weight',\n",
       "              tensor([[-0.0029, -0.0038, -0.0040,  ...,  0.0035, -0.0017,  0.0022],\n",
       "                      [ 0.0041, -0.0033,  0.0031,  ..., -0.0049, -0.0008,  0.0018],\n",
       "                      [ 0.0005, -0.0059, -0.0007,  ...,  0.0005, -0.0029,  0.0002],\n",
       "                      ...,\n",
       "                      [ 0.0011, -0.0024, -0.0002,  ..., -0.0002,  0.0020, -0.0010],\n",
       "                      [-0.0053, -0.0005, -0.0003,  ...,  0.0006,  0.0029, -0.0020],\n",
       "                      [ 0.0049,  0.0029, -0.0029,  ...,  0.0003, -0.0063,  0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.input_layernorm.weight',\n",
       "              tensor([2.2812, 3.4531, 2.6406,  ..., 2.1094, 2.2500, 2.3594],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.post_attention_layernorm.weight',\n",
       "              tensor([2.4844, 5.0938, 2.7656,  ..., 2.3125, 2.4375, 2.4375],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.q_proj.weight',\n",
       "              tensor([[-1.0452e-03,  1.1826e-03,  4.0588e-03,  ...,  2.3346e-03,\n",
       "                       -6.1417e-04, -5.0735e-04],\n",
       "                      [ 2.1057e-03,  2.4414e-03, -2.2125e-03,  ..., -8.5068e-04,\n",
       "                        1.4305e-04,  4.0283e-03],\n",
       "                      [ 4.3335e-03, -8.3618e-03, -2.2736e-03,  ...,  1.9150e-03,\n",
       "                        6.1951e-03,  4.7607e-03],\n",
       "                      ...,\n",
       "                      [ 6.7711e-05,  6.7444e-03,  6.3477e-03,  ..., -2.7466e-03,\n",
       "                        1.6479e-03,  3.1433e-03],\n",
       "                      [-3.0670e-03,  1.3885e-03,  4.0588e-03,  ..., -5.0659e-03,\n",
       "                        7.7438e-04,  2.2583e-03],\n",
       "                      [ 1.6174e-03,  8.1177e-03,  3.1738e-03,  ..., -1.0834e-03,\n",
       "                        2.1362e-03,  2.1515e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.k_proj.weight',\n",
       "              tensor([[-4.9438e-03,  7.5684e-03,  2.6855e-03,  ...,  6.7444e-03,\n",
       "                        1.0757e-03, -1.5259e-03],\n",
       "                      [-1.0910e-03, -5.6763e-03,  4.1504e-03,  ...,  4.8065e-04,\n",
       "                       -1.6403e-03, -1.5020e-05],\n",
       "                      [-2.8992e-03, -5.9814e-03, -4.4861e-03,  ...,  5.2185e-03,\n",
       "                       -2.9297e-03,  3.9816e-05],\n",
       "                      ...,\n",
       "                      [ 2.6703e-03,  1.1597e-03,  3.9673e-03,  ...,  6.1417e-04,\n",
       "                       -8.8882e-04, -2.6550e-03],\n",
       "                      [ 1.6861e-03, -1.4893e-02,  5.8594e-03,  ...,  4.1504e-03,\n",
       "                       -7.5073e-03, -1.1169e-02],\n",
       "                      [ 2.0447e-03, -1.3580e-03, -4.6692e-03,  ..., -7.9956e-03,\n",
       "                        2.6093e-03, -7.0190e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.v_proj.weight',\n",
       "              tensor([[-3.2349e-03, -4.0283e-03,  2.0752e-03,  ...,  6.8665e-04,\n",
       "                       -3.7193e-04,  1.9836e-03],\n",
       "                      [ 8.8692e-05, -1.9226e-03,  2.5635e-03,  ..., -2.8076e-03,\n",
       "                       -1.2512e-03,  2.5024e-03],\n",
       "                      [-8.8882e-04, -4.6997e-03, -3.8605e-03,  ...,  6.5613e-03,\n",
       "                       -5.9128e-04,  1.3809e-03],\n",
       "                      ...,\n",
       "                      [-1.4114e-03,  8.4686e-04,  4.0588e-03,  ...,  7.7438e-04,\n",
       "                       -8.9111e-03,  2.5330e-03],\n",
       "                      [ 1.6022e-03,  8.0109e-04, -2.7618e-03,  ...,  3.8910e-04,\n",
       "                        3.0365e-03, -4.1809e-03],\n",
       "                      [ 9.6130e-04, -2.9449e-03, -3.7537e-03,  ..., -2.1515e-03,\n",
       "                       -3.2663e-05, -1.2131e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.o_proj.weight',\n",
       "              tensor([[-1.2817e-03, -1.0376e-03, -3.8757e-03,  ...,  2.9755e-03,\n",
       "                        5.7983e-03,  2.4109e-03],\n",
       "                      [ 3.7384e-03,  2.1515e-03, -3.7079e-03,  ..., -1.3275e-03,\n",
       "                        1.4954e-03, -1.2741e-03],\n",
       "                      [ 1.4496e-03, -4.7607e-03,  2.1057e-03,  ...,  2.5482e-03,\n",
       "                        9.1553e-04, -2.9907e-03],\n",
       "                      ...,\n",
       "                      [ 2.8381e-03, -9.5749e-04, -7.3242e-03,  ..., -6.8665e-04,\n",
       "                       -1.3123e-03, -2.4567e-03],\n",
       "                      [-2.0905e-03, -8.1539e-05,  4.1199e-03,  ..., -1.2665e-03,\n",
       "                       -2.6245e-03,  2.0752e-03],\n",
       "                      [ 4.5776e-03, -9.2697e-04,  1.6022e-03,  ..., -1.5488e-03,\n",
       "                        1.0529e-03, -1.1292e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0032, -0.0023, -0.0027,  ..., -0.0009, -0.0064, -0.0039],\n",
       "                      [-0.0002, -0.0003, -0.0016,  ..., -0.0039,  0.0030, -0.0042],\n",
       "                      [ 0.0044,  0.0078, -0.0004,  ..., -0.0070,  0.0020,  0.0042],\n",
       "                      ...,\n",
       "                      [-0.0041, -0.0001,  0.0045,  ..., -0.0033, -0.0071,  0.0005],\n",
       "                      [-0.0033,  0.0009, -0.0024,  ..., -0.0011, -0.0008,  0.0021],\n",
       "                      [ 0.0015,  0.0011, -0.0026,  ...,  0.0006, -0.0008,  0.0017]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.up_proj.weight',\n",
       "              tensor([[-0.0050, -0.0047, -0.0055,  ...,  0.0017,  0.0037, -0.0038],\n",
       "                      [-0.0020, -0.0002, -0.0077,  ...,  0.0009, -0.0013,  0.0031],\n",
       "                      [-0.0025, -0.0057,  0.0019,  ...,  0.0032, -0.0047, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0023, -0.0004, -0.0010,  ...,  0.0079, -0.0004, -0.0033],\n",
       "                      [-0.0029, -0.0007,  0.0112,  ..., -0.0023, -0.0044,  0.0003],\n",
       "                      [-0.0002, -0.0002, -0.0054,  ..., -0.0008,  0.0047, -0.0013]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.down_proj.weight',\n",
       "              tensor([[-0.0014,  0.0078, -0.0052,  ...,  0.0023, -0.0020,  0.0002],\n",
       "                      [-0.0032, -0.0041, -0.0015,  ..., -0.0011,  0.0023,  0.0023],\n",
       "                      [-0.0011, -0.0036, -0.0015,  ..., -0.0045,  0.0029, -0.0064],\n",
       "                      ...,\n",
       "                      [ 0.0009,  0.0033, -0.0004,  ..., -0.0004, -0.0002, -0.0042],\n",
       "                      [ 0.0009,  0.0015, -0.0035,  ...,  0.0029,  0.0010,  0.0065],\n",
       "                      [-0.0049, -0.0003, -0.0011,  ..., -0.0020,  0.0051,  0.0027]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.input_layernorm.weight',\n",
       "              tensor([2.4219, 3.2656, 2.7031,  ..., 2.2500, 2.2812, 2.4375],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.post_attention_layernorm.weight',\n",
       "              tensor([2.6250, 3.7031, 2.9062,  ..., 2.5000, 2.6562, 2.6094],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.q_proj.weight',\n",
       "              tensor([[-2.5482e-03, -4.4556e-03, -3.3417e-03,  ...,  3.1433e-03,\n",
       "                       -3.7766e-04, -1.5259e-04],\n",
       "                      [ 2.0752e-03,  5.2261e-04, -1.1292e-03,  ...,  1.8387e-03,\n",
       "                       -1.3962e-03,  1.6937e-03],\n",
       "                      [-9.5367e-04,  2.5940e-03, -3.5858e-03,  ...,  4.2725e-04,\n",
       "                       -3.0823e-03, -7.9346e-03],\n",
       "                      ...,\n",
       "                      [-2.0294e-03,  6.1989e-05, -4.4861e-03,  ..., -2.2125e-03,\n",
       "                       -1.5450e-04,  4.4250e-03],\n",
       "                      [ 1.4267e-03,  2.1820e-03,  2.1820e-03,  ...,  1.1673e-03,\n",
       "                        1.7471e-03,  2.2278e-03],\n",
       "                      [ 2.6398e-03,  6.0320e-05,  2.9144e-03,  ...,  2.3499e-03,\n",
       "                        3.9673e-03, -2.5940e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.k_proj.weight',\n",
       "              tensor([[-4.5776e-03, -3.5858e-03, -4.2725e-03,  ...,  3.0518e-03,\n",
       "                       -2.3346e-03, -1.6403e-03],\n",
       "                      [ 2.0123e-04,  2.2736e-03, -1.5411e-03,  ...,  1.9989e-03,\n",
       "                       -8.7738e-04, -1.4038e-03],\n",
       "                      [ 1.0834e-03,  2.5787e-03,  2.8038e-04,  ..., -1.6022e-03,\n",
       "                       -3.5095e-03, -4.6997e-03],\n",
       "                      ...,\n",
       "                      [ 1.3428e-03,  4.1199e-03,  2.0599e-03,  ..., -4.7913e-03,\n",
       "                        4.7302e-03, -2.7313e-03],\n",
       "                      [-1.5564e-03,  1.2207e-03,  1.1673e-03,  ..., -3.5095e-03,\n",
       "                        6.7749e-03,  8.3923e-05],\n",
       "                      [-1.1301e-04, -5.0354e-03, -1.1597e-03,  ...,  6.7749e-03,\n",
       "                       -2.8229e-03, -4.4250e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0016,  0.0024,  0.0008,  ..., -0.0001,  0.0014, -0.0015],\n",
       "                      [ 0.0011, -0.0011,  0.0036,  ...,  0.0003, -0.0010,  0.0002],\n",
       "                      [ 0.0033, -0.0021,  0.0018,  ...,  0.0016,  0.0012,  0.0008],\n",
       "                      ...,\n",
       "                      [ 0.0010,  0.0025, -0.0028,  ...,  0.0021, -0.0025, -0.0031],\n",
       "                      [-0.0016, -0.0008, -0.0016,  ..., -0.0017,  0.0056,  0.0009],\n",
       "                      [ 0.0010,  0.0020, -0.0012,  ..., -0.0013,  0.0057, -0.0025]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.o_proj.weight',\n",
       "              tensor([[-1.4572e-03, -1.7700e-03, -8.8120e-04,  ..., -5.5695e-04,\n",
       "                       -1.8787e-04, -4.6692e-03],\n",
       "                      [ 4.3678e-04, -8.4400e-05, -3.3112e-03,  ..., -3.2616e-04,\n",
       "                       -4.1504e-03, -2.8801e-04],\n",
       "                      [-1.7319e-03, -2.9907e-03,  2.7618e-03,  ..., -2.1057e-03,\n",
       "                       -1.8311e-03, -1.1063e-03],\n",
       "                      ...,\n",
       "                      [-1.4725e-03,  1.4572e-03, -8.7357e-04,  ...,  3.5095e-03,\n",
       "                        8.8882e-04,  1.1139e-03],\n",
       "                      [-7.8201e-04, -1.8997e-03,  3.6469e-03,  ..., -5.1117e-04,\n",
       "                       -5.9204e-03,  3.1281e-03],\n",
       "                      [-3.4943e-03, -2.7618e-03,  3.0060e-03,  ..., -9.2697e-04,\n",
       "                       -2.8687e-03,  2.1210e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0015, -0.0026,  0.0042,  ..., -0.0053, -0.0013, -0.0008],\n",
       "                      [ 0.0002,  0.0022, -0.0030,  ..., -0.0031,  0.0031, -0.0030],\n",
       "                      [ 0.0001,  0.0002,  0.0045,  ..., -0.0039,  0.0022,  0.0029],\n",
       "                      ...,\n",
       "                      [-0.0029,  0.0004,  0.0035,  ...,  0.0027,  0.0047,  0.0035],\n",
       "                      [ 0.0002,  0.0011, -0.0003,  ..., -0.0010,  0.0049, -0.0028],\n",
       "                      [ 0.0023,  0.0024, -0.0028,  ..., -0.0023, -0.0033, -0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0006,  0.0022, -0.0020,  ..., -0.0001, -0.0028, -0.0006],\n",
       "                      [ 0.0031,  0.0017,  0.0002,  ...,  0.0004,  0.0046, -0.0011],\n",
       "                      [ 0.0040,  0.0053, -0.0026,  ...,  0.0002, -0.0008,  0.0009],\n",
       "                      ...,\n",
       "                      [-0.0021, -0.0008, -0.0007,  ...,  0.0003,  0.0005,  0.0010],\n",
       "                      [-0.0019, -0.0022, -0.0025,  ...,  0.0006, -0.0009, -0.0029],\n",
       "                      [ 0.0034,  0.0020,  0.0023,  ..., -0.0002,  0.0003,  0.0036]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.down_proj.weight',\n",
       "              tensor([[-4.8828e-04, -5.4932e-04,  3.6469e-03,  ..., -6.7749e-03,\n",
       "                        4.5166e-03,  2.7924e-03],\n",
       "                      [-9.7275e-04,  1.9836e-03, -2.2888e-03,  ...,  2.1362e-04,\n",
       "                        3.2196e-03, -7.0953e-04],\n",
       "                      [ 1.3504e-03,  9.8419e-04,  1.8082e-03,  ...,  6.0654e-04,\n",
       "                       -2.8419e-04,  6.4373e-05],\n",
       "                      ...,\n",
       "                      [ 1.6479e-03, -4.6082e-03, -9.3842e-04,  ..., -3.1128e-03,\n",
       "                        1.6556e-03, -7.4387e-04],\n",
       "                      [-3.9482e-04,  1.1520e-03, -5.7220e-04,  ..., -1.1520e-03,\n",
       "                       -2.9297e-03, -4.0054e-04],\n",
       "                      [ 3.1433e-03, -2.2278e-03,  9.8419e-04,  ...,  8.8882e-04,\n",
       "                       -2.9755e-03,  3.8605e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.20.input_layernorm.weight',\n",
       "              tensor([2.5781, 3.3281, 2.4844,  ..., 2.2500, 2.3750, 2.3594],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.post_attention_layernorm.weight',\n",
       "              tensor([2.8594, 4.6562, 3.0625,  ..., 2.7500, 2.8906, 2.8125],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.q_proj.weight',\n",
       "              tensor([[ 6.8665e-03,  4.0588e-03,  3.8452e-03,  ..., -2.2430e-03,\n",
       "                        1.8158e-03,  1.6174e-03],\n",
       "                      [-1.9684e-03,  1.9264e-04,  1.5717e-03,  ..., -3.7689e-03,\n",
       "                       -1.7929e-03,  2.3499e-03],\n",
       "                      [-7.5684e-03,  2.3041e-03, -1.6556e-03,  ..., -1.1368e-03,\n",
       "                        1.4603e-05,  3.1128e-03],\n",
       "                      ...,\n",
       "                      [-1.6785e-03,  5.3406e-04, -2.3556e-04,  ..., -5.8899e-03,\n",
       "                        2.2736e-03, -4.1809e-03],\n",
       "                      [-1.3580e-03,  2.5940e-03, -4.1008e-04,  ..., -7.5073e-03,\n",
       "                        9.2697e-04, -2.2125e-03],\n",
       "                      [ 8.0566e-03,  8.3618e-03, -1.4267e-03,  ..., -3.7994e-03,\n",
       "                       -4.7302e-03,  8.1635e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0042, -0.0005, -0.0023,  ...,  0.0012,  0.0047, -0.0007],\n",
       "                      [ 0.0005, -0.0037,  0.0011,  ..., -0.0017,  0.0005,  0.0029],\n",
       "                      [-0.0004,  0.0011, -0.0036,  ...,  0.0013,  0.0024,  0.0009],\n",
       "                      ...,\n",
       "                      [-0.0027, -0.0084,  0.0019,  ...,  0.0014,  0.0007, -0.0021],\n",
       "                      [-0.0054,  0.0025, -0.0009,  ..., -0.0084, -0.0056,  0.0018],\n",
       "                      [ 0.0059, -0.0034, -0.0006,  ...,  0.0032,  0.0058,  0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.v_proj.weight',\n",
       "              tensor([[ 1.7624e-03, -8.3447e-05,  9.3079e-04,  ...,  3.6926e-03,\n",
       "                        6.4087e-04, -3.4485e-03],\n",
       "                      [ 1.8539e-03,  2.0905e-03, -2.6855e-03,  ...,  2.1210e-03,\n",
       "                       -2.7313e-03,  3.8300e-03],\n",
       "                      [ 1.9531e-03, -5.4321e-03, -1.4210e-04,  ..., -3.9062e-03,\n",
       "                       -2.7161e-03, -6.5613e-03],\n",
       "                      ...,\n",
       "                      [-2.8534e-03, -2.9564e-04, -1.5106e-03,  ...,  4.0283e-03,\n",
       "                        2.5177e-03,  6.6376e-04],\n",
       "                      [-2.2278e-03,  1.1873e-04, -8.3542e-04,  ..., -2.8381e-03,\n",
       "                       -1.4954e-03, -1.1597e-03],\n",
       "                      [-6.2561e-04, -1.2302e-04, -9.0027e-04,  ...,  1.3504e-03,\n",
       "                        7.4768e-04,  3.0670e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.o_proj.weight',\n",
       "              tensor([[-2.2697e-04,  1.6327e-03, -9.0790e-04,  ..., -2.5177e-03,\n",
       "                       -1.0133e-05, -3.3855e-05],\n",
       "                      [-5.8365e-04, -1.0757e-03, -3.0823e-03,  ...,  1.9150e-03,\n",
       "                       -1.1139e-03,  2.3193e-03],\n",
       "                      [-9.0027e-04,  2.4872e-03, -4.8256e-04,  ...,  1.8463e-03,\n",
       "                        1.7643e-04,  4.0894e-03],\n",
       "                      ...,\n",
       "                      [-4.5776e-04, -7.6294e-04, -7.2021e-03,  ...,  3.2349e-03,\n",
       "                        6.7444e-03, -6.9427e-04],\n",
       "                      [ 1.6098e-03,  9.2316e-04,  3.0823e-03,  ...,  1.4648e-03,\n",
       "                        4.3640e-03,  6.2256e-03],\n",
       "                      [ 3.2806e-03,  4.2114e-03,  3.0975e-03,  ..., -3.7689e-03,\n",
       "                        7.1716e-04, -7.2098e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0011, -0.0039, -0.0004,  ...,  0.0012, -0.0022,  0.0040],\n",
       "                      [-0.0002,  0.0038,  0.0016,  ...,  0.0051,  0.0030,  0.0005],\n",
       "                      [-0.0039, -0.0058, -0.0011,  ..., -0.0006, -0.0029, -0.0045],\n",
       "                      ...,\n",
       "                      [-0.0022,  0.0031, -0.0021,  ..., -0.0021,  0.0019,  0.0037],\n",
       "                      [-0.0082, -0.0004, -0.0014,  ..., -0.0009, -0.0023,  0.0008],\n",
       "                      [-0.0066, -0.0050,  0.0003,  ..., -0.0019, -0.0023, -0.0097]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.up_proj.weight',\n",
       "              tensor([[-1.9169e-04, -9.5749e-04,  4.0894e-03,  ..., -3.5095e-03,\n",
       "                        4.2725e-04,  5.6152e-03],\n",
       "                      [-2.1458e-04, -1.5640e-03,  1.2970e-03,  ..., -1.5869e-03,\n",
       "                        2.1057e-03,  1.1978e-03],\n",
       "                      [ 6.3782e-03,  1.7624e-03,  6.6223e-03,  ...,  3.1128e-03,\n",
       "                        1.8311e-03,  2.2278e-03],\n",
       "                      ...,\n",
       "                      [ 7.5073e-03, -8.7738e-04, -4.9133e-03,  ...,  5.5542e-03,\n",
       "                       -3.9368e-03,  2.6550e-03],\n",
       "                      [-4.4556e-03, -4.8876e-06, -5.9204e-03,  ..., -2.7084e-04,\n",
       "                       -2.6703e-04, -1.1063e-03],\n",
       "                      [-9.4891e-05,  4.3640e-03,  3.2349e-03,  ..., -3.5095e-03,\n",
       "                       -2.3499e-03,  4.2915e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.down_proj.weight',\n",
       "              tensor([[-2.5787e-03,  2.0599e-03,  3.5248e-03,  ...,  2.7924e-03,\n",
       "                        2.4872e-03,  1.6174e-03],\n",
       "                      [-5.7678e-03,  1.3885e-03, -1.8616e-03,  ..., -3.9673e-04,\n",
       "                        2.8229e-04,  1.0223e-03],\n",
       "                      [ 4.5776e-03,  2.5635e-03,  3.8300e-03,  ..., -3.8300e-03,\n",
       "                       -2.6245e-03,  5.8594e-03],\n",
       "                      ...,\n",
       "                      [-1.8692e-03, -1.8311e-03, -1.2436e-03,  ..., -6.0425e-03,\n",
       "                        1.5335e-03, -3.7842e-03],\n",
       "                      [-2.9449e-03,  8.3160e-04,  4.6082e-03,  ..., -5.2185e-03,\n",
       "                        6.5327e-05,  4.1389e-04],\n",
       "                      [ 2.3651e-03, -1.4267e-03, -6.1340e-03,  ..., -7.3242e-04,\n",
       "                        1.1978e-03,  2.0905e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.21.input_layernorm.weight',\n",
       "              tensor([2.4062, 2.7656, 2.5000,  ..., 2.2188, 2.3125, 2.4062],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.post_attention_layernorm.weight',\n",
       "              tensor([3.0781, 3.9062, 3.3125,  ..., 2.9531, 3.1094, 3.0312],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0006, -0.0040, -0.0010,  ...,  0.0047,  0.0020,  0.0064],\n",
       "                      [-0.0068, -0.0033, -0.0028,  ..., -0.0064, -0.0028, -0.0004],\n",
       "                      [ 0.0063, -0.0010,  0.0030,  ..., -0.0042,  0.0028,  0.0011],\n",
       "                      ...,\n",
       "                      [ 0.0017,  0.0008, -0.0030,  ...,  0.0006,  0.0029,  0.0017],\n",
       "                      [-0.0035, -0.0038, -0.0014,  ..., -0.0033, -0.0058,  0.0023],\n",
       "                      [-0.0033,  0.0032, -0.0031,  ...,  0.0054, -0.0004, -0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.k_proj.weight',\n",
       "              tensor([[-8.4877e-05,  2.1973e-03, -3.5095e-04,  ..., -2.6550e-03,\n",
       "                        3.0212e-03, -1.4191e-03],\n",
       "                      [-1.0347e-04,  2.3079e-04,  3.8300e-03,  ..., -1.1673e-03,\n",
       "                        3.2501e-03,  9.6512e-04],\n",
       "                      [-2.3365e-04, -7.5150e-04,  2.5635e-03,  ..., -8.5831e-04,\n",
       "                        8.8882e-04, -8.8882e-04],\n",
       "                      ...,\n",
       "                      [-9.4986e-04, -3.9864e-04, -1.5030e-03,  ..., -9.5825e-03,\n",
       "                        8.2397e-04,  4.4823e-05],\n",
       "                      [ 7.7209e-03, -6.5613e-03,  6.9427e-04,  ..., -1.7319e-03,\n",
       "                       -5.7678e-03,  2.8076e-03],\n",
       "                      [-3.5248e-03,  2.4872e-03,  5.1117e-04,  ...,  2.0142e-03,\n",
       "                       -1.8539e-03,  2.4414e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0041, -0.0042, -0.0018,  ...,  0.0005,  0.0019,  0.0020],\n",
       "                      [ 0.0018,  0.0004, -0.0015,  ..., -0.0017,  0.0040,  0.0078],\n",
       "                      [-0.0027, -0.0021,  0.0018,  ...,  0.0013, -0.0024, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0051, -0.0016,  0.0021,  ...,  0.0031,  0.0010,  0.0004],\n",
       "                      [-0.0005,  0.0006, -0.0028,  ...,  0.0073, -0.0016,  0.0029],\n",
       "                      [-0.0009,  0.0009, -0.0022,  ...,  0.0005, -0.0038,  0.0008]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.o_proj.weight',\n",
       "              tensor([[-5.3024e-04, -1.8921e-03,  5.7373e-03,  ..., -3.8910e-04,\n",
       "                        1.0834e-03,  1.1292e-03],\n",
       "                      [ 2.7657e-04, -9.0790e-04, -1.8463e-03,  ...,  2.0294e-03,\n",
       "                       -1.2283e-03,  1.0376e-03],\n",
       "                      [-2.9755e-03, -5.7602e-04, -1.8387e-03,  ...,  6.1951e-03,\n",
       "                       -1.0300e-03, -1.6403e-03],\n",
       "                      ...,\n",
       "                      [ 2.1553e-04, -2.6550e-03, -2.8038e-04,  ...,  1.6403e-03,\n",
       "                        2.6245e-03,  1.8311e-03],\n",
       "                      [-2.7008e-03,  6.0425e-03,  3.2196e-03,  ...,  3.0365e-03,\n",
       "                        9.8419e-04, -3.5286e-05],\n",
       "                      [-4.2152e-04,  9.3384e-03,  3.0670e-03,  ...,  1.4114e-03,\n",
       "                        3.3722e-03, -1.9169e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0047,  0.0022,  0.0025,  ...,  0.0005, -0.0022, -0.0031],\n",
       "                      [-0.0020, -0.0047,  0.0010,  ...,  0.0068, -0.0041, -0.0045],\n",
       "                      [-0.0051, -0.0024,  0.0085,  ...,  0.0031, -0.0002,  0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0007, -0.0008, -0.0014,  ...,  0.0013, -0.0002, -0.0037],\n",
       "                      [ 0.0005,  0.0012, -0.0098,  ..., -0.0016, -0.0020,  0.0013],\n",
       "                      [ 0.0031,  0.0036,  0.0004,  ..., -0.0003, -0.0020, -0.0013]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.up_proj.weight',\n",
       "              tensor([[ 4.8523e-03,  4.7874e-04, -4.5166e-03,  ..., -4.9744e-03,\n",
       "                        1.6251e-03, -1.3885e-03],\n",
       "                      [ 6.8359e-03, -3.5858e-03, -4.3335e-03,  ...,  2.9144e-03,\n",
       "                        1.1749e-03, -1.6556e-03],\n",
       "                      [ 1.5945e-03,  1.7452e-04, -7.2479e-04,  ...,  2.7924e-03,\n",
       "                        3.3264e-03,  4.0283e-03],\n",
       "                      ...,\n",
       "                      [ 3.0823e-03,  2.1667e-03,  4.0627e-04,  ...,  3.4027e-03,\n",
       "                        6.7139e-04, -3.6774e-03],\n",
       "                      [-2.7847e-04, -4.4346e-05,  1.0986e-03,  ...,  3.3722e-03,\n",
       "                       -6.5804e-05,  1.0147e-03],\n",
       "                      [-6.0120e-03, -1.3657e-03,  1.4572e-03,  ...,  1.1215e-03,\n",
       "                        2.8992e-03,  1.4305e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.down_proj.weight',\n",
       "              tensor([[-0.0032, -0.0009,  0.0015,  ...,  0.0033,  0.0045, -0.0018],\n",
       "                      [-0.0034, -0.0025,  0.0015,  ...,  0.0056,  0.0079, -0.0009],\n",
       "                      [-0.0013, -0.0048,  0.0008,  ..., -0.0006,  0.0045,  0.0020],\n",
       "                      ...,\n",
       "                      [ 0.0014,  0.0004,  0.0024,  ..., -0.0059,  0.0027, -0.0054],\n",
       "                      [-0.0018,  0.0044,  0.0039,  ..., -0.0045, -0.0003,  0.0026],\n",
       "                      [ 0.0019,  0.0024, -0.0007,  ...,  0.0010, -0.0010,  0.0004]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.input_layernorm.weight',\n",
       "              tensor([2.4375, 2.5469, 2.4688,  ..., 2.3125, 2.3281, 2.4219],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.post_attention_layernorm.weight',\n",
       "              tensor([3.2344, 3.7500, 3.3906,  ..., 3.0938, 3.2188, 3.1562],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.q_proj.weight',\n",
       "              tensor([[-5.0659e-03, -1.4343e-03, -1.9379e-03,  ..., -1.9226e-03,\n",
       "                       -2.9182e-04,  2.7618e-03],\n",
       "                      [-9.6130e-04,  3.4180e-03,  1.5411e-03,  ...,  1.7548e-03,\n",
       "                        3.8605e-03, -1.5926e-04],\n",
       "                      [-4.4441e-04,  3.4485e-03, -2.1973e-03,  ..., -2.3651e-03,\n",
       "                        6.0272e-04, -3.0823e-03],\n",
       "                      ...,\n",
       "                      [-1.0986e-02, -4.0283e-03,  4.4250e-03,  ..., -5.9204e-03,\n",
       "                       -7.2861e-04,  1.2512e-03],\n",
       "                      [ 2.1362e-03, -5.4836e-05, -1.8845e-03,  ...,  1.2436e-03,\n",
       "                       -2.0294e-03, -1.7262e-04],\n",
       "                      [-1.1215e-03, -1.7242e-03,  9.9182e-04,  ..., -2.9755e-03,\n",
       "                       -3.0670e-03,  1.9302e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.k_proj.weight',\n",
       "              tensor([[ 2.5787e-03, -1.6861e-03, -3.7842e-03,  ...,  1.1368e-03,\n",
       "                        4.1580e-04,  5.6458e-03],\n",
       "                      [-2.8687e-03,  6.1646e-03,  8.7619e-06,  ..., -4.8523e-03,\n",
       "                       -2.7924e-03, -4.4556e-03],\n",
       "                      [-1.6022e-03,  6.1951e-03,  3.2959e-03,  ..., -2.9602e-03,\n",
       "                        3.2043e-03,  2.0599e-03],\n",
       "                      ...,\n",
       "                      [ 1.5640e-04,  6.3705e-04, -3.9062e-03,  ..., -2.1210e-03,\n",
       "                       -3.6469e-03, -2.1839e-04],\n",
       "                      [ 4.4556e-03, -9.2163e-03, -4.3030e-03,  ..., -4.7607e-03,\n",
       "                        1.4038e-03, -3.2806e-03],\n",
       "                      [-1.0071e-03, -2.7924e-03,  3.3875e-03,  ...,  1.7014e-03,\n",
       "                        2.1515e-03, -9.0942e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.v_proj.weight',\n",
       "              tensor([[-6.4087e-03,  5.2929e-05,  5.2490e-03,  ..., -8.3923e-04,\n",
       "                        3.7766e-04, -4.7302e-03],\n",
       "                      [-3.3188e-04,  6.9427e-04, -4.1199e-03,  ..., -1.1921e-04,\n",
       "                        3.7842e-03,  8.8501e-04],\n",
       "                      [-4.9973e-04, -4.6692e-03, -4.0283e-03,  ..., -1.3580e-03,\n",
       "                        5.3406e-04, -3.9978e-03],\n",
       "                      ...,\n",
       "                      [ 2.0294e-03,  3.4142e-04,  1.6327e-03,  ..., -2.5330e-03,\n",
       "                        6.1798e-04, -3.0212e-03],\n",
       "                      [ 2.5330e-03,  8.8882e-04,  5.4321e-03,  ...,  2.5330e-03,\n",
       "                       -2.6398e-03, -1.0538e-04],\n",
       "                      [-1.9379e-03,  3.5095e-03, -1.0681e-03,  ..., -1.9684e-03,\n",
       "                        3.4332e-03, -1.5640e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.3504e-03, -1.3351e-03, -3.0975e-03,  ...,  1.1597e-03,\n",
       "                        2.7161e-03, -1.1597e-03],\n",
       "                      [ 1.3275e-03, -1.7548e-03,  2.3804e-03,  ..., -7.9727e-04,\n",
       "                       -2.4261e-03, -4.3106e-04],\n",
       "                      [ 1.1444e-03,  1.8387e-03,  4.9133e-03,  ...,  7.2632e-03,\n",
       "                       -1.6479e-03,  1.2360e-03],\n",
       "                      ...,\n",
       "                      [-1.3733e-03,  2.2793e-04, -1.2207e-03,  ...,  2.1973e-03,\n",
       "                        8.3542e-04, -2.3346e-03],\n",
       "                      [ 1.2696e-05,  5.2643e-04,  6.6757e-05,  ..., -4.9591e-04,\n",
       "                        6.5994e-04, -2.6550e-03],\n",
       "                      [-5.5695e-04,  1.2970e-03, -3.0975e-03,  ..., -1.9150e-03,\n",
       "                        5.2261e-04,  3.2043e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0051,  0.0026,  0.0020,  ...,  0.0009, -0.0052, -0.0008],\n",
       "                      [ 0.0028, -0.0094,  0.0027,  ..., -0.0009,  0.0007, -0.0005],\n",
       "                      [ 0.0070, -0.0035, -0.0018,  ...,  0.0030,  0.0023,  0.0013],\n",
       "                      ...,\n",
       "                      [-0.0034, -0.0012,  0.0035,  ...,  0.0013, -0.0025, -0.0003],\n",
       "                      [ 0.0007,  0.0019,  0.0026,  ...,  0.0018,  0.0006, -0.0022],\n",
       "                      [-0.0018,  0.0050,  0.0012,  ..., -0.0018, -0.0002, -0.0061]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.up_proj.weight',\n",
       "              tensor([[ 2.5177e-03, -1.6332e-05,  1.9379e-03,  ...,  8.7357e-04,\n",
       "                        3.3379e-04,  5.3711e-03],\n",
       "                      [-4.6158e-04, -2.1553e-04,  5.3406e-03,  ..., -1.9836e-04,\n",
       "                        2.8229e-03,  1.4191e-03],\n",
       "                      [ 6.3705e-04,  4.3945e-03, -3.7994e-03,  ..., -2.3804e-03,\n",
       "                        9.0790e-04, -1.4954e-03],\n",
       "                      ...,\n",
       "                      [-6.3171e-03,  6.5231e-04, -4.4250e-03,  ...,  5.1270e-03,\n",
       "                        3.9673e-03, -7.5912e-04],\n",
       "                      [ 1.0757e-03,  6.2180e-04, -3.2349e-03,  ...,  7.2861e-04,\n",
       "                        1.9789e-05, -8.2016e-04],\n",
       "                      [ 7.7057e-04,  3.7231e-03,  5.6076e-04,  ..., -6.5918e-03,\n",
       "                        1.2665e-03,  1.8539e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.down_proj.weight',\n",
       "              tensor([[-1.5030e-03,  1.9989e-03,  1.0605e-03,  ...,  3.3417e-03,\n",
       "                        9.4604e-04,  3.7689e-03],\n",
       "                      [-3.8300e-03, -2.0752e-03,  1.2894e-03,  ..., -2.9755e-03,\n",
       "                       -9.4414e-05,  1.2131e-03],\n",
       "                      [-3.5400e-03, -4.1771e-04, -6.7520e-04,  ...,  2.1076e-04,\n",
       "                       -1.8997e-03, -1.4725e-03],\n",
       "                      ...,\n",
       "                      [-2.0599e-03, -2.4261e-03, -2.9907e-03,  ..., -1.7395e-03,\n",
       "                       -2.2278e-03,  4.3869e-04],\n",
       "                      [-1.3123e-03, -1.1265e-05, -1.2436e-03,  ..., -8.3923e-04,\n",
       "                       -2.3041e-03,  5.4321e-03],\n",
       "                      [ 8.1635e-04, -3.3264e-03,  4.2725e-03,  ..., -1.9741e-04,\n",
       "                       -2.0599e-03, -3.0060e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.23.input_layernorm.weight',\n",
       "              tensor([2.5156, 2.6719, 2.5000,  ..., 2.3281, 2.4062, 2.4531],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.post_attention_layernorm.weight',\n",
       "              tensor([3.3906, 3.7969, 3.5156,  ..., 3.2188, 3.3281, 3.3125],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.q_proj.weight',\n",
       "              tensor([[-4.5013e-04, -1.6022e-03,  1.8978e-04,  ...,  4.7112e-04,\n",
       "                        1.5945e-03, -1.3275e-03],\n",
       "                      [-1.1749e-03, -1.3428e-03,  4.7913e-03,  ..., -3.4180e-03,\n",
       "                       -1.4496e-04,  7.8201e-05],\n",
       "                      [ 2.8839e-03,  3.7689e-03,  6.6833e-03,  ...,  2.0599e-03,\n",
       "                       -3.0518e-03, -1.0452e-03],\n",
       "                      ...,\n",
       "                      [-4.4861e-03,  3.6469e-03,  1.6556e-03,  ...,  2.6245e-03,\n",
       "                        4.3945e-03,  6.2943e-04],\n",
       "                      [-2.8419e-04, -1.2741e-03, -6.0425e-03,  ..., -4.9438e-03,\n",
       "                       -8.6594e-04, -5.1880e-03],\n",
       "                      [ 3.7384e-03,  6.8665e-03, -5.7220e-04,  ..., -2.6584e-05,\n",
       "                        7.0496e-03,  1.0223e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.9836e-03, -4.3945e-03,  3.3875e-03,  ...,  4.4861e-03,\n",
       "                       -2.1210e-03,  1.1749e-03],\n",
       "                      [ 5.4550e-04, -2.9602e-03,  9.6130e-04,  ...,  4.1199e-03,\n",
       "                        8.8882e-04,  5.3406e-05],\n",
       "                      [-2.9449e-03, -4.7913e-03,  8.8501e-04,  ...,  4.0894e-03,\n",
       "                       -3.1662e-04, -2.0294e-03],\n",
       "                      ...,\n",
       "                      [ 2.6398e-03, -2.9755e-04, -4.8523e-03,  ...,  5.4016e-03,\n",
       "                        5.4016e-03,  6.3782e-03],\n",
       "                      [ 8.0566e-03,  1.1841e-02,  1.4420e-03,  ...,  3.6049e-04,\n",
       "                        1.2817e-02, -7.7820e-03],\n",
       "                      [-4.6082e-03,  1.9531e-03, -9.1553e-03,  ...,  1.1520e-03,\n",
       "                       -6.4087e-04,  3.2959e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.v_proj.weight',\n",
       "              tensor([[-7.5150e-04,  1.8768e-03, -2.4109e-03,  ...,  1.6327e-03,\n",
       "                       -4.1199e-03, -8.6594e-04],\n",
       "                      [-5.1498e-05,  1.3351e-03, -3.2501e-03,  ..., -3.2806e-03,\n",
       "                       -4.3030e-03,  3.1128e-03],\n",
       "                      [-4.3945e-03,  1.8082e-03,  6.1340e-03,  ...,  3.5553e-03,\n",
       "                        8.2016e-04,  4.1809e-03],\n",
       "                      ...,\n",
       "                      [-5.9509e-03, -6.5918e-03,  1.7014e-03,  ..., -1.4343e-03,\n",
       "                        4.6692e-03,  6.4087e-04],\n",
       "                      [ 2.3499e-03, -3.8605e-03, -2.5330e-03,  ...,  1.4496e-03,\n",
       "                       -6.0272e-04, -4.5471e-03],\n",
       "                      [ 3.2806e-03,  4.4861e-03, -2.9449e-03,  ..., -4.1962e-04,\n",
       "                        3.3951e-04,  1.6785e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.o_proj.weight',\n",
       "              tensor([[ 3.6316e-03, -2.4567e-03,  4.5166e-03,  ...,  1.8120e-04,\n",
       "                       -3.9368e-03,  2.3842e-04],\n",
       "                      [ 1.4343e-03,  2.3651e-03,  1.3046e-03,  ..., -7.2956e-05,\n",
       "                       -1.8158e-03,  1.6937e-03],\n",
       "                      [-5.1575e-03,  2.8534e-03, -2.7466e-03,  ...,  5.0354e-03,\n",
       "                        2.3193e-03, -2.2125e-03],\n",
       "                      ...,\n",
       "                      [ 1.0147e-03, -4.5300e-05, -3.1128e-03,  ..., -1.2283e-03,\n",
       "                        2.1515e-03,  3.7689e-03],\n",
       "                      [ 5.3787e-04, -1.9836e-03, -2.6855e-03,  ..., -1.6632e-03,\n",
       "                        8.8501e-04, -3.5400e-03],\n",
       "                      [ 2.8992e-03,  1.8954e-05,  2.1057e-03,  ..., -6.1646e-03,\n",
       "                        2.7161e-03, -6.2561e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0057,  0.0036,  0.0017,  ...,  0.0019,  0.0012,  0.0061],\n",
       "                      [ 0.0014, -0.0035,  0.0025,  ..., -0.0025,  0.0060, -0.0060],\n",
       "                      [ 0.0051, -0.0029, -0.0005,  ...,  0.0023,  0.0002, -0.0015],\n",
       "                      ...,\n",
       "                      [-0.0005, -0.0047, -0.0009,  ..., -0.0088, -0.0056, -0.0022],\n",
       "                      [-0.0005, -0.0010,  0.0008,  ...,  0.0014, -0.0070,  0.0042],\n",
       "                      [ 0.0053, -0.0022,  0.0022,  ..., -0.0030, -0.0025, -0.0002]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.up_proj.weight',\n",
       "              tensor([[ 3.3264e-03, -4.4861e-03, -1.6251e-03,  ...,  3.3569e-03,\n",
       "                       -8.4877e-05,  6.0654e-04],\n",
       "                      [-1.4343e-03,  9.0408e-04,  2.1362e-03,  ..., -2.6093e-03,\n",
       "                        5.1880e-04, -3.2501e-03],\n",
       "                      [ 9.7656e-04,  3.6163e-03, -3.0212e-03,  ...,  1.6212e-04,\n",
       "                        1.1520e-03, -1.6098e-03],\n",
       "                      ...,\n",
       "                      [ 5.4932e-03,  1.2131e-03, -1.3828e-04,  ..., -2.2125e-03,\n",
       "                        4.6349e-04,  3.1281e-03],\n",
       "                      [ 2.0294e-03, -3.0899e-04,  3.5400e-03,  ..., -1.4114e-03,\n",
       "                       -2.1362e-03,  1.8921e-03],\n",
       "                      [ 1.2360e-03, -6.3171e-03, -3.7956e-04,  ...,  2.6398e-03,\n",
       "                        1.0300e-03,  1.3428e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.down_proj.weight',\n",
       "              tensor([[ 2.1667e-03, -5.7678e-03,  1.0757e-03,  ...,  2.8687e-03,\n",
       "                       -6.8283e-04,  8.0109e-04],\n",
       "                      [-5.8289e-03,  5.2185e-03,  3.5858e-03,  ..., -7.2021e-03,\n",
       "                        3.8300e-03, -1.3199e-03],\n",
       "                      [-7.1411e-03,  7.4387e-05, -3.0060e-03,  ...,  3.8528e-04,\n",
       "                        6.1417e-04,  2.2125e-03],\n",
       "                      ...,\n",
       "                      [-1.3046e-03,  2.5940e-03,  1.9989e-03,  ...,  2.0752e-03,\n",
       "                        1.5488e-03, -3.3569e-03],\n",
       "                      [-2.4719e-03,  6.8054e-03,  8.3923e-04,  ...,  1.9684e-03,\n",
       "                       -1.5335e-03, -2.0905e-03],\n",
       "                      [ 1.7319e-03,  2.4414e-03, -4.3640e-03,  ...,  1.1978e-03,\n",
       "                       -2.8729e-05,  1.1520e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.24.input_layernorm.weight',\n",
       "              tensor([2.8125, 2.8438, 2.7812,  ..., 2.5625, 2.6406, 2.6562],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.post_attention_layernorm.weight',\n",
       "              tensor([3.4688, 3.8438, 3.6250,  ..., 3.3594, 3.4219, 3.3750],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0009, -0.0038, -0.0058,  ..., -0.0017,  0.0042,  0.0015],\n",
       "                      [ 0.0010,  0.0035, -0.0004,  ...,  0.0011,  0.0024, -0.0009],\n",
       "                      [ 0.0027, -0.0029, -0.0007,  ..., -0.0043, -0.0005,  0.0011],\n",
       "                      ...,\n",
       "                      [-0.0017,  0.0022,  0.0021,  ...,  0.0091, -0.0012,  0.0063],\n",
       "                      [ 0.0151, -0.0026,  0.0121,  ..., -0.0058,  0.0021, -0.0033],\n",
       "                      [ 0.0124, -0.0051,  0.0052,  ...,  0.0067, -0.0092,  0.0081]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.k_proj.weight',\n",
       "              tensor([[-3.8910e-03, -3.9978e-03, -4.6082e-03,  ..., -5.0735e-04,\n",
       "                       -3.7432e-05, -1.7471e-03],\n",
       "                      [-5.7068e-03,  3.3760e-04,  4.3945e-03,  ...,  2.6855e-03,\n",
       "                        8.4686e-04,  1.1139e-03],\n",
       "                      [ 1.1826e-03, -2.7466e-03, -4.4823e-04,  ...,  2.3041e-03,\n",
       "                       -1.7471e-03,  2.1553e-04],\n",
       "                      ...,\n",
       "                      [ 5.9509e-03, -1.5564e-03,  4.0283e-03,  ..., -3.4943e-03,\n",
       "                       -4.0894e-03,  2.5940e-03],\n",
       "                      [-5.4016e-03,  1.9531e-03,  4.0054e-04,  ..., -4.7302e-03,\n",
       "                        3.8300e-03, -5.7068e-03],\n",
       "                      [-3.7689e-03, -4.6997e-03,  1.1292e-03,  ..., -1.5411e-03,\n",
       "                       -1.1215e-03,  2.5635e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0042, -0.0034,  0.0014,  ...,  0.0009,  0.0003, -0.0052],\n",
       "                      [-0.0031,  0.0019,  0.0008,  ..., -0.0034, -0.0013, -0.0072],\n",
       "                      [ 0.0067,  0.0025, -0.0065,  ...,  0.0026, -0.0029,  0.0008],\n",
       "                      ...,\n",
       "                      [-0.0009,  0.0006,  0.0022,  ...,  0.0027,  0.0045,  0.0004],\n",
       "                      [ 0.0020,  0.0031, -0.0035,  ..., -0.0026,  0.0011, -0.0028],\n",
       "                      [-0.0009,  0.0015, -0.0034,  ...,  0.0017,  0.0025,  0.0023]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.3657e-03, -3.0212e-03, -3.7384e-03,  ..., -3.4790e-03,\n",
       "                        8.4229e-03, -5.7678e-03],\n",
       "                      [-2.2278e-03,  1.8845e-03, -3.9978e-03,  ..., -4.6997e-03,\n",
       "                        2.3651e-03,  7.2098e-04],\n",
       "                      [-3.0060e-03,  1.4420e-03,  2.1458e-06,  ...,  2.9144e-03,\n",
       "                        2.1667e-03,  4.1809e-03],\n",
       "                      ...,\n",
       "                      [ 1.7700e-03,  3.8452e-03, -1.4496e-03,  ...,  3.7537e-03,\n",
       "                       -6.7234e-05, -3.6926e-03],\n",
       "                      [-2.2316e-04, -1.9989e-03,  4.0588e-03,  ..., -2.6093e-03,\n",
       "                       -2.0027e-05,  9.1553e-03],\n",
       "                      [-2.5330e-03,  8.6784e-05,  2.2430e-03,  ..., -2.0599e-03,\n",
       "                       -1.0529e-03,  2.9449e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0045,  0.0029, -0.0023,  ...,  0.0018,  0.0011,  0.0050],\n",
       "                      [ 0.0004,  0.0027,  0.0042,  ...,  0.0024,  0.0014, -0.0066],\n",
       "                      [ 0.0012, -0.0002, -0.0056,  ...,  0.0020,  0.0014, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0034,  0.0005,  0.0059,  ...,  0.0053, -0.0035,  0.0014],\n",
       "                      [ 0.0010,  0.0019,  0.0026,  ..., -0.0003,  0.0032,  0.0069],\n",
       "                      [-0.0019,  0.0085, -0.0009,  ..., -0.0056,  0.0015,  0.0055]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.up_proj.weight',\n",
       "              tensor([[ 4.4861e-03, -2.2411e-04, -1.1749e-03,  ...,  3.3722e-03,\n",
       "                       -2.6703e-03,  2.6245e-03],\n",
       "                      [ 2.6093e-03,  1.3113e-05,  5.3406e-04,  ...,  2.1362e-03,\n",
       "                       -1.1749e-03, -1.4191e-03],\n",
       "                      [-3.7842e-03,  5.5542e-03,  2.2316e-04,  ...,  2.6855e-03,\n",
       "                        1.7242e-03, -3.5706e-03],\n",
       "                      ...,\n",
       "                      [-2.4719e-03,  1.0376e-03,  1.9989e-03,  ..., -3.0823e-03,\n",
       "                        5.2490e-03,  1.7700e-03],\n",
       "                      [ 4.5471e-03, -6.4850e-05, -2.2430e-03,  ...,  4.3945e-03,\n",
       "                       -1.7242e-03, -7.3624e-04],\n",
       "                      [ 4.1199e-03,  5.9204e-03, -9.6893e-04,  ...,  4.9133e-03,\n",
       "                        2.4414e-03,  2.6703e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0013, -0.0016, -0.0012,  ..., -0.0006,  0.0003, -0.0052],\n",
       "                      [-0.0010, -0.0013,  0.0032,  ...,  0.0014,  0.0018,  0.0055],\n",
       "                      [-0.0030,  0.0039, -0.0007,  ..., -0.0030,  0.0011,  0.0027],\n",
       "                      ...,\n",
       "                      [ 0.0026,  0.0002, -0.0011,  ..., -0.0011, -0.0009,  0.0033],\n",
       "                      [-0.0009,  0.0023, -0.0021,  ..., -0.0012,  0.0027,  0.0013],\n",
       "                      [-0.0033, -0.0007, -0.0002,  ...,  0.0008, -0.0053,  0.0007]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.input_layernorm.weight',\n",
       "              tensor([2.7500, 2.7969, 2.8906,  ..., 2.6250, 2.7344, 2.7031],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.post_attention_layernorm.weight',\n",
       "              tensor([3.5625, 3.9688, 3.7656,  ..., 3.4219, 3.6094, 3.5469],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.q_proj.weight',\n",
       "              tensor([[ 4.3640e-03, -5.2490e-03,  1.2360e-03,  ...,  9.4223e-04,\n",
       "                       -2.2430e-03, -4.0588e-03],\n",
       "                      [ 1.8845e-03,  1.6785e-03,  4.2114e-03,  ..., -5.7068e-03,\n",
       "                        4.4632e-04,  3.7384e-03],\n",
       "                      [ 1.9073e-03,  1.3809e-03, -3.1891e-03,  ..., -2.1362e-03,\n",
       "                       -7.7209e-03, -9.1553e-04],\n",
       "                      ...,\n",
       "                      [ 4.4250e-03, -4.4250e-04,  3.5095e-04,  ..., -4.7607e-03,\n",
       "                        9.1934e-04, -4.9744e-03],\n",
       "                      [ 9.5749e-04, -8.3008e-03,  2.8534e-03,  ...,  3.0212e-03,\n",
       "                       -7.3624e-04, -2.3041e-03],\n",
       "                      [ 1.6708e-03,  8.5449e-04, -6.9141e-06,  ...,  3.4332e-03,\n",
       "                        6.8970e-03,  2.1210e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.k_proj.weight',\n",
       "              tensor([[-1.3123e-03, -4.3030e-03,  2.8229e-03,  ..., -7.8964e-04,\n",
       "                       -2.4567e-03,  2.2221e-04],\n",
       "                      [ 6.6757e-05,  7.6771e-05, -8.7891e-03,  ..., -2.3956e-03,\n",
       "                       -1.8845e-03,  2.7771e-03],\n",
       "                      [-1.2894e-03,  1.8387e-03, -5.4359e-05,  ...,  2.6245e-03,\n",
       "                        1.8387e-03, -3.3264e-03],\n",
       "                      ...,\n",
       "                      [ 4.5204e-04, -3.9062e-03,  2.3193e-03,  ..., -2.3499e-03,\n",
       "                        1.1978e-03,  3.7231e-03],\n",
       "                      [ 1.4877e-03,  7.2098e-04, -2.3041e-03,  ...,  1.2207e-03,\n",
       "                       -1.2741e-03, -3.8605e-03],\n",
       "                      [-8.1787e-03, -4.0054e-04,  7.8125e-03,  ..., -5.5237e-03,\n",
       "                        4.1504e-03, -6.9046e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0044,  0.0007,  0.0021,  ...,  0.0022, -0.0027,  0.0081],\n",
       "                      [ 0.0001, -0.0043, -0.0050,  ..., -0.0022, -0.0007, -0.0050],\n",
       "                      [-0.0088, -0.0049, -0.0025,  ...,  0.0033,  0.0024,  0.0006],\n",
       "                      ...,\n",
       "                      [ 0.0016, -0.0022,  0.0063,  ...,  0.0014, -0.0064,  0.0073],\n",
       "                      [-0.0009,  0.0030,  0.0054,  ..., -0.0007,  0.0042,  0.0023],\n",
       "                      [-0.0015, -0.0002, -0.0017,  ..., -0.0036,  0.0033, -0.0003]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0016,  0.0009, -0.0008,  ...,  0.0030, -0.0026, -0.0018],\n",
       "                      [-0.0008,  0.0050, -0.0014,  ..., -0.0044,  0.0003,  0.0018],\n",
       "                      [ 0.0046,  0.0028,  0.0003,  ..., -0.0002, -0.0023, -0.0017],\n",
       "                      ...,\n",
       "                      [-0.0023, -0.0106,  0.0027,  ...,  0.0012, -0.0037, -0.0017],\n",
       "                      [ 0.0006, -0.0025, -0.0019,  ..., -0.0015,  0.0040,  0.0021],\n",
       "                      [-0.0031, -0.0022, -0.0024,  ..., -0.0027,  0.0025,  0.0061]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0026,  0.0014, -0.0004,  ...,  0.0061,  0.0014, -0.0041],\n",
       "                      [-0.0020, -0.0055,  0.0014,  ..., -0.0041,  0.0024,  0.0009],\n",
       "                      [ 0.0080, -0.0041,  0.0002,  ..., -0.0020, -0.0048, -0.0028],\n",
       "                      ...,\n",
       "                      [ 0.0019, -0.0020,  0.0018,  ..., -0.0006,  0.0033, -0.0034],\n",
       "                      [-0.0027,  0.0021, -0.0021,  ...,  0.0023,  0.0049,  0.0016],\n",
       "                      [ 0.0007,  0.0040, -0.0011,  ...,  0.0056,  0.0022, -0.0049]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.up_proj.weight',\n",
       "              tensor([[ 1.3657e-03, -1.2360e-03, -1.7471e-03,  ...,  8.3542e-04,\n",
       "                       -1.2283e-03, -2.3346e-03],\n",
       "                      [-4.7302e-03,  9.2506e-05,  2.8687e-03,  ...,  1.9836e-04,\n",
       "                       -2.6093e-03, -3.6469e-03],\n",
       "                      [ 1.0300e-04, -1.9836e-03,  3.7231e-03,  ...,  3.0670e-03,\n",
       "                       -1.3885e-03,  4.2343e-04],\n",
       "                      ...,\n",
       "                      [ 9.6130e-04,  7.5912e-04, -3.3875e-03,  ..., -2.5482e-03,\n",
       "                        2.9182e-04,  1.7548e-03],\n",
       "                      [ 8.4877e-05,  2.2430e-03, -3.9062e-03,  ..., -5.5313e-04,\n",
       "                        7.3242e-03,  3.3417e-03],\n",
       "                      [-4.1199e-03, -2.3956e-03, -3.2654e-03,  ...,  1.0376e-03,\n",
       "                        1.0452e-03, -1.9684e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.down_proj.weight',\n",
       "              tensor([[ 1.3962e-03,  1.5488e-03, -2.2278e-03,  ...,  1.2207e-03,\n",
       "                        2.8534e-03, -6.6757e-04],\n",
       "                      [ 1.0147e-03, -7.0496e-03, -1.8387e-03,  ...,  1.1520e-03,\n",
       "                        3.1471e-04, -5.2185e-03],\n",
       "                      [ 6.6757e-05,  2.3460e-04, -1.4954e-03,  ..., -7.6675e-04,\n",
       "                       -1.1063e-04, -1.5106e-03],\n",
       "                      ...,\n",
       "                      [ 5.7220e-04, -2.5635e-03, -5.5237e-03,  ...,  9.0408e-04,\n",
       "                       -1.2360e-03,  2.7466e-04],\n",
       "                      [-5.4169e-04,  6.1417e-04, -1.8463e-03,  ...,  2.8229e-03,\n",
       "                        3.2806e-03,  2.7618e-03],\n",
       "                      [ 3.3875e-03, -3.6812e-04,  5.8594e-03,  ...,  2.0905e-03,\n",
       "                        8.3542e-04,  2.1820e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.26.input_layernorm.weight',\n",
       "              tensor([2.6094, 2.5938, 2.5625,  ..., 2.5469, 2.4688, 2.5469],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.post_attention_layernorm.weight',\n",
       "              tensor([3.7031, 3.9531, 3.7812,  ..., 3.5156, 3.6094, 3.5938],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.q_proj.weight',\n",
       "              tensor([[ 3.3112e-03,  5.0545e-05,  2.6093e-03,  ..., -2.9297e-03,\n",
       "                        5.3711e-03,  6.1417e-04],\n",
       "                      [-1.5564e-03, -4.5395e-04, -4.3678e-04,  ...,  2.7847e-04,\n",
       "                       -2.1362e-03, -6.1646e-03],\n",
       "                      [ 9.8419e-04, -4.2114e-03, -2.6855e-03,  ...,  2.4719e-03,\n",
       "                       -2.3956e-03, -1.9455e-03],\n",
       "                      ...,\n",
       "                      [ 5.1575e-03,  4.2915e-04,  3.9577e-05,  ..., -1.8997e-03,\n",
       "                        5.5847e-03,  1.6212e-04],\n",
       "                      [-2.3499e-03,  1.5717e-03,  1.4114e-03,  ..., -1.3275e-03,\n",
       "                        3.4637e-03, -8.0872e-04],\n",
       "                      [ 2.5787e-03,  1.7471e-03, -2.0294e-03,  ...,  6.4697e-03,\n",
       "                        9.7656e-04, -7.8678e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.k_proj.weight',\n",
       "              tensor([[-2.1839e-04,  1.9836e-03, -2.8992e-03,  ...,  3.2425e-04,\n",
       "                       -4.0627e-04, -5.0354e-04],\n",
       "                      [ 6.0425e-03,  1.8234e-03, -1.0529e-03,  ..., -6.5918e-03,\n",
       "                        1.6174e-03, -7.6294e-04],\n",
       "                      [ 5.4932e-03,  5.6267e-05, -1.9836e-03,  ..., -2.8687e-03,\n",
       "                        1.9989e-03, -4.7607e-03],\n",
       "                      ...,\n",
       "                      [-8.1253e-04, -2.5749e-04,  1.7014e-03,  ..., -5.6076e-04,\n",
       "                        1.5831e-04, -2.0447e-03],\n",
       "                      [ 1.0824e-04, -1.0910e-03,  1.4725e-03,  ...,  3.0518e-04,\n",
       "                        2.9297e-03,  2.7313e-03],\n",
       "                      [ 4.2114e-03, -2.5024e-03, -1.9226e-03,  ...,  8.7357e-04,\n",
       "                       -2.5787e-03,  5.4016e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.v_proj.weight',\n",
       "              tensor([[-1.6937e-03,  5.7983e-03, -1.8311e-03,  ...,  3.4904e-04,\n",
       "                       -3.3722e-03,  1.4191e-03],\n",
       "                      [ 2.3804e-03,  3.3417e-03, -5.0049e-03,  ..., -1.8997e-03,\n",
       "                       -6.2180e-04, -2.0905e-03],\n",
       "                      [-5.3787e-04,  1.2279e-05,  2.4567e-03,  ...,  5.1270e-03,\n",
       "                        2.0294e-03, -9.7656e-04],\n",
       "                      ...,\n",
       "                      [ 1.0529e-03, -1.5564e-03, -4.1504e-03,  ...,  1.4801e-03,\n",
       "                       -6.8970e-03,  3.8910e-03],\n",
       "                      [ 6.5308e-03, -4.0588e-03,  1.3428e-03,  ...,  3.6926e-03,\n",
       "                        8.2016e-04, -3.3264e-03],\n",
       "                      [-4.4556e-03, -5.1270e-03,  2.2583e-03,  ..., -3.6469e-03,\n",
       "                       -9.2316e-04,  1.8234e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.6251e-03, -4.2114e-03, -2.5635e-03,  ...,  1.3275e-03,\n",
       "                       -3.3264e-03,  1.1520e-03],\n",
       "                      [-1.1139e-03, -4.7607e-03,  1.8234e-03,  ...,  4.7302e-03,\n",
       "                        3.4094e-05,  5.0049e-03],\n",
       "                      [ 5.0354e-03, -2.1973e-03, -9.2030e-05,  ..., -1.7242e-03,\n",
       "                       -3.0518e-03, -1.4725e-03],\n",
       "                      ...,\n",
       "                      [-2.5330e-03,  2.3041e-03, -3.8528e-04,  ...,  4.6387e-03,\n",
       "                        4.6387e-03,  2.7313e-03],\n",
       "                      [ 3.7994e-03,  5.6028e-05, -6.5565e-07,  ...,  2.6398e-03,\n",
       "                       -4.9438e-03, -2.0447e-03],\n",
       "                      [ 4.1008e-04,  9.6130e-04, -4.1008e-04,  ..., -1.7242e-03,\n",
       "                        1.5564e-03, -4.0588e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.gate_proj.weight',\n",
       "              tensor([[-4.1809e-03,  1.2360e-03,  4.6997e-03,  ..., -3.4637e-03,\n",
       "                       -1.5488e-03,  7.7515e-03],\n",
       "                      [ 6.1646e-03,  7.5989e-03,  1.2589e-03,  ..., -9.0790e-04,\n",
       "                        4.0436e-04,  2.4872e-03],\n",
       "                      [-1.7776e-03, -2.0294e-03, -3.0365e-03,  ..., -2.3193e-03,\n",
       "                        1.1139e-03,  4.1199e-03],\n",
       "                      ...,\n",
       "                      [-1.7624e-03,  4.3869e-05,  2.9755e-03,  ...,  2.1973e-03,\n",
       "                       -1.8463e-03,  3.8910e-04],\n",
       "                      [ 2.5330e-03, -7.7820e-04, -5.4836e-06,  ..., -3.9482e-04,\n",
       "                       -2.6245e-03,  1.8921e-03],\n",
       "                      [-2.1515e-03, -4.4556e-03, -6.2866e-03,  ...,  4.5776e-03,\n",
       "                       -5.8899e-03,  2.1210e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.up_proj.weight',\n",
       "              tensor([[-3.7842e-03,  5.1498e-04,  9.0027e-04,  ...,  1.0061e-04,\n",
       "                        1.8768e-03,  2.8381e-03],\n",
       "                      [ 4.6387e-03,  4.3945e-03, -4.7913e-03,  ..., -3.0823e-03,\n",
       "                       -1.3123e-03, -2.0981e-04],\n",
       "                      [ 4.7302e-03,  9.3460e-05,  3.6774e-03,  ...,  4.1771e-04,\n",
       "                        1.6174e-03, -5.6076e-04],\n",
       "                      ...,\n",
       "                      [-1.4877e-03, -6.8970e-03, -4.2152e-04,  ...,  5.9204e-03,\n",
       "                       -1.3885e-03, -1.8539e-03],\n",
       "                      [ 6.4697e-03, -5.8289e-03, -1.3199e-03,  ..., -1.7395e-03,\n",
       "                       -3.5858e-03,  2.3346e-03],\n",
       "                      [ 6.5918e-03,  6.4392e-03,  3.0670e-03,  ...,  1.0605e-03,\n",
       "                        2.8992e-03, -3.1281e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0004, -0.0023,  0.0033,  ...,  0.0008, -0.0056, -0.0029],\n",
       "                      [-0.0022,  0.0041,  0.0016,  ...,  0.0018,  0.0011, -0.0048],\n",
       "                      [ 0.0030, -0.0029,  0.0011,  ...,  0.0053, -0.0022, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0081, -0.0002,  0.0007,  ...,  0.0055, -0.0027,  0.0014],\n",
       "                      [-0.0048,  0.0015, -0.0039,  ...,  0.0006,  0.0010,  0.0042],\n",
       "                      [ 0.0045, -0.0013,  0.0022,  ...,  0.0040, -0.0042, -0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.input_layernorm.weight',\n",
       "              tensor([2.6875, 2.8125, 2.7031,  ..., 2.5781, 2.5938, 2.6406],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.post_attention_layernorm.weight',\n",
       "              tensor([3.7500, 4.0000, 3.8125,  ..., 3.6406, 3.6562, 3.7031],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.8997e-03,  1.3123e-03, -1.7242e-03,  ..., -5.5847e-03,\n",
       "                        5.7602e-04, -1.1063e-03],\n",
       "                      [-1.7319e-03,  8.8120e-04, -1.8005e-03,  ...,  2.8687e-03,\n",
       "                        4.1809e-03,  8.5354e-05],\n",
       "                      [ 1.8406e-04,  5.6458e-04, -3.1738e-03,  ..., -1.4725e-03,\n",
       "                       -1.5793e-03,  2.8076e-03],\n",
       "                      ...,\n",
       "                      [-5.8594e-03, -2.9297e-03,  3.8528e-04,  ...,  1.5259e-03,\n",
       "                        7.2861e-04, -2.8839e-03],\n",
       "                      [ 1.7395e-03,  3.5858e-03,  7.9956e-03,  ..., -8.0872e-04,\n",
       "                        2.0905e-03,  3.1281e-04],\n",
       "                      [-6.2561e-04, -3.1128e-03,  1.4267e-03,  ...,  4.3335e-03,\n",
       "                        5.0659e-03,  2.0695e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.k_proj.weight',\n",
       "              tensor([[-2.4719e-03, -6.6757e-05, -4.8637e-04,  ..., -8.3160e-04,\n",
       "                        6.2561e-03, -1.2665e-03],\n",
       "                      [-3.5400e-03, -4.2419e-03, -3.1853e-04,  ..., -1.6332e-05,\n",
       "                        9.5749e-04,  8.8882e-04],\n",
       "                      [-3.1891e-03,  9.4223e-04, -2.5024e-03,  ...,  4.0817e-04,\n",
       "                        5.4932e-04, -9.7275e-04],\n",
       "                      ...,\n",
       "                      [-5.3406e-03, -2.1973e-03,  2.8534e-03,  ...,  7.2098e-04,\n",
       "                       -6.7139e-03,  2.1973e-03],\n",
       "                      [ 2.0294e-03, -2.4261e-03, -1.4725e-03,  ..., -3.9368e-03,\n",
       "                       -4.6082e-03, -3.2501e-03],\n",
       "                      [ 2.5787e-03,  6.2256e-03,  2.1515e-03,  ..., -6.8359e-03,\n",
       "                       -8.7738e-04, -5.3787e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.v_proj.weight',\n",
       "              tensor([[ 5.3101e-03, -1.4954e-03,  7.1716e-04,  ..., -4.6387e-03,\n",
       "                       -2.6512e-04, -4.5166e-03],\n",
       "                      [ 3.5248e-03, -1.6174e-03, -3.2806e-03,  ...,  5.3711e-03,\n",
       "                       -1.1902e-03,  5.1270e-03],\n",
       "                      [-5.8899e-03,  2.5024e-03, -2.1458e-04,  ...,  8.8811e-06,\n",
       "                        8.6212e-04,  4.1809e-03],\n",
       "                      ...,\n",
       "                      [ 7.2479e-04, -4.2152e-04,  2.0313e-04,  ..., -2.5024e-03,\n",
       "                       -3.3417e-03,  1.6174e-03],\n",
       "                      [ 3.3112e-03, -3.9482e-04, -3.2654e-03,  ...,  7.2861e-04,\n",
       "                       -5.8594e-03,  2.8992e-03],\n",
       "                      [ 7.6294e-05,  1.3046e-03, -1.7090e-03,  ...,  3.2654e-03,\n",
       "                        1.6327e-03, -5.4016e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.o_proj.weight',\n",
       "              tensor([[-1.7929e-03,  4.1199e-03,  2.6245e-03,  ...,  1.3199e-03,\n",
       "                       -4.8447e-04, -1.2665e-03],\n",
       "                      [-2.6245e-03,  8.6594e-04, -2.8687e-03,  ...,  2.4261e-03,\n",
       "                        1.6327e-03,  5.3406e-03],\n",
       "                      [ 4.6997e-03,  1.0147e-03,  2.1210e-03,  ...,  7.8583e-04,\n",
       "                        7.5912e-04, -2.3193e-03],\n",
       "                      ...,\n",
       "                      [-1.3123e-03, -1.7319e-03,  4.4556e-03,  ..., -1.5106e-03,\n",
       "                       -2.3346e-03,  4.4861e-03],\n",
       "                      [-1.4572e-03,  3.4332e-03,  3.0518e-03,  ..., -4.2419e-03,\n",
       "                       -2.2888e-03, -9.7656e-04],\n",
       "                      [-1.5945e-03,  1.2436e-03,  2.5177e-03,  ..., -3.0060e-03,\n",
       "                       -8.3160e-04, -3.6955e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.28.mlp.gate_proj.weight',\n",
       "              tensor([[ 5.3406e-03,  9.0790e-04,  1.4191e-03,  ..., -1.1902e-03,\n",
       "                        5.0659e-03, -1.3275e-03],\n",
       "                      [ 1.6937e-03, -8.9645e-05, -2.5177e-03,  ..., -2.7161e-03,\n",
       "                       -4.3945e-03, -1.4648e-03],\n",
       "                      [ 6.6528e-03, -2.6464e-05,  1.4420e-03,  ..., -5.8289e-03,\n",
       "                        2.2602e-04, -4.5776e-04],\n",
       "                      ...,\n",
       "                      [-2.9449e-03,  5.7983e-04, -3.5248e-03,  ..., -2.2278e-03,\n",
       "                       -2.9449e-03, -3.9291e-04],\n",
       "                      [ 5.1270e-03, -2.6703e-03, -2.3193e-03,  ..., -1.9455e-03,\n",
       "                       -1.8501e-04,  4.7302e-03],\n",
       "                      [ 1.7548e-03,  2.5024e-03, -4.1504e-03,  ...,  2.8133e-05,\n",
       "                        5.4932e-03, -2.3651e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.28.mlp.up_proj.weight',\n",
       "              tensor([[-0.0025,  0.0024,  0.0023,  ...,  0.0065, -0.0032,  0.0091],\n",
       "                      [ 0.0035,  0.0033,  0.0008,  ..., -0.0015,  0.0024, -0.0025],\n",
       "                      [ 0.0008, -0.0025,  0.0032,  ...,  0.0019, -0.0022,  0.0042],\n",
       "                      ...,\n",
       "                      [-0.0003,  0.0034, -0.0005,  ...,  0.0002,  0.0032,  0.0007],\n",
       "                      [ 0.0034, -0.0009, -0.0039,  ...,  0.0018, -0.0002, -0.0006],\n",
       "                      [-0.0016,  0.0043,  0.0009,  ...,  0.0011, -0.0078, -0.0021]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.mlp.down_proj.weight',\n",
       "              tensor([[ 2.9907e-03,  8.0109e-04, -4.1199e-03,  ...,  4.7302e-03,\n",
       "                       -1.9150e-03,  7.4387e-04],\n",
       "                      [ 3.4332e-03,  3.5400e-03, -1.1063e-03,  ...,  1.1215e-03,\n",
       "                        3.1586e-03,  3.7003e-04],\n",
       "                      [-3.0975e-03, -8.1787e-03,  3.3569e-03,  ...,  6.2561e-03,\n",
       "                       -1.4420e-03,  1.8311e-03],\n",
       "                      ...,\n",
       "                      [ 1.6632e-03,  5.2185e-03,  2.8381e-03,  ..., -2.3804e-03,\n",
       "                        2.4261e-03, -1.5106e-03],\n",
       "                      [ 1.1504e-05, -6.3782e-03,  2.6550e-03,  ..., -2.2888e-03,\n",
       "                        1.2589e-03,  1.0834e-03],\n",
       "                      [-2.4261e-03,  4.0588e-03,  4.7607e-03,  ...,  7.8583e-04,\n",
       "                        1.9455e-03,  2.3499e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.28.input_layernorm.weight',\n",
       "              tensor([2.5469, 2.6562, 2.6562,  ..., 2.4219, 2.4844, 2.4531],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.post_attention_layernorm.weight',\n",
       "              tensor([3.7500, 4.0312, 3.9062,  ..., 3.7500, 3.7812, 3.7500],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.3351e-03,  3.2654e-03,  2.3499e-03,  ..., -3.4332e-03,\n",
       "                       -3.2043e-03, -5.3406e-03],\n",
       "                      [-1.2665e-03,  1.7319e-03, -1.3428e-03,  ..., -9.9182e-04,\n",
       "                        3.7842e-03, -3.6430e-04],\n",
       "                      [-1.1215e-03, -6.6757e-04,  2.8992e-03,  ..., -4.2419e-03,\n",
       "                       -6.2180e-04, -1.6937e-03],\n",
       "                      ...,\n",
       "                      [ 8.3008e-03,  1.4648e-03, -6.4087e-04,  ..., -7.8735e-03,\n",
       "                       -5.2691e-05,  1.5182e-03],\n",
       "                      [-1.8158e-03,  4.0894e-03, -6.6833e-03,  ..., -5.4016e-03,\n",
       "                       -1.3809e-03, -1.0681e-03],\n",
       "                      [ 5.8365e-04, -3.1433e-03,  2.0599e-03,  ...,  5.2185e-03,\n",
       "                       -3.7384e-04, -5.8365e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.k_proj.weight',\n",
       "              tensor([[-9.1553e-04, -4.4250e-04, -7.8964e-04,  ..., -3.5095e-04,\n",
       "                       -1.1215e-03,  4.2419e-03],\n",
       "                      [-1.4572e-03,  2.6093e-03,  9.9659e-05,  ..., -3.0518e-03,\n",
       "                       -5.0735e-04,  1.1902e-03],\n",
       "                      [-1.0910e-03,  2.6512e-04, -4.4584e-05,  ...,  1.4877e-03,\n",
       "                        1.2100e-05, -1.8616e-03],\n",
       "                      ...,\n",
       "                      [ 3.4027e-03, -2.0981e-04,  2.3193e-03,  ...,  3.6926e-03,\n",
       "                       -4.0283e-03, -4.5204e-04],\n",
       "                      [-9.5825e-03,  7.2479e-04,  1.0071e-02,  ..., -2.2125e-03,\n",
       "                       -4.5166e-03, -2.5940e-03],\n",
       "                      [ 5.5847e-03, -2.8076e-03, -1.2665e-03,  ..., -7.7515e-03,\n",
       "                        6.2561e-04, -1.3733e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0064,  0.0006, -0.0015,  ..., -0.0015,  0.0007, -0.0043],\n",
       "                      [-0.0124, -0.0003, -0.0040,  ...,  0.0118,  0.0056, -0.0055],\n",
       "                      [-0.0055,  0.0079, -0.0039,  ..., -0.0029, -0.0032,  0.0024],\n",
       "                      ...,\n",
       "                      [-0.0017,  0.0011,  0.0034,  ..., -0.0039, -0.0040, -0.0014],\n",
       "                      [-0.0028, -0.0052, -0.0036,  ...,  0.0026,  0.0003,  0.0030],\n",
       "                      [-0.0016, -0.0006,  0.0030,  ...,  0.0055,  0.0037,  0.0008]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.o_proj.weight',\n",
       "              tensor([[ 2.7313e-03,  5.9509e-03,  3.0823e-03,  ...,  1.9531e-03,\n",
       "                       -7.1716e-04,  1.3809e-03],\n",
       "                      [-6.8283e-04,  1.5640e-03, -3.0670e-03,  ..., -3.3264e-03,\n",
       "                       -9.3079e-04,  1.9073e-03],\n",
       "                      [-1.8311e-03, -1.0452e-03, -2.4319e-04,  ...,  3.0823e-03,\n",
       "                       -3.0899e-04, -8.3160e-04],\n",
       "                      ...,\n",
       "                      [ 2.7618e-03, -3.9368e-03,  1.4954e-03,  ...,  1.3962e-03,\n",
       "                        6.4697e-03,  2.6398e-03],\n",
       "                      [ 1.5259e-03, -3.8452e-03,  2.2888e-03,  ..., -1.8539e-03,\n",
       "                       -3.0670e-03,  6.9809e-04],\n",
       "                      [ 1.1978e-03,  1.5411e-03,  2.5024e-03,  ...,  3.3188e-04,\n",
       "                        9.8419e-04, -5.0783e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.29.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0068,  0.0005, -0.0081,  ..., -0.0024,  0.0003,  0.0041],\n",
       "                      [-0.0008, -0.0014,  0.0048,  ...,  0.0056,  0.0015,  0.0036],\n",
       "                      [ 0.0101,  0.0017, -0.0019,  ..., -0.0063, -0.0007,  0.0002],\n",
       "                      ...,\n",
       "                      [-0.0041, -0.0007,  0.0020,  ...,  0.0011, -0.0024, -0.0005],\n",
       "                      [ 0.0002,  0.0023,  0.0035,  ..., -0.0045, -0.0021,  0.0016],\n",
       "                      [ 0.0021, -0.0005, -0.0045,  ...,  0.0053,  0.0005,  0.0007]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0020,  0.0020, -0.0047,  ..., -0.0013, -0.0015,  0.0023],\n",
       "                      [ 0.0089, -0.0051,  0.0010,  ...,  0.0021,  0.0035,  0.0028],\n",
       "                      [-0.0042, -0.0048,  0.0011,  ...,  0.0008,  0.0006, -0.0018],\n",
       "                      ...,\n",
       "                      [-0.0004, -0.0032, -0.0052,  ..., -0.0022,  0.0020,  0.0038],\n",
       "                      [ 0.0049,  0.0065, -0.0035,  ..., -0.0037, -0.0030,  0.0012],\n",
       "                      [ 0.0013, -0.0013, -0.0017,  ..., -0.0030,  0.0007,  0.0022]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.mlp.down_proj.weight',\n",
       "              tensor([[-1.1673e-03, -1.3046e-03,  1.0834e-03,  ...,  2.1820e-03,\n",
       "                       -5.3101e-03,  2.0752e-03],\n",
       "                      [-2.1515e-03, -5.9128e-04,  8.3618e-03,  ..., -7.8678e-05,\n",
       "                       -2.4261e-03, -1.0376e-03],\n",
       "                      [ 5.3787e-04,  5.3406e-03, -1.1139e-03,  ...,  2.7466e-03,\n",
       "                       -2.0142e-03, -2.3956e-03],\n",
       "                      ...,\n",
       "                      [ 3.1738e-03, -3.2959e-03, -7.1411e-03,  ...,  2.3346e-03,\n",
       "                        1.4210e-04, -2.4986e-04],\n",
       "                      [ 1.7395e-03,  6.3324e-04,  1.8158e-03,  ...,  5.5237e-03,\n",
       "                        4.6082e-03,  2.0142e-03],\n",
       "                      [-3.8910e-03, -2.8687e-03,  2.3746e-04,  ..., -1.5335e-03,\n",
       "                        3.1281e-03,  1.0757e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.29.input_layernorm.weight',\n",
       "              tensor([2.8906, 3.0781, 2.9375,  ..., 2.5938, 2.9062, 2.8438],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.post_attention_layernorm.weight',\n",
       "              tensor([3.8125, 3.9375, 3.8281,  ..., 3.7344, 3.7031, 3.6719],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0072,  0.0039, -0.0010,  ...,  0.0024, -0.0055, -0.0004],\n",
       "                      [ 0.0003,  0.0007, -0.0045,  ..., -0.0003,  0.0081,  0.0048],\n",
       "                      [-0.0022, -0.0004,  0.0046,  ..., -0.0010,  0.0058,  0.0003],\n",
       "                      ...,\n",
       "                      [-0.0027, -0.0021, -0.0015,  ..., -0.0059, -0.0028, -0.0043],\n",
       "                      [ 0.0010,  0.0048, -0.0030,  ..., -0.0002, -0.0053, -0.0017],\n",
       "                      [-0.0033,  0.0019,  0.0007,  ...,  0.0019,  0.0024,  0.0020]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0005,  0.0004,  0.0006,  ..., -0.0025,  0.0009, -0.0004],\n",
       "                      [-0.0003, -0.0006, -0.0010,  ...,  0.0007, -0.0006, -0.0016],\n",
       "                      [ 0.0011, -0.0016,  0.0014,  ..., -0.0013, -0.0007, -0.0001],\n",
       "                      ...,\n",
       "                      [-0.0016,  0.0004, -0.0010,  ...,  0.0063, -0.0005,  0.0010],\n",
       "                      [ 0.0034, -0.0007, -0.0014,  ...,  0.0006, -0.0020, -0.0035],\n",
       "                      [-0.0026, -0.0055,  0.0044,  ...,  0.0009,  0.0014,  0.0015]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0009, -0.0032,  0.0030,  ...,  0.0004, -0.0019,  0.0040],\n",
       "                      [-0.0074,  0.0026, -0.0001,  ...,  0.0034, -0.0039, -0.0017],\n",
       "                      [-0.0104, -0.0052,  0.0025,  ..., -0.0033, -0.0035, -0.0009],\n",
       "                      ...,\n",
       "                      [-0.0025,  0.0038, -0.0101,  ...,  0.0003, -0.0038,  0.0036],\n",
       "                      [ 0.0089,  0.0017, -0.0057,  ...,  0.0022,  0.0067, -0.0087],\n",
       "                      [-0.0025,  0.0010, -0.0030,  ..., -0.0021, -0.0006, -0.0042]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.o_proj.weight',\n",
       "              tensor([[-1.7548e-03, -1.3123e-03, -2.9907e-03,  ..., -7.6294e-04,\n",
       "                       -6.1035e-04,  5.5542e-03],\n",
       "                      [-3.5095e-03, -1.9836e-03, -2.1515e-03,  ...,  7.8201e-04,\n",
       "                       -6.8359e-03,  3.1662e-04],\n",
       "                      [ 7.8201e-04,  4.2725e-03,  5.7983e-03,  ..., -1.8692e-03,\n",
       "                       -4.9829e-05, -3.6163e-03],\n",
       "                      ...,\n",
       "                      [-2.4414e-03,  2.6321e-04,  4.8523e-03,  ..., -1.2398e-04,\n",
       "                        2.4109e-03,  2.3746e-04],\n",
       "                      [-1.2589e-03, -3.6316e-03,  3.7766e-04,  ..., -4.6082e-03,\n",
       "                        1.7242e-03, -1.0633e-04],\n",
       "                      [-1.7319e-03,  5.2643e-04,  6.3477e-03,  ...,  6.4850e-05,\n",
       "                       -7.4158e-03,  1.2360e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.30.mlp.gate_proj.weight',\n",
       "              tensor([[-2.6855e-03,  4.7493e-04, -2.7618e-03,  ..., -4.6997e-03,\n",
       "                        2.4109e-03, -6.4392e-03],\n",
       "                      [-1.0071e-03, -1.5335e-03,  3.7842e-03,  ..., -6.4850e-04,\n",
       "                       -1.7548e-03,  1.4420e-03],\n",
       "                      [ 1.8921e-03,  2.4261e-03, -1.1902e-03,  ...,  3.0518e-03,\n",
       "                        2.6703e-03, -6.3896e-05],\n",
       "                      ...,\n",
       "                      [ 5.6624e-06, -3.8605e-03, -2.5749e-04,  ...,  6.0425e-03,\n",
       "                        1.0834e-03, -5.4550e-04],\n",
       "                      [ 7.1716e-04,  2.8229e-03, -6.4468e-04,  ..., -1.3161e-04,\n",
       "                       -5.0049e-03, -6.1035e-03],\n",
       "                      [-1.3275e-03, -2.4033e-04, -1.8387e-03,  ...,  2.3651e-03,\n",
       "                       -5.2261e-04, -1.3275e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.30.mlp.up_proj.weight',\n",
       "              tensor([[-0.0005, -0.0021, -0.0014,  ...,  0.0016,  0.0008,  0.0063],\n",
       "                      [-0.0044,  0.0056,  0.0021,  ...,  0.0007,  0.0029, -0.0036],\n",
       "                      [-0.0015, -0.0075, -0.0021,  ...,  0.0011,  0.0056, -0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0002, -0.0024,  0.0002,  ...,  0.0017,  0.0044,  0.0001],\n",
       "                      [-0.0019, -0.0021, -0.0036,  ...,  0.0018, -0.0053,  0.0013],\n",
       "                      [ 0.0017, -0.0018,  0.0045,  ..., -0.0026, -0.0026, -0.0014]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.mlp.down_proj.weight',\n",
       "              tensor([[ 3.9673e-03, -3.0327e-04,  1.8311e-03,  ..., -7.7438e-04,\n",
       "                       -1.9379e-03,  8.2397e-04],\n",
       "                      [-6.6376e-04, -4.6997e-03, -3.1281e-03,  ...,  1.5030e-03,\n",
       "                       -2.5024e-03,  3.3379e-04],\n",
       "                      [-1.6403e-03,  3.0060e-03,  3.8910e-03,  ...,  2.7466e-03,\n",
       "                        2.8076e-03, -1.5945e-03],\n",
       "                      ...,\n",
       "                      [ 2.9755e-04,  2.9144e-03, -2.9945e-04,  ..., -9.7275e-05,\n",
       "                       -4.8065e-04,  1.2131e-03],\n",
       "                      [ 1.2207e-03, -1.9360e-04, -9.1553e-04,  ..., -1.5411e-03,\n",
       "                        1.2665e-03,  4.4861e-03],\n",
       "                      [-1.4267e-03, -2.9087e-05,  1.4038e-03,  ...,  2.6245e-03,\n",
       "                        1.3123e-03, -9.9945e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.30.input_layernorm.weight',\n",
       "              tensor([2.7188, 2.6094, 2.6562,  ..., 2.4219, 2.4219, 2.6250],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.post_attention_layernorm.weight',\n",
       "              tensor([3.7500, 3.9844, 3.8750,  ..., 3.7969, 3.7188, 3.8438],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.q_proj.weight',\n",
       "              tensor([[ 4.8447e-04, -1.6632e-03, -5.9128e-04,  ..., -2.6550e-03,\n",
       "                        3.8300e-03, -5.7602e-04],\n",
       "                      [ 2.4414e-03, -2.2125e-03, -1.1978e-03,  ...,  9.3460e-04,\n",
       "                        6.1417e-04, -1.2741e-03],\n",
       "                      [-2.1815e-05,  2.2430e-03, -1.0872e-04,  ...,  2.8687e-03,\n",
       "                       -7.5531e-04,  6.6528e-03],\n",
       "                      ...,\n",
       "                      [-5.2643e-04, -1.4114e-03,  2.6245e-03,  ..., -6.2180e-04,\n",
       "                       -2.3346e-03,  1.0729e-04],\n",
       "                      [ 7.1106e-03, -5.5313e-04, -7.9346e-04,  ..., -2.7466e-03,\n",
       "                        2.9755e-04, -1.4267e-03],\n",
       "                      [-2.4261e-03, -7.4768e-04,  4.1504e-03,  ...,  1.6098e-03,\n",
       "                        9.0027e-04, -1.6861e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0017, -0.0012, -0.0014,  ...,  0.0005,  0.0011,  0.0009],\n",
       "                      [ 0.0020,  0.0010, -0.0018,  ...,  0.0019,  0.0022, -0.0006],\n",
       "                      [ 0.0012, -0.0020,  0.0023,  ...,  0.0026, -0.0015,  0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0005,  0.0011,  0.0017,  ..., -0.0024, -0.0024, -0.0003],\n",
       "                      [ 0.0014, -0.0033, -0.0001,  ..., -0.0040,  0.0011, -0.0002],\n",
       "                      [-0.0043,  0.0061, -0.0062,  ..., -0.0009,  0.0039,  0.0007]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.v_proj.weight',\n",
       "              tensor([[ 1.7700e-03, -2.2583e-03,  8.6784e-05,  ..., -6.0730e-03,\n",
       "                       -3.3188e-04,  1.0824e-04],\n",
       "                      [ 2.1515e-03,  2.5482e-03, -3.9978e-03,  ..., -4.5166e-03,\n",
       "                       -5.8594e-03, -3.8910e-03],\n",
       "                      [ 6.1340e-03,  1.3809e-03, -7.4387e-04,  ...,  1.7548e-03,\n",
       "                       -8.8501e-04,  2.5177e-03],\n",
       "                      ...,\n",
       "                      [ 4.0588e-03, -2.3499e-03, -3.6163e-03,  ..., -4.7302e-03,\n",
       "                       -2.8229e-04, -2.7161e-03],\n",
       "                      [ 5.3101e-03,  5.7373e-03,  5.3787e-04,  ..., -8.3618e-03,\n",
       "                       -1.0872e-04,  7.3853e-03],\n",
       "                      [-1.1719e-02,  1.1749e-03, -6.4087e-03,  ...,  8.1787e-03,\n",
       "                        1.0620e-02, -4.9591e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0022, -0.0017, -0.0015,  ...,  0.0017,  0.0081, -0.0010],\n",
       "                      [ 0.0017, -0.0030,  0.0018,  ..., -0.0024, -0.0038,  0.0017],\n",
       "                      [-0.0006, -0.0042,  0.0007,  ..., -0.0017, -0.0077, -0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0033,  0.0013,  0.0030,  ...,  0.0032, -0.0004,  0.0027],\n",
       "                      [ 0.0023,  0.0019, -0.0010,  ...,  0.0021, -0.0038,  0.0009],\n",
       "                      [-0.0047,  0.0005,  0.0012,  ...,  0.0039,  0.0003, -0.0017]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0003,  0.0002,  0.0041,  ..., -0.0007,  0.0015,  0.0025],\n",
       "                      [ 0.0068,  0.0033,  0.0024,  ...,  0.0061, -0.0011,  0.0013],\n",
       "                      [ 0.0006,  0.0017, -0.0045,  ..., -0.0010, -0.0020, -0.0008],\n",
       "                      ...,\n",
       "                      [ 0.0033, -0.0042, -0.0043,  ..., -0.0037, -0.0035, -0.0046],\n",
       "                      [ 0.0024, -0.0042,  0.0071,  ..., -0.0021,  0.0037, -0.0001],\n",
       "                      [-0.0067,  0.0004,  0.0017,  ..., -0.0017, -0.0029,  0.0012]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0015, -0.0070,  0.0053,  ...,  0.0056, -0.0015,  0.0007],\n",
       "                      [ 0.0005,  0.0033, -0.0060,  ...,  0.0018,  0.0019, -0.0057],\n",
       "                      [-0.0022, -0.0032, -0.0003,  ..., -0.0019, -0.0020,  0.0041],\n",
       "                      ...,\n",
       "                      [-0.0034,  0.0015,  0.0007,  ...,  0.0023,  0.0027,  0.0068],\n",
       "                      [ 0.0017,  0.0022,  0.0019,  ..., -0.0013, -0.0005,  0.0008],\n",
       "                      [-0.0035, -0.0009,  0.0007,  ..., -0.0049,  0.0037,  0.0014]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0018, -0.0019,  0.0023,  ...,  0.0070,  0.0033,  0.0008],\n",
       "                      [ 0.0021,  0.0014,  0.0024,  ..., -0.0031,  0.0003, -0.0028],\n",
       "                      [-0.0003,  0.0035, -0.0030,  ...,  0.0002,  0.0021, -0.0025],\n",
       "                      ...,\n",
       "                      [-0.0051, -0.0029,  0.0025,  ..., -0.0012, -0.0015,  0.0016],\n",
       "                      [ 0.0004, -0.0076,  0.0016,  ..., -0.0028,  0.0058,  0.0093],\n",
       "                      [-0.0038,  0.0039, -0.0045,  ..., -0.0046, -0.0039,  0.0025]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.input_layernorm.weight',\n",
       "              tensor([2.5312, 2.5938, 2.5781,  ..., 2.6094, 2.4219, 2.6719],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.post_attention_layernorm.weight',\n",
       "              tensor([3.7031, 3.7969, 3.7969,  ..., 3.5312, 3.5312, 3.7188],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.norm.weight',\n",
       "              tensor([5.3438, 5.5312, 5.4062,  ..., 5.2812, 5.5625, 5.2812],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.weight',\n",
       "              tensor([[ 0.0116, -0.0311, -0.0229,  ..., -0.0412, -0.0381, -0.0169],\n",
       "                      [-0.0001,  0.0220,  0.0222,  ...,  0.0388,  0.0287,  0.0233]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.bias', tensor([-0.0129,  0.0118], dtype=torch.float16))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_state_dict = torch.load(f\"/home/it/environments/Genety/models/mistral/{today}_mistral.pth\", mmap=True)\n",
    "loaded_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "80931c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(loaded_state_dict, assign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80ef3a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"./models/mistral/{today}_mistral.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281e1c64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Genety (conda)",
   "language": "python",
   "name": "genety-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
