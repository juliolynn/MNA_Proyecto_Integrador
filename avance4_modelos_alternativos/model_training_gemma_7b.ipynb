{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682cc4d4-ebb1-4e48-b84e-1e62006da233",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61331a7-f1dd-442e-b58a-a5adb337fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ec5c8-c16a-4697-86b5-e22292d3266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:42:31,591] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_symbind_alt@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__nptl_change_stack_perm@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_find_dso_for_object@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_fatal_printf@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_exception_create@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__tunable_get_val@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_preinit@GLIBC_PRIVATE'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# DeepSpeed ZeRO-3\n",
    "import deepspeed\n",
    "from deepspeed.accelerator import get_accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd5b450-0317-4c55-847e-7723042940f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a0b869e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free(GB): 22.8790283203125, Global(GB): 23.64971923828125, Free(%): 0.9674122593082944\n"
     ]
    }
   ],
   "source": [
    "(free_memory, global_memory) = torch.cuda.mem_get_info()\n",
    "print(f\"Free(GB): {free_memory/1024/1024/1024}, Global(GB): {global_memory/1024/1024/1024}, Free(%): {free_memory/global_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c439d2-2280-4823-90dd-0867012b907c",
   "metadata": {},
   "source": [
    "# Load Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a715f751-ab65-45a3-ae17-c795b8c78d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Identity</th>\n",
       "      <th>Text</th>\n",
       "      <th>A2-Unambiguous</th>\n",
       "      <th>A4-Tolerances</th>\n",
       "      <th>A5-Sources specified</th>\n",
       "      <th>E1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSS_CONNECTIVITY</td>\n",
       "      <td>SRD_GSS_FUNC_61</td>\n",
       "      <td>The User and Rights Administration HMI shall p...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR2146</td>\n",
       "      <td>The Network Function shall support WiFi 802.11...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR797</td>\n",
       "      <td>The PwrCon software shall monitor the output v...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR3013</td>\n",
       "      <td>When prompted, the TETRA Software shall place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR3198</td>\n",
       "      <td>The TETRA software shall allow users to select...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type         Identity  \\\n",
       "0  GSS_CONNECTIVITY  SRD_GSS_FUNC_61   \n",
       "1        Cobham_ATR     SHLR-ATR2146   \n",
       "2        Cobham_ATR      SHLR-ATR797   \n",
       "3        Cobham_ATR     SHLR-ATR3013   \n",
       "4        Cobham_ATR     SHLR-ATR3198   \n",
       "\n",
       "                                                Text A2-Unambiguous  \\\n",
       "0  The User and Rights Administration HMI shall p...              1   \n",
       "1  The Network Function shall support WiFi 802.11...              1   \n",
       "2  The PwrCon software shall monitor the output v...              1   \n",
       "3  When prompted, the TETRA Software shall place ...              1   \n",
       "4  The TETRA software shall allow users to select...              1   \n",
       "\n",
       "  A4-Tolerances A5-Sources specified  E1  \n",
       "0            na                   na   1  \n",
       "1            na                   na   1  \n",
       "2            na                   na   1  \n",
       "3            na                   na   1  \n",
       "4            na                   na   1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "master_df = pd.read_excel('./DATASETS/Training_Dataset.xlsx')\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bdf637-9e84-4cee-a95c-e5c7bd8075d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The User and Rights Administration HMI shall p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Network Function shall support WiFi 802.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The PwrCon software shall monitor the output v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>When prompted, the TETRA Software shall place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The TETRA software shall allow users to select...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   E1                                               Text\n",
       "0   1  The User and Rights Administration HMI shall p...\n",
       "1   1  The Network Function shall support WiFi 802.11...\n",
       "2   1  The PwrCon software shall monitor the output v...\n",
       "3   1  When prompted, the TETRA Software shall place ...\n",
       "4   1  The TETRA software shall allow users to select..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = master_df[['E1','Text']].copy()\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9296d035-a173-4c8e-81d9-a30a5021dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3255 entries, 0 to 3254\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   E1      3255 non-null   int64 \n",
      " 1   Text    3255 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 51.0+ KB\n"
     ]
    }
   ],
   "source": [
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c738bf70-5be7-4b1e-9d88-1e09f2f497a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2127"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df_label1 = model_df.query('E1 == 1')\n",
    "len(model_df_label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a967f2ce-ce0f-4aa7-8b6e-759011eef775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df_label0 = model_df.query('E1 == 0')\n",
    "len(model_df_label0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fceb9f5-cd13-4779-b8db-57ff9cda386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat([model_df_label1[:1500],model_df_label0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7716b-cbb9-40df-8796-9e2718512f6b",
   "metadata": {},
   "source": [
    "# Data process and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e0084e-559e-46cc-b1a4-892e5e0fcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "train_df, test_df = train_test_split(model_df, test_size=0.1, shuffle=True)\n",
    "\n",
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "test_iter = iter(list(test_df.itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6daca09-ad60-40b6-90f2-e91c79aabb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2365"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6424905-7a3d-4fd5-a22d-f2be29be1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\",\n",
    "    \"tokenizer\",\n",
    "    \"google/gemma-7b\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc4e1ac2-7bff-4984-8bf5-bde7daceef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaTokenizerFast(name_or_path='google/gemma-7b', vocab_size=256000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<bos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t4: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t5: AddedToken(\"<2mass>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t6: AddedToken(\"[@BOS@]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t7: AddedToken(\"<unused0>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t8: AddedToken(\"<unused1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t9: AddedToken(\"<unused2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t10: AddedToken(\"<unused3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t11: AddedToken(\"<unused4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t12: AddedToken(\"<unused5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t13: AddedToken(\"<unused6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t14: AddedToken(\"<unused7>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t15: AddedToken(\"<unused8>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t16: AddedToken(\"<unused9>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t17: AddedToken(\"<unused10>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t18: AddedToken(\"<unused11>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t19: AddedToken(\"<unused12>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t20: AddedToken(\"<unused13>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t21: AddedToken(\"<unused14>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t22: AddedToken(\"<unused15>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t23: AddedToken(\"<unused16>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t24: AddedToken(\"<unused17>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t25: AddedToken(\"<unused18>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t26: AddedToken(\"<unused19>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t27: AddedToken(\"<unused20>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t28: AddedToken(\"<unused21>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t29: AddedToken(\"<unused22>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t30: AddedToken(\"<unused23>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t31: AddedToken(\"<unused24>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t32: AddedToken(\"<unused25>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t33: AddedToken(\"<unused26>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t34: AddedToken(\"<unused27>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t35: AddedToken(\"<unused28>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t36: AddedToken(\"<unused29>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t37: AddedToken(\"<unused30>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t38: AddedToken(\"<unused31>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t39: AddedToken(\"<unused32>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t40: AddedToken(\"<unused33>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t41: AddedToken(\"<unused34>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t42: AddedToken(\"<unused35>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t43: AddedToken(\"<unused36>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t44: AddedToken(\"<unused37>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t45: AddedToken(\"<unused38>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t46: AddedToken(\"<unused39>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t47: AddedToken(\"<unused40>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t48: AddedToken(\"<unused41>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t49: AddedToken(\"<unused42>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t50: AddedToken(\"<unused43>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t51: AddedToken(\"<unused44>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t52: AddedToken(\"<unused45>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t53: AddedToken(\"<unused46>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t54: AddedToken(\"<unused47>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t55: AddedToken(\"<unused48>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t56: AddedToken(\"<unused49>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t57: AddedToken(\"<unused50>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t58: AddedToken(\"<unused51>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t59: AddedToken(\"<unused52>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t60: AddedToken(\"<unused53>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t61: AddedToken(\"<unused54>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t62: AddedToken(\"<unused55>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t63: AddedToken(\"<unused56>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t64: AddedToken(\"<unused57>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t65: AddedToken(\"<unused58>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t66: AddedToken(\"<unused59>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t67: AddedToken(\"<unused60>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t68: AddedToken(\"<unused61>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t69: AddedToken(\"<unused62>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t70: AddedToken(\"<unused63>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t71: AddedToken(\"<unused64>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t72: AddedToken(\"<unused65>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t73: AddedToken(\"<unused66>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t74: AddedToken(\"<unused67>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t75: AddedToken(\"<unused68>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t76: AddedToken(\"<unused69>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t77: AddedToken(\"<unused70>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t78: AddedToken(\"<unused71>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t79: AddedToken(\"<unused72>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t80: AddedToken(\"<unused73>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t81: AddedToken(\"<unused74>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t82: AddedToken(\"<unused75>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t83: AddedToken(\"<unused76>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t84: AddedToken(\"<unused77>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t85: AddedToken(\"<unused78>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t86: AddedToken(\"<unused79>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t87: AddedToken(\"<unused80>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t88: AddedToken(\"<unused81>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t89: AddedToken(\"<unused82>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t90: AddedToken(\"<unused83>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t91: AddedToken(\"<unused84>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t92: AddedToken(\"<unused85>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t93: AddedToken(\"<unused86>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t94: AddedToken(\"<unused87>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t95: AddedToken(\"<unused88>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t96: AddedToken(\"<unused89>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t97: AddedToken(\"<unused90>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t98: AddedToken(\"<unused91>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t99: AddedToken(\"<unused92>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t100: AddedToken(\"<unused93>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t101: AddedToken(\"<unused94>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t102: AddedToken(\"<unused95>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t103: AddedToken(\"<unused96>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t104: AddedToken(\"<unused97>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t105: AddedToken(\"<unused98>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t106: AddedToken(\"<start_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t107: AddedToken(\"<end_of_turn>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t108: AddedToken(\"\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t109: AddedToken(\"\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t110: AddedToken(\"\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t111: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t112: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t113: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t114: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t115: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t116: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t117: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t118: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t119: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t120: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t121: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t122: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t123: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t124: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t125: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t126: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t127: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t128: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t129: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t130: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t131: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t132: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t133: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t134: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t135: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t136: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t137: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t138: AddedToken(\"\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t139: AddedToken(\"▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t140: AddedToken(\"▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t141: AddedToken(\"▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t142: AddedToken(\"▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t143: AddedToken(\"▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t144: AddedToken(\"▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t145: AddedToken(\"▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t146: AddedToken(\"▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t147: AddedToken(\"▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t148: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t149: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t150: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t151: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t152: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t153: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t154: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t155: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t156: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t157: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t158: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t159: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t160: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t161: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t162: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t163: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t164: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t165: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t166: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t167: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t168: AddedToken(\"▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t169: AddedToken(\"<table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t170: AddedToken(\"<caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t171: AddedToken(\"<thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t172: AddedToken(\"<tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t173: AddedToken(\"<tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t174: AddedToken(\"<tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t175: AddedToken(\"<th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t176: AddedToken(\"<td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t177: AddedToken(\"</table>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t178: AddedToken(\"</caption>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t179: AddedToken(\"</thead>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t180: AddedToken(\"</tbody>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t181: AddedToken(\"</tfoot>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t182: AddedToken(\"</tr>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t183: AddedToken(\"</th>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t184: AddedToken(\"</td>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t185: AddedToken(\"<h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t186: AddedToken(\"<h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t187: AddedToken(\"<h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t188: AddedToken(\"<h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t189: AddedToken(\"<h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t190: AddedToken(\"<h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t191: AddedToken(\"<blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t192: AddedToken(\"</h1>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t193: AddedToken(\"</h2>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t194: AddedToken(\"</h3>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t195: AddedToken(\"</h4>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t196: AddedToken(\"</h5>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t197: AddedToken(\"</h6>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t198: AddedToken(\"</blockquote>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t199: AddedToken(\"<strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t200: AddedToken(\"<em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t201: AddedToken(\"<b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t202: AddedToken(\"<i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t203: AddedToken(\"<u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t204: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t205: AddedToken(\"<sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t206: AddedToken(\"<sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t207: AddedToken(\"<code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t208: AddedToken(\"</strong>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t209: AddedToken(\"</em>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t210: AddedToken(\"</b>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t211: AddedToken(\"</i>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t212: AddedToken(\"</u>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t213: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t214: AddedToken(\"</sub>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t215: AddedToken(\"</sup>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "\t216: AddedToken(\"</code>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d1986-12ed-45c3-8a18-bd1ada1e9440",
   "metadata": {},
   "source": [
    "# Dataset iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da70fca8-0081-41de-a40f-d5241ec710f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "train_df, test_df = train_test_split(model_df, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "880ea8d4-bae1-48bc-996e-d352db324265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "test_iter = iter(list(test_df.itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78c4e14f-9c36-4343-ba96-f05c6eb3196e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 'The firewall shall report dropped packet.')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "092203c5-49af-4337-abbd-38851caa6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Get label and text\n",
    "    y, x = list(zip(*batch))\n",
    "\n",
    "    # Create list with indices from tokeniser\n",
    "\n",
    "    encoded_x = tokenizer(x, padding=True, truncation=True)\n",
    "    encoded_x.input_ids = torch.tensor(encoded_x.input_ids).to(device)\n",
    "    encoded_x.attention_mask = torch.tensor(encoded_x.attention_mask).to(device)  \n",
    "    \n",
    "    return encoded_x, torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a81c8d9-de06-4aed-9ff3-0c5f31985d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'The firewall shall report dropped packet.')\n",
      "(0, 'The dog barked its way out of the room.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_ids': [[1, 1, 1, 2, 651, 72466, 3213, 3484, 15143, 25514, 235265], [2, 651, 5929, 214592, 1277, 1703, 921, 576, 573, 2965, 235265]], 'attention_mask': [[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]},\n",
       " tensor([0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "first = next(train_iter)\n",
    "second = next(train_iter)\n",
    "\n",
    "print(first)\n",
    "print(second)\n",
    "\n",
    "collate_batch([first, second])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf18db8-7d58-43c0-ac22-d3e225cb2ed1",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d306ecb-d8ef-4389-ab6c-8fb803124ca1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:43:14,712] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-10 11:43:14,713] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-10 11:43:14,798] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.1.1.204, master_port=29500\n",
      "[2024-05-10 11:43:14,799] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Gemma's activation function should be approximate GeLU and not exact GeLU.\n",
      "Changing the activation function to `gelu_pytorch_tanh`.if you want to use the legacy `gelu`, edit the `model.config` to set `hidden_activation=gelu`   instead of `hidden_act`. See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49e7149f9c98477eb9c316213a9a45ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at google/gemma-7b and are newly initialized because the shapes did not match:\n",
      "- model.embed_tokens.weight: found shape torch.Size([256000, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.0.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.1.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.2.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.3.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.4.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.5.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.10.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.11.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.12.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.13.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.14.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.6.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.7.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.8.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.9.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.15.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.16.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.17.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.18.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.19.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.20.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.21.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.22.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.23.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.24.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.25.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.26.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.input_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.mlp.down_proj.weight: found shape torch.Size([3072, 24576]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.mlp.gate_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.mlp.up_proj.weight: found shape torch.Size([24576, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.post_attention_layernorm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.self_attn.k_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.self_attn.o_proj.weight: found shape torch.Size([3072, 4096]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.self_attn.q_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.layers.27.self_attn.v_proj.weight: found shape torch.Size([4096, 3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "- model.norm.weight: found shape torch.Size([3072]) in the checkpoint and torch.Size([0]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:43:17,846] [INFO] [partition_parameters.py:345:__exit__] finished initializing model - num_params = 255, num_elems = 8.54B\n"
     ]
    }
   ],
   "source": [
    "with deepspeed.zero.Init():\n",
    "    model = torch.hub.load(\n",
    "        \"huggingface/pytorch-transformers\",\n",
    "        \"modelForSequenceClassification\",\n",
    "        \"google/gemma-7b\",\n",
    "        ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "402baf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f536f52c38fb40c19dde1cbff6cabaab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GemmaForSequenceClassification were not initialized from the model checkpoint at google/gemma-7b and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "with torch.device('meta'):\n",
    "    model = torch.hub.load(\n",
    "        \"huggingface/pytorch-transformers\",\n",
    "        \"modelForSequenceClassification\",\n",
    "        \"google/gemma-7b\",\n",
    "        ignore_mismatched_sizes=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fd9daab-93a4-44fb-8193-94c8039002e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForSequenceClassification(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (score): Linear(in_features=3072, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e6724c0-3768-48ee-9176-2518cabbfe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7e98c25-1d44-4a02-997c-8ec0c4e123c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parameter in enumerate(model.parameters()):\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41954657-e0cf-40d1-9143-75e11701c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score = nn.Sequential(\n",
    "    nn.Linear(in_features=3072, out_features=3072),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=3072, out_features=2),\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d8afdced-aa5f-4dae-9ef0-7183668838d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GemmaForSequenceClassification(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "          (rotary_emb): GemmaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "          (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm()\n",
       "        (post_attention_layernorm): GemmaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm()\n",
       "  )\n",
       "  (score): Sequential(\n",
       "    (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=3072, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1421e4-51bc-490c-b47d-01d83e0a28cf",
   "metadata": {},
   "source": [
    "# Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a28ea92-68d0-44a7-9a5d-313ee209533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import time\n",
    "\n",
    "def train(model, dataloader, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 5\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    for idx, (data, label) in enumerate(dataloader):         \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "        predicted_label = outputs.logits\n",
    "        loss = criterion(predicted_label, label)\n",
    "        \n",
    "        # Deepspeed model engine, backward pass \n",
    "        model.backward(loss)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Deepspeed model engine, optimizer step\n",
    "        model.step()\n",
    "        \n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Deepspeed model engine, empty cache\n",
    "        model.empty_partition_cache()      \n",
    "        \n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "        \n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, label) in enumerate(dataloader):      \n",
    "            outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "            predicted_label = outputs.logits\n",
    "            loss = criterion(predicted_label, label)\n",
    "            \n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count, loss.item() / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa69937",
   "metadata": {},
   "source": [
    "# Deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14c32717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:44:09,049] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-10 11:44:09,057] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-10 11:44:10,159] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/it/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/it/.cache/torch_extensions/py311_cu121/cpu_adam/build.ninja...\n",
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.196258544921875 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:44:12,727] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-05-10 11:44:12,728] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-10 11:44:12,731] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-05-10 11:44:12,731] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-05-10 11:44:12,732] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-05-10 11:44:12,732] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-05-10 11:44:12,820] [INFO] [utils.py:779:see_memory_usage] Stage 3 initialize beginning\n",
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.010000, betas=(0.800000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-05-10 11:44:12,821] [INFO] [utils.py:780:see_memory_usage] MA 15.9 GB         Max_MA 18.83 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:12,821] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.72 GB, percent = 12.2%\n",
      "[2024-05-10 11:44:12,822] [INFO] [stage3.py:130:__init__] Reduce bucket size 200000000\n",
      "[2024-05-10 11:44:12,822] [INFO] [stage3.py:131:__init__] Prefetch bucket size 0\n",
      "[2024-05-10 11:44:12,902] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-05-10 11:44:12,903] [INFO] [utils.py:780:see_memory_usage] MA 15.9 GB         Max_MA 15.9 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:12,903] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.72 GB, percent = 12.2%\n",
      "Parameter Offload: Total persistent parameters: 184322 in 60 params\n",
      "[2024-05-10 11:44:12,992] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-05-10 11:44:12,992] [INFO] [utils.py:780:see_memory_usage] MA 15.92 GB         Max_MA 15.94 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:12,992] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.72 GB, percent = 12.2%\n",
      "[2024-05-10 11:44:13,071] [INFO] [utils.py:779:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-05-10 11:44:13,072] [INFO] [utils.py:780:see_memory_usage] MA 15.92 GB         Max_MA 15.92 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:13,072] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.72 GB, percent = 12.2%\n",
      "[2024-05-10 11:44:13,184] [INFO] [utils.py:779:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2024-05-10 11:44:13,184] [INFO] [utils.py:780:see_memory_usage] MA 15.9 GB         Max_MA 15.92 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:13,185] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.75 GB, percent = 12.3%\n",
      "[2024-05-10 11:44:13,265] [INFO] [utils.py:779:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-05-10 11:44:13,265] [INFO] [utils.py:780:see_memory_usage] MA 15.9 GB         Max_MA 15.9 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:13,265] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.75 GB, percent = 12.3%\n",
      "[2024-05-10 11:44:13,369] [INFO] [utils.py:779:see_memory_usage] After creating fp32 partitions\n",
      "[2024-05-10 11:44:13,370] [INFO] [utils.py:780:see_memory_usage] MA 15.9 GB         Max_MA 15.9 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:13,370] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.8 GB, percent = 12.4%\n",
      "[2024-05-10 11:44:13,454] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-10 11:44:13,454] [INFO] [utils.py:780:see_memory_usage] MA 15.9 GB         Max_MA 15.9 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:13,454] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.8 GB, percent = 12.4%\n",
      "[2024-05-10 11:44:13,559] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-10 11:44:13,560] [INFO] [utils.py:780:see_memory_usage] MA 15.9 GB         Max_MA 15.9 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:13,560] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.84 GB, percent = 12.6%\n",
      "[2024-05-10 11:44:13,560] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-05-10 11:44:13,650] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-10 11:44:13,650] [INFO] [utils.py:780:see_memory_usage] MA 16.28 GB         Max_MA 16.31 GB         CA 19.28 GB         Max_CA 19 GB \n",
      "[2024-05-10 11:44:13,650] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 3.84 GB, percent = 12.6%\n",
      "[2024-05-10 11:44:13,651] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-05-10 11:44:13,651] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2024-05-10 11:44:13,651] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x756391733890>\n",
      "[2024-05-10 11:44:13,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:44:13,651] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-10 11:44:13,652] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-10 11:44:13,652] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-10 11:44:13,652] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-10 11:44:13,652] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-10 11:44:13,652] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7563ac37c5d0>\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-10 11:44:13,653] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:44:13,654] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-10 11:44:13,654] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-10 11:44:13,654] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-10 11:44:13,654] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-10 11:44:13,654] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-10 11:44:13,654] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-10 11:44:13,654] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-10 11:44:13,655] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   fp16_enabled ................. True\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-10 11:44:13,656] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   loss_scale ................... 0\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-10 11:44:13,657] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-10 11:44:13,658] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-10 11:44:13,658] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-10 11:44:13,658] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-10 11:44:13,658] [INFO] [config.py:1000:print]   optimizer_name ............... adam\n",
      "[2024-05-10 11:44:13,658] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.01, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
      "[2024-05-10 11:44:13,658] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupLR\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.01, 'warmup_num_steps': 1000}\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-10 11:44:13,659] [INFO] [config.py:1000:print]   steps_per_print .............. 10\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   train_batch_size ............. 16\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-10 11:44:13,660] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False\n",
      "[2024-05-10 11:44:13,661] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=0 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-10 11:44:13,661] [INFO] [config.py:1000:print]   zero_enabled ................. True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:44:13,661] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-10 11:44:13,661] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\n",
      "[2024-05-10 11:44:13,661] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.01, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.01, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"load_from_fp32_weights\": true, \n",
      "        \"gather_16bit_weights_on_model_save\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"stage3_prefetch_bucket_size\": 0\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepSpeedEngine(\n",
       "  (module): GemmaForSequenceClassification(\n",
       "    (model): GemmaModel(\n",
       "      (embed_tokens): Embedding(256000, 3072, padding_idx=0)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x GemmaDecoderLayer(\n",
       "          (self_attn): GemmaSdpaAttention(\n",
       "            (q_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "            (k_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "            (v_proj): Linear(in_features=3072, out_features=4096, bias=False)\n",
       "            (o_proj): Linear(in_features=4096, out_features=3072, bias=False)\n",
       "            (rotary_emb): GemmaRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): GemmaMLP(\n",
       "            (gate_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "            (up_proj): Linear(in_features=3072, out_features=24576, bias=False)\n",
       "            (down_proj): Linear(in_features=24576, out_features=3072, bias=False)\n",
       "            (act_fn): PytorchGELUTanh()\n",
       "          )\n",
       "          (input_layernorm): GemmaRMSNorm()\n",
       "          (post_attention_layernorm): GemmaRMSNorm()\n",
       "        )\n",
       "      )\n",
       "      (norm): GemmaRMSNorm()\n",
       "    )\n",
       "    (score): Sequential(\n",
       "      (0): Linear(in_features=3072, out_features=3072, bias=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=3072, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepspeed_config = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "            \"lr\": 0.01,\n",
    "            \"betas\": [\n",
    "                0.8,\n",
    "                0.999\n",
    "            ],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 3e-7,\n",
    "        },\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": 0,\n",
    "            \"warmup_max_lr\": 0.01,\n",
    "            \"warmup_num_steps\": 1000,\n",
    "        },\n",
    "    },\n",
    "    \"train_batch_size\": 16,\n",
    "    # \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    # \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"fp16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"load_from_fp32_weights\": True,\n",
    "        \"gather_16bit_weights_on_model_save\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"stage3_prefetch_bucket_size\": 0,\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"train_batch_size\": 16,\n",
    "}\n",
    "\n",
    "# Initialize DeepSpeed Engine\n",
    "model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    config=deepspeed_config,\n",
    ")\n",
    "model_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c09d96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = get_accelerator().device_name(model_engine.local_rank)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0ddbc-f1e1-4662-a3d2-c09beafd25ed",
   "metadata": {},
   "source": [
    "# Split the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab8adb52-47f3-4740-952f-639d21a70d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "BATCH_SIZE = 16  # batch size for training\n",
    "\n",
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.8)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b2abb8af-8fe2-4382-8adf-7d2af9cd8e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today = date.today().isoformat()\n",
    "model_name = \"gemma_7b\"\n",
    "checkpoint_path = f\"./models/{model_name}\"\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "# EPOCHS = 20  # epoch\n",
    "# LR = 0.1 # learning rate\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "    \n",
    "\n",
    "def train_with_hist(model, epochs):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_accu = None\n",
    "    best_accu_val = 0.85\n",
    "\n",
    "    loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid = [], [], [], []\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        accu_train, loss_train = train(model, train_dataloader, epoch)\n",
    "        accu_val, loss_val = evaluate(model, valid_dataloader)\n",
    "        \n",
    "        print({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss_train\": loss_train,\n",
    "            \"loss_val\": loss_val,\n",
    "            \"accuracy_train\": accu_train,\n",
    "            \"accuracy_val\": accu_val,\n",
    "        })\n",
    "\n",
    "        loss_hist_train.append(loss_train)\n",
    "        loss_hist_valid.append(loss_val)\n",
    "        accuracy_hist_train.append(accu_train)\n",
    "        accuracy_hist_valid.append(accu_val)\n",
    "        \n",
    "        get_accelerator().empty_cache()\n",
    "        \n",
    "        if accu_val > best_accu_val:\n",
    "            best_accu_val = accu_val\n",
    "            model_engine.save_16bit_model(f\"./models/{model_name}/\", f\"{today}_{model_name}_checkpoint.pth\")\n",
    "        \n",
    "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00c4e1f4-9821-42a5-929f-e297c8cd9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "get_accelerator().empty_cache()\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "18f039e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free(GB): 4.8634033203125, Global(GB): 23.64971923828125, Free(%): 0.2056431736593398\n"
     ]
    }
   ],
   "source": [
    "(free_memory, global_memory) = torch.cuda.mem_get_info()\n",
    "print(f\"Free(GB): {free_memory/1024/1024/1024}, Global(GB): {global_memory/1024/1024/1024}, Free(%): {free_memory/global_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97e7023b-7888-4afe-906a-76ac56aacab8",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:44:45,122] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2024-05-10 11:44:45,478] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2024-05-10 11:44:45,858] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2024-05-10 11:44:46,167] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2024-05-10 11:44:46,751] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2024-05-10 11:44:47,149] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "[2024-05-10 11:44:47,500] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2024-05-10 11:44:48,121] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2024-05-10 11:44:48,451] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2024-05-10 11:44:48,829] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2024-05-10 11:44:48,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:44:48,829] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=50.75324409643696, CurrSamplesPerSec=59.21089971916036, MemAllocated=16.98GB, MaxMemAllocated=19.11GB\n",
      "[2024-05-10 11:44:49,169] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "[2024-05-10 11:44:49,421] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2024-05-10 11:44:49,870] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2024-05-10 11:44:50,221] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2024-05-10 11:44:50,600] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2024-05-10 11:44:50,890] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "[2024-05-10 11:44:51,371] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2024-05-10 11:44:52,313] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2024-05-10 11:44:52,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=18, lr=[0.0010034333188799374], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:44:52,639] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=54.31002965390943, CurrSamplesPerSec=74.01244921277036, MemAllocated=16.75GB, MaxMemAllocated=19.11GB\n",
      "[2024-05-10 11:44:56,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=18, lr=[0.0035972708201587496], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:44:56,587] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=54.32330714818567, CurrSamplesPerSec=26.541188381952793, MemAllocated=18.19GB, MaxMemAllocated=19.84GB\n",
      "[2024-05-10 11:44:57,427] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "[2024-05-10 11:45:00,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=19, lr=[0.004407397649113065], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:00,606] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=54.249172010597036, CurrSamplesPerSec=58.09104286916266, MemAllocated=16.97GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:01,537] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "[2024-05-10 11:45:03,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=20, lr=[0.004923737515732209], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:03,912] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=56.89196140886215, CurrSamplesPerSec=52.08309843414337, MemAllocated=17.17GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:07,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=20, lr=[0.005340199971093208], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:07,736] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=56.91092724295579, CurrSamplesPerSec=45.44928164403081, MemAllocated=17.26GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:11,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=20, lr=[0.005663233347786729], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:11,596] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=56.816129227046375, CurrSamplesPerSec=55.94162832823312, MemAllocated=17.02GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:13,583] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "[2024-05-10 11:45:15,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=21, lr=[0.005902840038807148], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:15,200] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=57.41755572981036, CurrSamplesPerSec=72.80742863156387, MemAllocated=16.76GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:19,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=21, lr=[0.006129496969124185], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:19,693] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=55.910175855877284, CurrSamplesPerSec=23.85504305781969, MemAllocated=18.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:23,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=21, lr=[0.006325423637634805], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:23,201] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=56.67802187650899, CurrSamplesPerSec=89.4122010882599, MemAllocated=16.59GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:27,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=21, lr=[0.0064979666888163755], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:27,187] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=56.43370279388107, CurrSamplesPerSec=39.49671237714084, MemAllocated=17.49GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 1, 'loss_train': 0.7499438683023917, 'loss_val': 0.10306553911205074, 'accuracy_train': 0.6601479915433404, 'accuracy_val': 0.7082452431289641}\n",
      "[2024-05-10 11:45:38,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=21, lr=[0.006652117315325167], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:38,743] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=56.61798470558258, CurrSamplesPerSec=53.29903161064919, MemAllocated=17.08GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:39,402] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "[2024-05-10 11:45:43,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=22, lr=[0.006778079184956499], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:43,004] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=56.035653298367166, CurrSamplesPerSec=63.2505190852789, MemAllocated=16.92GB, MaxMemAllocated=21.42GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:45:46,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=22, lr=[0.006906273357687085], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:46,626] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=56.39331484230752, CurrSamplesPerSec=69.24114506219014, MemAllocated=16.83GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:50,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=22, lr=[0.007024033232159561], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:50,260] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=56.69701918322493, CurrSamplesPerSec=56.122352311500634, MemAllocated=17.02GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:54,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=22, lr=[0.007132930288004122], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:54,645] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=56.03399226258286, CurrSamplesPerSec=30.092257580812376, MemAllocated=17.96GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:45:57,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=22, lr=[0.007234205717983191], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:45:57,973] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=56.68838200050753, CurrSamplesPerSec=63.52912487172788, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:01,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=22, lr=[0.007328856956514742], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:01,949] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=56.54861424582928, CurrSamplesPerSec=68.20949236886399, MemAllocated=16.83GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:06,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=22, lr=[0.007417697605752877], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:06,502] [INFO] [timer.py:260:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=55.83293922323339, CurrSamplesPerSec=30.49040542339491, MemAllocated=17.92GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:10,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=22, lr=[0.007501400007696314], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:10,244] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=55.978014867437935, CurrSamplesPerSec=82.07689993322195, MemAllocated=16.65GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:14,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=22, lr=[0.007580526164212267], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:14,505] [INFO] [timer.py:260:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=55.619584906806296, CurrSamplesPerSec=39.34875217606211, MemAllocated=17.48GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:17,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=22, lr=[0.007655550634205104], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:17,941] [INFO] [timer.py:260:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=56.014927060396225, CurrSamplesPerSec=62.547406722538696, MemAllocated=16.94GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:21,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=22, lr=[0.007726877783209205], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:21,288] [INFO] [timer.py:260:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=56.4620433283151, CurrSamplesPerSec=52.46503366387412, MemAllocated=17.13GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:22,715] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1024, reducing to 512\n",
      "{'epoch': 2, 'loss_train': 4.357131538633061, 'loss_val': 0.3810782241014799, 'accuracy_train': 0.7600422832980972, 'accuracy_val': 0.6871035940803383}\n",
      "[2024-05-10 11:46:32,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=23, lr=[0.007788199112828432], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:32,534] [INFO] [timer.py:260:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=56.65027997090674, CurrSamplesPerSec=52.848907526627684, MemAllocated=17.11GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:36,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=23, lr=[0.007853419523977077], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:36,488] [INFO] [timer.py:260:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=56.575463524523855, CurrSamplesPerSec=39.59930559899168, MemAllocated=17.46GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:40,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=23, lr=[0.00791582782003368], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:40,003] [INFO] [timer.py:260:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=56.83056733144748, CurrSamplesPerSec=46.62231809450367, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:43,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=23, lr=[0.007975656510865552], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:43,549] [INFO] [timer.py:260:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=57.04340565288194, CurrSamplesPerSec=46.64010686193155, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:47,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=23, lr=[0.008033110411104317], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:47,443] [INFO] [timer.py:260:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=57.00164882939456, CurrSamplesPerSec=69.7192213672097, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:51,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=23, lr=[0.008088370871215252], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:51,297] [INFO] [timer.py:260:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=56.98474051574166, CurrSamplesPerSec=68.91435108405105, MemAllocated=16.79GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:55,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=23, lr=[0.00814159923021483], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:55,144] [INFO] [timer.py:260:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=56.98876100035415, CurrSamplesPerSec=57.58069799867007, MemAllocated=16.99GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:46:59,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=23, lr=[0.008192939655779975], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:46:59,273] [INFO] [timer.py:260:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=56.79403967652018, CurrSamplesPerSec=91.4889239406205, MemAllocated=16.6GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:03,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=23, lr=[0.008242521497724042], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:03,072] [INFO] [timer.py:260:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=56.817039265104036, CurrSamplesPerSec=41.075122658231564, MemAllocated=17.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:07,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=23, lr=[0.008290461251590622], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:07,333] [INFO] [timer.py:260:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=56.5581313432164, CurrSamplesPerSec=33.16417259894195, MemAllocated=17.77GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:11,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=23, lr=[0.008336864207392505], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:11,796] [INFO] [timer.py:260:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=56.22076200872941, CurrSamplesPerSec=46.750316620154734, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:15,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=23, lr=[0.008381825842200954], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:15,898] [INFO] [timer.py:260:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=56.09858697232997, CurrSamplesPerSec=92.61683775286853, MemAllocated=16.59GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 3, 'loss_train': 6.5242984441067655, 'loss_val': 0.4312896405919662, 'accuracy_train': 0.8282241014799154, 'accuracy_val': 0.7420718816067653}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:47:26,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=23, lr=[0.008425433002904463], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:26,778] [INFO] [timer.py:260:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=56.50917825091995, CurrSamplesPerSec=69.50562651926377, MemAllocated=16.84GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:30,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=23, lr=[0.00846776491596958], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:30,958] [INFO] [timer.py:260:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=56.33392499013452, CurrSamplesPerSec=55.27457705296104, MemAllocated=17.02GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:34,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=23, lr=[0.00850889405370731], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:34,684] [INFO] [timer.py:260:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=56.412390625281034, CurrSamplesPerSec=45.74340182349605, MemAllocated=17.23GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:38,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=23, lr=[0.008548886880840299], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:38,382] [INFO] [timer.py:260:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=56.49789698828183, CurrSamplesPerSec=63.06015012159323, MemAllocated=16.91GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:42,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=23, lr=[0.008587804500685978], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:42,821] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=56.217041226706996, CurrSamplesPerSec=72.01016378807128, MemAllocated=16.76GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:46,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=23, lr=[0.008625703216729706], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:46,689] [INFO] [timer.py:260:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=56.2178641273864, CurrSamplesPerSec=57.74231057473974, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:50,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=23, lr=[0.008662635022543717], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:50,165] [INFO] [timer.py:260:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=56.3936349014647, CurrSamplesPerSec=41.255161746823894, MemAllocated=17.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:54,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=23, lr=[0.008698648030750733], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:54,363] [INFO] [timer.py:260:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=56.23560966969741, CurrSamplesPerSec=81.73452090531862, MemAllocated=16.73GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:47:58,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=23, lr=[0.008733786849912526], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:47:58,786] [INFO] [timer.py:260:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=55.99881185370066, CurrSamplesPerSec=41.17719563272584, MemAllocated=17.43GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:02,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=23, lr=[0.00876809291675008], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:02,537] [INFO] [timer.py:260:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=56.04934373847003, CurrSamplesPerSec=46.76575888501742, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:05,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=23, lr=[0.008801604789901406], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:05,983] [INFO] [timer.py:260:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=56.22482854467963, CurrSamplesPerSec=53.105984277640346, MemAllocated=17.1GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:09,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=23, lr=[0.008834358410439789], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:09,268] [INFO] [timer.py:260:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=56.47125527197847, CurrSamplesPerSec=61.36060273571794, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 4, 'loss_train': 5.635289373092873, 'loss_val': 0.6395348837209303, 'accuracy_train': 0.8768498942917548, 'accuracy_val': 0.7484143763213531}\n",
      "[2024-05-10 11:48:20,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=23, lr=[0.008866387333566168], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:20,788] [INFO] [timer.py:260:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=56.416097103869504, CurrSamplesPerSec=57.42244416169599, MemAllocated=17.0GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:24,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=23, lr=[0.008897722935220375], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:24,544] [INFO] [timer.py:260:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=56.46163394107529, CurrSamplesPerSec=84.51359343122687, MemAllocated=16.66GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:28,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=23, lr=[0.008928394596800381], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:28,775] [INFO] [timer.py:260:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=56.32422593124141, CurrSamplesPerSec=62.764435316277876, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:33,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=23, lr=[0.008958429870715448], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:33,255] [INFO] [timer.py:260:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=56.102723481697595, CurrSamplesPerSec=72.87589820440564, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:36,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=23, lr=[0.008987854629111107], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:36,963] [INFO] [timer.py:260:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=56.160992912327295, CurrSamplesPerSec=73.8950124702009, MemAllocated=16.75GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:40,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=23, lr=[0.009016693197777788], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:40,802] [INFO] [timer.py:260:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=56.17011825619302, CurrSamplesPerSec=56.253652861055066, MemAllocated=17.02GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:44,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=23, lr=[0.009044968476979809], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:44,566] [INFO] [timer.py:260:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=56.20507111480391, CurrSamplesPerSec=52.37691411945042, MemAllocated=17.15GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:48,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=23, lr=[0.00907270205070849], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:48,549] [INFO] [timer.py:260:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=56.162647993701206, CurrSamplesPerSec=51.64977611124708, MemAllocated=17.17GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:50,421] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
      "[2024-05-10 11:48:52,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=24, lr=[0.009097215965642566], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:52,508] [INFO] [timer.py:260:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=56.13824639791635, CurrSamplesPerSec=68.92687937027024, MemAllocated=16.81GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:56,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=24, lr=[0.009123975475682458], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:48:56,013] [INFO] [timer.py:260:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=56.256591609267446, CurrSamplesPerSec=82.66878218236468, MemAllocated=16.68GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:48:59,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=24, lr=[0.009150249305273526], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:48:59,883] [INFO] [timer.py:260:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=56.25051074713383, CurrSamplesPerSec=57.80652274657557, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:03,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=24, lr=[0.009176054770627572], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:03,218] [INFO] [timer.py:260:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=56.42504431262859, CurrSamplesPerSec=83.15998934304851, MemAllocated=16.65GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 5, 'loss_train': 8.407128493327168, 'loss_val': 1.5412262156448202, 'accuracy_train': 0.888477801268499, 'accuracy_val': 0.7674418604651163}\n",
      "[2024-05-10 11:49:14,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=24, lr=[0.009201408278077375], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:14,388] [INFO] [timer.py:260:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=56.5118256975594, CurrSamplesPerSec=80.18542251897132, MemAllocated=16.73GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:18,031] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=24, lr=[0.009226325386726969], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:18,032] [INFO] [timer.py:260:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=56.579765981449434, CurrSamplesPerSec=33.1848194806248, MemAllocated=17.77GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:22,007] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=24, lr=[0.009250820865800789], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:22,008] [INFO] [timer.py:260:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=56.54988803127671, CurrSamplesPerSec=63.79666856161792, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:25,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=24, lr=[0.009274908747220954], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:25,923] [INFO] [timer.py:260:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=56.529259288887125, CurrSamplesPerSec=57.01348602886829, MemAllocated=16.99GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:30,177] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=24, lr=[0.009298602373881418], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:30,178] [INFO] [timer.py:260:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=56.399267179043015, CurrSamplesPerSec=57.322972713256426, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:33,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=24, lr=[0.009321914444034766], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:33,386] [INFO] [timer.py:260:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=56.60302695056175, CurrSamplesPerSec=73.23440973332445, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:37,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=24, lr=[0.009344857052161381], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:37,445] [INFO] [timer.py:260:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=56.54519810943031, CurrSamplesPerSec=63.42189552551945, MemAllocated=16.9GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:40,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=24, lr=[0.00936744172665028], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:40,919] [INFO] [timer.py:260:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=56.66209887181784, CurrSamplesPerSec=63.94182538850723, MemAllocated=16.91GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:44,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=24, lr=[0.009389679464585535], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:44,768] [INFO] [timer.py:260:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=56.66229203134893, CurrSamplesPerSec=56.091205124604656, MemAllocated=17.02GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:48,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=24, lr=[0.009411580763901004], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:48,624] [INFO] [timer.py:260:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=56.660942270413436, CurrSamplesPerSec=52.52535637777884, MemAllocated=17.11GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:53,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=24, lr=[0.009433155653138788], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:53,006] [INFO] [timer.py:260:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=56.50856706219622, CurrSamplesPerSec=46.38276972925302, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:49:57,407] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=24, lr=[0.009454413719022505], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:49:57,408] [INFO] [timer.py:260:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=56.358147653867476, CurrSamplesPerSec=68.83652305565072, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 6, 'loss_train': 7.0049982978224, 'loss_val': 1.0591966173361522, 'accuracy_train': 0.9149048625792812, 'accuracy_val': 0.7632135306553911}\n",
      "[2024-05-10 11:50:08,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=24, lr=[0.009475364132035207], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:08,824] [INFO] [timer.py:260:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=56.335602498111605, CurrSamplesPerSec=26.519016483876907, MemAllocated=18.19GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:12,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=24, lr=[0.00949601567017268], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:12,544] [INFO] [timer.py:260:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=56.38125165234267, CurrSamplesPerSec=55.62299387896344, MemAllocated=17.05GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:16,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=24, lr=[0.009516376741026187], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:16,014] [INFO] [timer.py:260:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=56.48347534215417, CurrSamplesPerSec=73.7817601716876, MemAllocated=16.75GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:19,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=24, lr=[0.009536455402333647], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:19,545] [INFO] [timer.py:260:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=56.56589321401078, CurrSamplesPerSec=63.92732870500911, MemAllocated=16.91GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:23,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=24, lr=[0.009556259381124997], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:23,379] [INFO] [timer.py:260:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=56.57212263557578, CurrSamplesPerSec=68.62562762169469, MemAllocated=16.84GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:27,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=24, lr=[0.009575796091575562], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:27,403] [INFO] [timer.py:260:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=56.527493961160985, CurrSamplesPerSec=52.06483426794564, MemAllocated=17.16GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:31,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=24, lr=[0.00959507265167069], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:31,519] [INFO] [timer.py:260:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=56.46157753016979, CurrSamplesPerSec=33.23825311722242, MemAllocated=17.77GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:35,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=24, lr=[0.009614095898775347], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:35,381] [INFO] [timer.py:260:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=56.45911070103052, CurrSamplesPerSec=64.41403829393494, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:38,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=24, lr=[0.009632872404193962], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:38,886] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=56.54462271420187, CurrSamplesPerSec=95.50633588742272, MemAllocated=16.55GB, MaxMemAllocated=21.42GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:50:43,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=24, lr=[0.009651408486798027], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:43,077] [INFO] [timer.py:260:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=56.46499709017206, CurrSamplesPerSec=63.26870658511015, MemAllocated=16.94GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:46,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=24, lr=[0.00966971022579223], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:46,669] [INFO] [timer.py:260:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=56.53369289998925, CurrSamplesPerSec=40.780168021268516, MemAllocated=17.39GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:50:50,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=24, lr=[0.009687783472683635], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:50:50,357] [INFO] [timer.py:260:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=56.57429214656596, CurrSamplesPerSec=81.63946400004866, MemAllocated=16.73GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 7, 'loss_train': 6.036147780952696, 'loss_val': 4.293868921775899, 'accuracy_train': 0.9265327695560254, 'accuracy_val': 0.7272727272727273}\n",
      "[2024-05-10 11:51:02,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=24, lr=[0.00970563386251287], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:02,132] [INFO] [timer.py:260:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=56.51896864754559, CurrSamplesPerSec=57.66674228519183, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:06,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=24, lr=[0.009723266824401274], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:06,110] [INFO] [timer.py:260:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=56.490542503143736, CurrSamplesPerSec=37.97500771000207, MemAllocated=17.53GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:10,177] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=24, lr=[0.009740687591463388], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:10,177] [INFO] [timer.py:260:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=56.43887442784353, CurrSamplesPerSec=23.86991724172764, MemAllocated=18.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:14,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=24, lr=[0.00975790121013008], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:14,086] [INFO] [timer.py:260:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=56.431428655108306, CurrSamplesPerSec=69.56730291781258, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:18,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=24, lr=[0.009774912548923844], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:18,135] [INFO] [timer.py:260:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=56.38359919985477, CurrSamplesPerSec=21.70245881281563, MemAllocated=18.64GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:21,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=24, lr=[0.009791726306724488], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:21,766] [INFO] [timer.py:260:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=56.43829924400086, CurrSamplesPerSec=82.74298565320103, MemAllocated=16.65GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:25,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=24, lr=[0.009808347020560269], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:25,302] [INFO] [timer.py:260:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=56.50675814434394, CurrSamplesPerSec=64.18639332549681, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:29,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=24, lr=[0.009824779072956837], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:29,229] [INFO] [timer.py:260:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=56.4876927783464, CurrSamplesPerSec=46.462120026752, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:32,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=24, lr=[0.009841026698873752], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:32,671] [INFO] [timer.py:260:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=56.57281061537173, CurrSamplesPerSec=52.385704572722595, MemAllocated=17.17GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:36,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=24, lr=[0.009857093992256044], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:36,639] [INFO] [timer.py:260:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=56.543459872965705, CurrSamplesPerSec=46.54677818846666, MemAllocated=17.18GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:40,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=24, lr=[0.00987298491222617], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:40,620] [INFO] [timer.py:260:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=56.514809627406905, CurrSamplesPerSec=73.72283436194779, MemAllocated=16.76GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:44,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=24, lr=[0.009888703288939782], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:44,535] [INFO] [timer.py:260:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=56.503968221613896, CurrSamplesPerSec=64.48323561233687, MemAllocated=16.85GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 8, 'loss_train': 4.804948157538319, 'loss_val': 2.321353065539112, 'accuracy_train': 0.9455602536997886, 'accuracy_val': 0.7484143763213531}\n",
      "[2024-05-10 11:51:56,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=24, lr=[0.009904252829127018], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:56,434] [INFO] [timer.py:260:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=56.40760240193975, CurrSamplesPerSec=57.4352218837805, MemAllocated=16.96GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:51:59,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=24, lr=[0.00991963712133931], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:51:59,706] [INFO] [timer.py:260:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=56.52717906330962, CurrSamplesPerSec=55.92778185212347, MemAllocated=17.05GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:03,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=24, lr=[0.009934859640920333], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:03,265] [INFO] [timer.py:260:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=56.58484450345045, CurrSamplesPerSec=57.65148396452368, MemAllocated=16.97GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:07,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=24, lr=[0.009949923754718312], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:07,870] [INFO] [timer.py:260:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=56.437235506840764, CurrSamplesPerSec=63.77872181894558, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:11,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=24, lr=[0.00996483272555564], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:11,780] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=56.42521249691397, CurrSamplesPerSec=46.64535860051824, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:15,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=24, lr=[0.009979589716470704], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:15,486] [INFO] [timer.py:260:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=56.45355562191152, CurrSamplesPerSec=56.37571678430358, MemAllocated=17.03GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:19,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=24, lr=[0.009994197794745663], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:19,161] [INFO] [timer.py:260:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=56.48641041373144, CurrSamplesPerSec=39.44452516475172, MemAllocated=17.49GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:23,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:52:23,467] [INFO] [timer.py:260:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=56.41140765386191, CurrSamplesPerSec=79.41642436614302, MemAllocated=16.71GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:27,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:27,673] [INFO] [timer.py:260:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=56.34963107195065, CurrSamplesPerSec=72.69992438473221, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:31,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:31,153] [INFO] [timer.py:260:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=56.41769384367964, CurrSamplesPerSec=57.73053808765968, MemAllocated=16.99GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:35,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:35,126] [INFO] [timer.py:260:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=56.39439677208213, CurrSamplesPerSec=35.550183766465615, MemAllocated=17.61GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:38,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:38,333] [INFO] [timer.py:260:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=56.51465101549098, CurrSamplesPerSec=64.27067116150782, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 9, 'loss_train': 5.147917217293451, 'loss_val': 1.4862579281183932, 'accuracy_train': 0.9497885835095138, 'accuracy_val': 0.7357293868921776}\n",
      "[2024-05-10 11:52:49,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:49,947] [INFO] [timer.py:260:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=56.478095107347414, CurrSamplesPerSec=23.925462242336128, MemAllocated=18.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:53,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:53,784] [INFO] [timer.py:260:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=56.48061856587091, CurrSamplesPerSec=39.557896623109606, MemAllocated=17.48GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:52:57,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:52:57,531] [INFO] [timer.py:260:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=56.502446094632994, CurrSamplesPerSec=62.99805210593164, MemAllocated=16.94GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:01,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:01,451] [INFO] [timer.py:260:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=56.494195507724505, CurrSamplesPerSec=69.62200968354699, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:05,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:05,256] [INFO] [timer.py:260:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=56.49975049963743, CurrSamplesPerSec=81.82959986830954, MemAllocated=16.69GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:08,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:08,554] [INFO] [timer.py:260:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=56.597480140006596, CurrSamplesPerSec=92.19656293748085, MemAllocated=16.6GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:12,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:12,221] [INFO] [timer.py:260:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=56.62638235902929, CurrSamplesPerSec=56.16970216413113, MemAllocated=17.02GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:15,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:15,871] [INFO] [timer.py:260:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=56.65886905818699, CurrSamplesPerSec=64.0144075395387, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:20,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:20,332] [INFO] [timer.py:260:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=56.55352281554367, CurrSamplesPerSec=39.318574312836084, MemAllocated=17.46GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:23,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:23,933] [INFO] [timer.py:260:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=56.596531331925966, CurrSamplesPerSec=46.39745684258268, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:28,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:28,650] [INFO] [timer.py:260:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=56.45302871547212, CurrSamplesPerSec=39.61379818402695, MemAllocated=17.46GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:32,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:32,245] [INFO] [timer.py:260:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=56.49782657004109, CurrSamplesPerSec=110.64867502934852, MemAllocated=16.41GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 10, 'loss_train': 3.3332328554439745, 'loss_val': 0.8604651162790697, 'accuracy_train': 0.9698731501057083, 'accuracy_val': 0.758985200845666}\n",
      "[2024-05-10 11:53:43,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:43,437] [INFO] [timer.py:260:stop] epoch=0/micro_step=1200/global_step=1200, RunningAvgSamplesPerSec=56.52519294102874, CurrSamplesPerSec=39.37955408985685, MemAllocated=17.49GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:46,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:46,734] [INFO] [timer.py:260:stop] epoch=0/micro_step=1210/global_step=1210, RunningAvgSamplesPerSec=56.61700568383561, CurrSamplesPerSec=81.3530510638143, MemAllocated=16.72GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:50,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:50,515] [INFO] [timer.py:260:stop] epoch=0/micro_step=1220/global_step=1220, RunningAvgSamplesPerSec=56.6283066428574, CurrSamplesPerSec=63.82457882292969, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:54,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:54,364] [INFO] [timer.py:260:stop] epoch=0/micro_step=1230/global_step=1230, RunningAvgSamplesPerSec=56.62804262215353, CurrSamplesPerSec=46.58345041287893, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:53:58,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:53:58,487] [INFO] [timer.py:260:stop] epoch=0/micro_step=1240/global_step=1240, RunningAvgSamplesPerSec=56.587900753502915, CurrSamplesPerSec=41.2521185789508, MemAllocated=17.39GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:02,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:02,321] [INFO] [timer.py:260:stop] epoch=0/micro_step=1250/global_step=1250, RunningAvgSamplesPerSec=56.59178293933785, CurrSamplesPerSec=57.7067593980372, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:06,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:06,188] [INFO] [timer.py:260:stop] epoch=0/micro_step=1260/global_step=1260, RunningAvgSamplesPerSec=56.58975263894409, CurrSamplesPerSec=73.55097307907712, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:54:10,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:10,675] [INFO] [timer.py:260:stop] epoch=0/micro_step=1270/global_step=1270, RunningAvgSamplesPerSec=56.4919090676902, CurrSamplesPerSec=73.46763472031249, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:14,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:14,917] [INFO] [timer.py:260:stop] epoch=0/micro_step=1280/global_step=1280, RunningAvgSamplesPerSec=56.43378190080982, CurrSamplesPerSec=52.26258049593831, MemAllocated=17.15GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:18,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:18,879] [INFO] [timer.py:260:stop] epoch=0/micro_step=1290/global_step=1290, RunningAvgSamplesPerSec=56.41665007919103, CurrSamplesPerSec=46.58461453025865, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:22,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:22,804] [INFO] [timer.py:260:stop] epoch=0/micro_step=1300/global_step=1300, RunningAvgSamplesPerSec=56.409182040840115, CurrSamplesPerSec=98.85232147065756, MemAllocated=16.53GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 11, 'loss_train': 3.97688972351415, 'loss_val': 0.0, 'accuracy_train': 0.9651162790697675, 'accuracy_val': 0.7547568710359408}\n",
      "[2024-05-10 11:54:34,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:34,169] [INFO] [timer.py:260:stop] epoch=0/micro_step=1310/global_step=1310, RunningAvgSamplesPerSec=56.43510336058075, CurrSamplesPerSec=70.32621815434304, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:38,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:38,066] [INFO] [timer.py:260:stop] epoch=0/micro_step=1320/global_step=1320, RunningAvgSamplesPerSec=56.42957789961249, CurrSamplesPerSec=42.743546481447865, MemAllocated=17.31GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:41,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:41,950] [INFO] [timer.py:260:stop] epoch=0/micro_step=1330/global_step=1330, RunningAvgSamplesPerSec=56.42525608786827, CurrSamplesPerSec=82.28512013144262, MemAllocated=16.69GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:46,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:46,213] [INFO] [timer.py:260:stop] epoch=0/micro_step=1340/global_step=1340, RunningAvgSamplesPerSec=56.36789747921265, CurrSamplesPerSec=52.28187974886199, MemAllocated=17.12GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:50,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:50,322] [INFO] [timer.py:260:stop] epoch=0/micro_step=1350/global_step=1350, RunningAvgSamplesPerSec=56.33131161774281, CurrSamplesPerSec=63.07739824590921, MemAllocated=16.95GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:53,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:53,798] [INFO] [timer.py:260:stop] epoch=0/micro_step=1360/global_step=1360, RunningAvgSamplesPerSec=56.38849421083373, CurrSamplesPerSec=57.519007478197516, MemAllocated=17.0GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:54:57,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:54:57,342] [INFO] [timer.py:260:stop] epoch=0/micro_step=1370/global_step=1370, RunningAvgSamplesPerSec=56.43132165447839, CurrSamplesPerSec=69.53075941183296, MemAllocated=16.81GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:00,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:00,769] [INFO] [timer.py:260:stop] epoch=0/micro_step=1380/global_step=1380, RunningAvgSamplesPerSec=56.49065218337699, CurrSamplesPerSec=80.20468496032126, MemAllocated=16.69GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:04,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:04,851] [INFO] [timer.py:260:stop] epoch=0/micro_step=1390/global_step=1390, RunningAvgSamplesPerSec=56.46085823109179, CurrSamplesPerSec=83.09285483453581, MemAllocated=16.67GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:08,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:08,731] [INFO] [timer.py:260:stop] epoch=0/micro_step=1400/global_step=1400, RunningAvgSamplesPerSec=56.45820244821686, CurrSamplesPerSec=33.254987465368096, MemAllocated=17.73GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:12,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:12,988] [INFO] [timer.py:260:stop] epoch=0/micro_step=1410/global_step=1410, RunningAvgSamplesPerSec=56.40819253729727, CurrSamplesPerSec=63.66503145358142, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:17,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:17,097] [INFO] [timer.py:260:stop] epoch=0/micro_step=1420/global_step=1420, RunningAvgSamplesPerSec=56.37118442668736, CurrSamplesPerSec=52.47241769721081, MemAllocated=17.13GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 12, 'loss_train': 6.4252940010570825, 'loss_val': 0.3583509513742072, 'accuracy_train': 0.9614164904862579, 'accuracy_val': 0.7378435517970402}\n",
      "[2024-05-10 11:55:28,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:28,300] [INFO] [timer.py:260:stop] epoch=0/micro_step=1430/global_step=1430, RunningAvgSamplesPerSec=56.406233508192884, CurrSamplesPerSec=33.19310841831246, MemAllocated=17.77GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:32,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:32,734] [INFO] [timer.py:260:stop] epoch=0/micro_step=1440/global_step=1440, RunningAvgSamplesPerSec=56.334056882488355, CurrSamplesPerSec=43.54260480151153, MemAllocated=17.31GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:36,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:36,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=1450/global_step=1450, RunningAvgSamplesPerSec=56.30443607128695, CurrSamplesPerSec=73.49265663312296, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:40,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:40,820] [INFO] [timer.py:260:stop] epoch=0/micro_step=1460/global_step=1460, RunningAvgSamplesPerSec=56.28599800495904, CurrSamplesPerSec=46.37645520598516, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:44,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:44,770] [INFO] [timer.py:260:stop] epoch=0/micro_step=1470/global_step=1470, RunningAvgSamplesPerSec=56.274603543046325, CurrSamplesPerSec=73.16885490057545, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:48,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:48,354] [INFO] [timer.py:260:stop] epoch=0/micro_step=1480/global_step=1480, RunningAvgSamplesPerSec=56.31442038034982, CurrSamplesPerSec=63.0083439819957, MemAllocated=16.9GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:52,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:52,117] [INFO] [timer.py:260:stop] epoch=0/micro_step=1490/global_step=1490, RunningAvgSamplesPerSec=56.32898027755665, CurrSamplesPerSec=81.83528769047666, MemAllocated=16.7GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:55,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:55:55,608] [INFO] [timer.py:260:stop] epoch=0/micro_step=1500/global_step=1500, RunningAvgSamplesPerSec=56.37900309664275, CurrSamplesPerSec=51.491730952494216, MemAllocated=17.12GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:55:59,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:55:59,239] [INFO] [timer.py:260:stop] epoch=0/micro_step=1510/global_step=1510, RunningAvgSamplesPerSec=56.409777616379195, CurrSamplesPerSec=73.42904911546643, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:03,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:03,003] [INFO] [timer.py:260:stop] epoch=0/micro_step=1520/global_step=1520, RunningAvgSamplesPerSec=56.42186169069265, CurrSamplesPerSec=63.64269206221555, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:07,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:07,039] [INFO] [timer.py:260:stop] epoch=0/micro_step=1530/global_step=1530, RunningAvgSamplesPerSec=56.399097183187784, CurrSamplesPerSec=53.11199452330169, MemAllocated=17.08GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:11,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:11,358] [INFO] [timer.py:260:stop] epoch=0/micro_step=1540/global_step=1540, RunningAvgSamplesPerSec=56.34048759399298, CurrSamplesPerSec=52.862270619076426, MemAllocated=17.07GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 13, 'loss_train': 6.000933205602537, 'loss_val': 4.494714587737843, 'accuracy_train': 0.9630021141649049, 'accuracy_val': 0.7547568710359408}\n",
      "[2024-05-10 11:56:23,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:23,061] [INFO] [timer.py:260:stop] epoch=0/micro_step=1550/global_step=1550, RunningAvgSamplesPerSec=56.31074118771124, CurrSamplesPerSec=21.665653691698115, MemAllocated=18.64GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:26,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:26,565] [INFO] [timer.py:260:stop] epoch=0/micro_step=1560/global_step=1560, RunningAvgSamplesPerSec=56.35609517716267, CurrSamplesPerSec=63.23031639248497, MemAllocated=16.91GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:30,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:30,016] [INFO] [timer.py:260:stop] epoch=0/micro_step=1570/global_step=1570, RunningAvgSamplesPerSec=56.40623569587624, CurrSamplesPerSec=73.39467778414534, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:34,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:34,349] [INFO] [timer.py:260:stop] epoch=0/micro_step=1580/global_step=1580, RunningAvgSamplesPerSec=56.349097288075505, CurrSamplesPerSec=97.41777414545703, MemAllocated=16.56GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:38,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:38,528] [INFO] [timer.py:260:stop] epoch=0/micro_step=1590/global_step=1590, RunningAvgSamplesPerSec=56.311989810805215, CurrSamplesPerSec=64.76733069344006, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:42,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:42,524] [INFO] [timer.py:260:stop] epoch=0/micro_step=1600/global_step=1600, RunningAvgSamplesPerSec=56.29678067850804, CurrSamplesPerSec=39.58449672394551, MemAllocated=17.48GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:45,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:45,820] [INFO] [timer.py:260:stop] epoch=0/micro_step=1610/global_step=1610, RunningAvgSamplesPerSec=56.366804828569784, CurrSamplesPerSec=56.95919171068892, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:49,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:49,835] [INFO] [timer.py:260:stop] epoch=0/micro_step=1620/global_step=1620, RunningAvgSamplesPerSec=56.34639197410104, CurrSamplesPerSec=39.41151521664955, MemAllocated=17.49GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:53,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=24, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:53,777] [INFO] [timer.py:260:stop] epoch=0/micro_step=1630/global_step=1630, RunningAvgSamplesPerSec=56.336604505983765, CurrSamplesPerSec=72.74381840511546, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:56:56,435] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 512, reducing to 256\n",
      "[2024-05-10 11:56:57,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:56:57,086] [INFO] [timer.py:260:stop] epoch=0/micro_step=1640/global_step=1640, RunningAvgSamplesPerSec=56.402191491232095, CurrSamplesPerSec=51.463615964957214, MemAllocated=17.09GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:00,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:00,821] [INFO] [timer.py:260:stop] epoch=0/micro_step=1650/global_step=1650, RunningAvgSamplesPerSec=56.41753859324941, CurrSamplesPerSec=63.92343155857318, MemAllocated=16.9GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:04,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:04,736] [INFO] [timer.py:260:stop] epoch=0/micro_step=1660/global_step=1660, RunningAvgSamplesPerSec=56.40981195797606, CurrSamplesPerSec=68.94932842361146, MemAllocated=16.83GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 14, 'loss_train': 8.446799022198732, 'loss_val': 5.437632135306554, 'accuracy_train': 0.9550739957716702, 'accuracy_val': 0.7357293868921776}\n",
      "[2024-05-10 11:57:16,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:16,596] [INFO] [timer.py:260:stop] epoch=0/micro_step=1670/global_step=1670, RunningAvgSamplesPerSec=56.34839781401878, CurrSamplesPerSec=57.31651018450746, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:20,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:20,666] [INFO] [timer.py:260:stop] epoch=0/micro_step=1680/global_step=1680, RunningAvgSamplesPerSec=56.324179341140095, CurrSamplesPerSec=64.26667049725873, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:24,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:24,271] [INFO] [timer.py:260:stop] epoch=0/micro_step=1690/global_step=1690, RunningAvgSamplesPerSec=56.35340255160802, CurrSamplesPerSec=98.47792316801696, MemAllocated=16.57GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:29,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:29,122] [INFO] [timer.py:260:stop] epoch=0/micro_step=1700/global_step=1700, RunningAvgSamplesPerSec=56.239594082530786, CurrSamplesPerSec=39.60133859629142, MemAllocated=17.46GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:32,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:32,714] [INFO] [timer.py:260:stop] epoch=0/micro_step=1710/global_step=1710, RunningAvgSamplesPerSec=56.270934294297504, CurrSamplesPerSec=69.40089805930666, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:37,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:37,240] [INFO] [timer.py:260:stop] epoch=0/micro_step=1720/global_step=1720, RunningAvgSamplesPerSec=56.195368119623865, CurrSamplesPerSec=41.22044558882785, MemAllocated=17.39GB, MaxMemAllocated=21.42GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:57:41,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:41,191] [INFO] [timer.py:260:stop] epoch=0/micro_step=1730/global_step=1730, RunningAvgSamplesPerSec=56.18463485731841, CurrSamplesPerSec=52.27560796447286, MemAllocated=17.16GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:45,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:45,431] [INFO] [timer.py:260:stop] epoch=0/micro_step=1740/global_step=1740, RunningAvgSamplesPerSec=56.14430264624275, CurrSamplesPerSec=39.50022278479224, MemAllocated=17.47GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:48,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:48,891] [INFO] [timer.py:260:stop] epoch=0/micro_step=1750/global_step=1750, RunningAvgSamplesPerSec=56.18832387487105, CurrSamplesPerSec=80.52767338161955, MemAllocated=16.73GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:52,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:52,579] [INFO] [timer.py:260:stop] epoch=0/micro_step=1760/global_step=1760, RunningAvgSamplesPerSec=56.206215087266, CurrSamplesPerSec=68.77789723213215, MemAllocated=16.84GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:56,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:56,281] [INFO] [timer.py:260:stop] epoch=0/micro_step=1770/global_step=1770, RunningAvgSamplesPerSec=56.22258460454851, CurrSamplesPerSec=46.82733601607131, MemAllocated=17.18GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:57:59,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:57:59,746] [INFO] [timer.py:260:stop] epoch=0/micro_step=1780/global_step=1780, RunningAvgSamplesPerSec=56.26763165174389, CurrSamplesPerSec=57.293609319440115, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 15, 'loss_train': 5.439845401691332, 'loss_val': 3.7758985200845667, 'accuracy_train': 0.9704016913319239, 'accuracy_val': 0.7441860465116279}\n",
      "[2024-05-10 11:58:11,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:11,090] [INFO] [timer.py:260:stop] epoch=0/micro_step=1790/global_step=1790, RunningAvgSamplesPerSec=56.293990884389004, CurrSamplesPerSec=43.186542508349795, MemAllocated=17.33GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:14,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:14,883] [INFO] [timer.py:260:stop] epoch=0/micro_step=1800/global_step=1800, RunningAvgSamplesPerSec=56.303207919137016, CurrSamplesPerSec=56.3695134130802, MemAllocated=16.97GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:19,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:19,644] [INFO] [timer.py:260:stop] epoch=0/micro_step=1810/global_step=1810, RunningAvgSamplesPerSec=56.2088775973491, CurrSamplesPerSec=52.11379652832838, MemAllocated=17.16GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:23,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:23,553] [INFO] [timer.py:260:stop] epoch=0/micro_step=1820/global_step=1820, RunningAvgSamplesPerSec=56.20469919432107, CurrSamplesPerSec=57.230823810336005, MemAllocated=16.97GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:27,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:27,542] [INFO] [timer.py:260:stop] epoch=0/micro_step=1830/global_step=1830, RunningAvgSamplesPerSec=56.19170608993597, CurrSamplesPerSec=81.3371762395918, MemAllocated=16.73GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:31,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:31,027] [INFO] [timer.py:260:stop] epoch=0/micro_step=1840/global_step=1840, RunningAvgSamplesPerSec=56.23260117747367, CurrSamplesPerSec=55.851074595861405, MemAllocated=17.02GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:34,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:34,550] [INFO] [timer.py:260:stop] epoch=0/micro_step=1850/global_step=1850, RunningAvgSamplesPerSec=56.270158604881495, CurrSamplesPerSec=55.99227058299361, MemAllocated=17.05GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:38,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:38,318] [INFO] [timer.py:260:stop] epoch=0/micro_step=1860/global_step=1860, RunningAvgSamplesPerSec=56.27754461592344, CurrSamplesPerSec=71.67583305474948, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:42,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:42,112] [INFO] [timer.py:260:stop] epoch=0/micro_step=1870/global_step=1870, RunningAvgSamplesPerSec=56.28418370046401, CurrSamplesPerSec=66.48918479768557, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:45,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:45,941] [INFO] [timer.py:260:stop] epoch=0/micro_step=1880/global_step=1880, RunningAvgSamplesPerSec=56.28770711359248, CurrSamplesPerSec=63.56613272484615, MemAllocated=16.9GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:50,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:50,326] [INFO] [timer.py:260:stop] epoch=0/micro_step=1890/global_step=1890, RunningAvgSamplesPerSec=56.23390143526049, CurrSamplesPerSec=64.16227565291551, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:58:53,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:58:53,646] [INFO] [timer.py:260:stop] epoch=0/micro_step=1900/global_step=1900, RunningAvgSamplesPerSec=56.2887579482507, CurrSamplesPerSec=63.351547093435435, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 16, 'loss_train': 2.358520251562933, 'loss_val': 5.158562367864693, 'accuracy_train': 0.9830866807610994, 'accuracy_val': 0.7547568710359408}\n",
      "[2024-05-10 11:59:04,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:04,864] [INFO] [timer.py:260:stop] epoch=0/micro_step=1910/global_step=1910, RunningAvgSamplesPerSec=56.30164891390705, CurrSamplesPerSec=45.85808830513536, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:09,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:09,005] [INFO] [timer.py:260:stop] epoch=0/micro_step=1920/global_step=1920, RunningAvgSamplesPerSec=56.276581001082334, CurrSamplesPerSec=63.89458310244824, MemAllocated=16.87GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:12,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:12,605] [INFO] [timer.py:260:stop] epoch=0/micro_step=1930/global_step=1930, RunningAvgSamplesPerSec=56.301779210451414, CurrSamplesPerSec=57.269553774640364, MemAllocated=16.99GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:16,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:16,796] [INFO] [timer.py:260:stop] epoch=0/micro_step=1940/global_step=1940, RunningAvgSamplesPerSec=56.270027940284166, CurrSamplesPerSec=63.40284113025437, MemAllocated=16.95GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:20,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:20,564] [INFO] [timer.py:260:stop] epoch=0/micro_step=1950/global_step=1950, RunningAvgSamplesPerSec=56.27894490264416, CurrSamplesPerSec=41.13834819778373, MemAllocated=17.39GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:24,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 11:59:24,601] [INFO] [timer.py:260:stop] epoch=0/micro_step=1960/global_step=1960, RunningAvgSamplesPerSec=56.26464297791168, CurrSamplesPerSec=57.65737826750006, MemAllocated=16.97GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:28,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:28,742] [INFO] [timer.py:260:stop] epoch=0/micro_step=1970/global_step=1970, RunningAvgSamplesPerSec=56.23633250183809, CurrSamplesPerSec=69.06108268534946, MemAllocated=16.79GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:32,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:32,477] [INFO] [timer.py:260:stop] epoch=0/micro_step=1980/global_step=1980, RunningAvgSamplesPerSec=56.2487487854419, CurrSamplesPerSec=73.06457685302024, MemAllocated=16.76GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:36,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:36,162] [INFO] [timer.py:260:stop] epoch=0/micro_step=1990/global_step=1990, RunningAvgSamplesPerSec=56.266019880805594, CurrSamplesPerSec=57.676158914215684, MemAllocated=17.0GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:41,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:41,182] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=56.153701479186346, CurrSamplesPerSec=52.45502751375688, MemAllocated=17.08GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:44,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:44,913] [INFO] [timer.py:260:stop] epoch=0/micro_step=2010/global_step=2010, RunningAvgSamplesPerSec=56.167740698726476, CurrSamplesPerSec=52.327701245952156, MemAllocated=17.17GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 11:59:48,440] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 11:59:48,441] [INFO] [timer.py:260:stop] epoch=0/micro_step=2020/global_step=2020, RunningAvgSamplesPerSec=56.19912617133018, CurrSamplesPerSec=57.246885745702144, MemAllocated=16.98GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 17, 'loss_train': 4.542869648520084, 'loss_val': 5.59830866807611, 'accuracy_train': 0.9783298097251586, 'accuracy_val': 0.7251585623678647}\n",
      "[2024-05-10 12:00:00,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:00,665] [INFO] [timer.py:260:stop] epoch=0/micro_step=2030/global_step=2030, RunningAvgSamplesPerSec=56.14066272367551, CurrSamplesPerSec=52.99280072205986, MemAllocated=17.1GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:04,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:04,940] [INFO] [timer.py:260:stop] epoch=0/micro_step=2040/global_step=2040, RunningAvgSamplesPerSec=56.101336884144125, CurrSamplesPerSec=63.69959782481792, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:08,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:08,530] [INFO] [timer.py:260:stop] epoch=0/micro_step=2050/global_step=2050, RunningAvgSamplesPerSec=56.129865741643904, CurrSamplesPerSec=68.51639632980314, MemAllocated=16.83GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:12,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:12,202] [INFO] [timer.py:260:stop] epoch=0/micro_step=2060/global_step=2060, RunningAvgSamplesPerSec=56.1478563332688, CurrSamplesPerSec=43.38614451561016, MemAllocated=17.33GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:16,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:16,135] [INFO] [timer.py:260:stop] epoch=0/micro_step=2070/global_step=2070, RunningAvgSamplesPerSec=56.14382614170803, CurrSamplesPerSec=69.39860145645426, MemAllocated=16.81GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:19,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:19,461] [INFO] [timer.py:260:stop] epoch=0/micro_step=2080/global_step=2080, RunningAvgSamplesPerSec=56.192964000880785, CurrSamplesPerSec=70.18654395230874, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:23,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:23,145] [INFO] [timer.py:260:stop] epoch=0/micro_step=2090/global_step=2090, RunningAvgSamplesPerSec=56.20875153475361, CurrSamplesPerSec=62.932594121502945, MemAllocated=16.92GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:27,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:27,081] [INFO] [timer.py:260:stop] epoch=0/micro_step=2100/global_step=2100, RunningAvgSamplesPerSec=56.20170246793326, CurrSamplesPerSec=30.714563462277795, MemAllocated=17.92GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:30,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:30,488] [INFO] [timer.py:260:stop] epoch=0/micro_step=2110/global_step=2110, RunningAvgSamplesPerSec=56.24466001326939, CurrSamplesPerSec=45.972815911754815, MemAllocated=17.23GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:34,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:34,317] [INFO] [timer.py:260:stop] epoch=0/micro_step=2120/global_step=2120, RunningAvgSamplesPerSec=56.248583982559836, CurrSamplesPerSec=82.69842721219881, MemAllocated=16.64GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:37,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:37,709] [INFO] [timer.py:260:stop] epoch=0/micro_step=2130/global_step=2130, RunningAvgSamplesPerSec=56.29095771193341, CurrSamplesPerSec=73.76432388703128, MemAllocated=16.76GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:41,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:41,375] [INFO] [timer.py:260:stop] epoch=0/micro_step=2140/global_step=2140, RunningAvgSamplesPerSec=56.30753779914487, CurrSamplesPerSec=84.08378199240966, MemAllocated=16.66GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 18, 'loss_train': 7.1954239891649046, 'loss_val': 7.784355179704017, 'accuracy_train': 0.968816067653277, 'accuracy_val': 0.7484143763213531}\n",
      "[2024-05-10 12:00:53,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:53,126] [INFO] [timer.py:260:stop] epoch=0/micro_step=2150/global_step=2150, RunningAvgSamplesPerSec=56.28225354346668, CurrSamplesPerSec=45.59537176381863, MemAllocated=17.23GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:00:56,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:00:56,652] [INFO] [timer.py:260:stop] epoch=0/micro_step=2160/global_step=2160, RunningAvgSamplesPerSec=56.31241664657995, CurrSamplesPerSec=64.41750081591123, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:00,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:00,917] [INFO] [timer.py:260:stop] epoch=0/micro_step=2170/global_step=2170, RunningAvgSamplesPerSec=56.27614639739175, CurrSamplesPerSec=39.561861036537124, MemAllocated=17.49GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:05,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:05,139] [INFO] [timer.py:260:stop] epoch=0/micro_step=2180/global_step=2180, RunningAvgSamplesPerSec=56.24462553329141, CurrSamplesPerSec=23.92750104735843, MemAllocated=18.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:08,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:08,868] [INFO] [timer.py:260:stop] epoch=0/micro_step=2190/global_step=2190, RunningAvgSamplesPerSec=56.257593260550784, CurrSamplesPerSec=80.2348909918042, MemAllocated=16.69GB, MaxMemAllocated=21.42GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 12:01:13,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:13,518] [INFO] [timer.py:260:stop] epoch=0/micro_step=2200/global_step=2200, RunningAvgSamplesPerSec=56.18791719055626, CurrSamplesPerSec=81.44170890922572, MemAllocated=16.73GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:17,021] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:17,022] [INFO] [timer.py:260:stop] epoch=0/micro_step=2210/global_step=2210, RunningAvgSamplesPerSec=56.22008688886424, CurrSamplesPerSec=55.74293524622913, MemAllocated=17.05GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:21,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:21,425] [INFO] [timer.py:260:stop] epoch=0/micro_step=2220/global_step=2220, RunningAvgSamplesPerSec=56.17388713959816, CurrSamplesPerSec=64.3253763160518, MemAllocated=16.89GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:24,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:24,873] [INFO] [timer.py:260:stop] epoch=0/micro_step=2230/global_step=2230, RunningAvgSamplesPerSec=56.21064053834097, CurrSamplesPerSec=68.79975559321443, MemAllocated=16.83GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:28,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:28,279] [INFO] [timer.py:260:stop] epoch=0/micro_step=2240/global_step=2240, RunningAvgSamplesPerSec=56.25020815117842, CurrSamplesPerSec=73.58678200684453, MemAllocated=16.78GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:31,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:31,853] [INFO] [timer.py:260:stop] epoch=0/micro_step=2250/global_step=2250, RunningAvgSamplesPerSec=56.275356963826624, CurrSamplesPerSec=65.18702026844568, MemAllocated=16.85GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:36,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:36,114] [INFO] [timer.py:260:stop] epoch=0/micro_step=2260/global_step=2260, RunningAvgSamplesPerSec=56.241861864058215, CurrSamplesPerSec=81.21511781292735, MemAllocated=16.63GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 19, 'loss_train': 6.857857508588795, 'loss_val': 4.54122621564482, 'accuracy_train': 0.968816067653277, 'accuracy_val': 0.7399577167019028}\n",
      "[2024-05-10 12:01:47,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:47,195] [INFO] [timer.py:260:stop] epoch=0/micro_step=2270/global_step=2270, RunningAvgSamplesPerSec=56.27898871967688, CurrSamplesPerSec=69.09336358900336, MemAllocated=16.79GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:50,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:50,775] [INFO] [timer.py:260:stop] epoch=0/micro_step=2280/global_step=2280, RunningAvgSamplesPerSec=56.30260049819506, CurrSamplesPerSec=52.35558398801676, MemAllocated=17.15GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:54,885] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:54,886] [INFO] [timer.py:260:stop] epoch=0/micro_step=2290/global_step=2290, RunningAvgSamplesPerSec=56.28140551293606, CurrSamplesPerSec=57.005785620299875, MemAllocated=16.97GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:01:59,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:01:59,108] [INFO] [timer.py:260:stop] epoch=0/micro_step=2300/global_step=2300, RunningAvgSamplesPerSec=56.2501819995005, CurrSamplesPerSec=23.897678741531422, MemAllocated=18.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:03,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:03,636] [INFO] [timer.py:260:stop] epoch=0/micro_step=2310/global_step=2310, RunningAvgSamplesPerSec=56.197145039724504, CurrSamplesPerSec=40.90058630651276, MemAllocated=17.43GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:07,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:07,822] [INFO] [timer.py:260:stop] epoch=0/micro_step=2320/global_step=2320, RunningAvgSamplesPerSec=56.17168745740215, CurrSamplesPerSec=52.98464201008707, MemAllocated=17.08GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:11,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:11,006] [INFO] [timer.py:260:stop] epoch=0/micro_step=2330/global_step=2330, RunningAvgSamplesPerSec=56.22873388865045, CurrSamplesPerSec=82.16865245291844, MemAllocated=16.64GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:14,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:14,530] [INFO] [timer.py:260:stop] epoch=0/micro_step=2340/global_step=2340, RunningAvgSamplesPerSec=56.25564353798866, CurrSamplesPerSec=40.80030009490441, MemAllocated=17.42GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:18,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:18,440] [INFO] [timer.py:260:stop] epoch=0/micro_step=2350/global_step=2350, RunningAvgSamplesPerSec=56.25226816850172, CurrSamplesPerSec=68.89744158350787, MemAllocated=16.8GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:22,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:22,878] [INFO] [timer.py:260:stop] epoch=0/micro_step=2360/global_step=2360, RunningAvgSamplesPerSec=56.20664832762606, CurrSamplesPerSec=52.65872940519189, MemAllocated=17.11GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:26,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:26,994] [INFO] [timer.py:260:stop] epoch=0/micro_step=2370/global_step=2370, RunningAvgSamplesPerSec=56.18572003477288, CurrSamplesPerSec=46.605934631136016, MemAllocated=17.22GB, MaxMemAllocated=21.42GB\n",
      "[2024-05-10 12:02:30,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=25, lr=[0.01], mom=[[0.8, 0.999]]\n",
      "[2024-05-10 12:02:30,190] [INFO] [timer.py:260:stop] epoch=0/micro_step=2380/global_step=2380, RunningAvgSamplesPerSec=56.23995332459168, CurrSamplesPerSec=138.11079509205484, MemAllocated=16.33GB, MaxMemAllocated=21.42GB\n",
      "{'epoch': 20, 'loss_train': 2.8175459170190273, 'loss_val': 4.05708245243129, 'accuracy_train': 0.9873150105708245, 'accuracy_val': 0.7547568710359408}\n"
     ]
    }
   ],
   "source": [
    "hist = train_with_hist(model_engine, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d3a2a32-9601-4458-82d3-b169d7e74ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_learning_curves(hist):\n",
    "    x_arr = np.arange(len(hist[0])) + 1\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "    ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "    ax.plot(x_arr, hist[3], '--<', label='Validation acc.')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Accuracy', size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad6a0c1b-3a25-4cff-96c6-3266913e208e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curves(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a230c-2886-4676-9a87-6bac8b503bb4",
   "metadata": {},
   "source": [
    "### Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c316c-7bce-4176-b1fa-00dd769a6a1a",
   "metadata": {},
   "source": [
    "Checking the results of the test dataset…\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ebc0b680-b1de-4aee-bad4-baf7f50b9f0e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.764\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test, _ = evaluate(model, test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ef9b09f4-0640-4e59-aefc-9d2d136d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(model, dataloader):\n",
    "    model.eval()\n",
    "    y_test = np.asarray([])\n",
    "    y_predict = np.asarray([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, label) in enumerate(dataloader):\n",
    "            outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "            predicted_label = outputs.logits\n",
    "                  \n",
    "            y_test = np.concatenate((y_test, np.asarray(label.to(device='cpu', dtype=torch.long))), axis=None)\n",
    "            y_predict = np.concatenate((y_predict, np.asarray((predicted_label.argmax(1).to(device='cpu', dtype=torch.long)))), axis=None)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    sns.heatmap(cm, annot=True, fmt = \"d\")\n",
    "    print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2efeb419-1df7-4e6c-8814-ddf3294b9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.71      0.78      0.74       113\n",
      "         1.0       0.82      0.76      0.79       150\n",
      "\n",
      "    accuracy                           0.77       263\n",
      "   macro avg       0.76      0.77      0.77       263\n",
      "weighted avg       0.77      0.77      0.77       263\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a457def5-9ac2-48c9-a2ef-69ead17b1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    with torch.no_grad():\n",
    "        encoded_text = tokenizer(text)\n",
    "        encoded_text.input_ids = torch.tensor(encoded_text.input_ids).to(device).unsqueeze(0)\n",
    "        encoded_text.attention_mask = torch.tensor(encoded_text.attention_mask).to(device).unsqueeze(0)\n",
    "\n",
    "        outputs = model(input_ids=encoded_text.input_ids, attention_mask=encoded_text.attention_mask)\n",
    "        predicted_label = outputs.logits\n",
    "        return predicted_label.argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40c741cb-e91b-406f-a1f7-3c0ff292ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 0\n"
     ]
    }
   ],
   "source": [
    "ex_text_str = 'The ePump Software shall define Fault ID 1 as follows:'\n",
    "\n",
    "print(\"This is a %s\" % predict(ex_text_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "61cc330f-c045-4453-a430-d19c5cac6bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"The IO Service shall select the XLR-PW DEV_INFO_DATA file if HPP_XLR_WIRING is grounded (logical 1) and bits AC_TYPE_BIT1 - AC_TYPE_BIT6 do not indicate a CFM engine configuration. NOTE: HPP_XLR_WIRING and bits AC_TYPE_BIT[1-6] are discrete inputs which are received on constant pins between hardware configurations. See 282100-ICD-x for more details.\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "587df1de-6144-420e-a789-7771209aeca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"I shall like waffles\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3f924b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"Bumblebe is red\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c0e6726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"Bumblebee is red\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f5f1d9-eeef-4d53-b3a9-abfac4069485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83e817c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "model_name = \"gemma_7b\"\n",
    "today = date.today().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38fa186d-4971-48b2-8048-ba14511e893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-10 12:05:52,039] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2380 is about to be saved!\n",
      "[2024-05-10 12:05:52,039] [INFO] [engine.py:3596:save_16bit_model] Saving model weights to ./models/gemma_7b/2024-05-10_gemma_7b_2.pth, tag: global_step2380\n",
      "[2024-05-10 12:05:52,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/gemma_7b/2024-05-10_gemma_7b_2.pth...\n",
      "[2024-05-10 12:06:05,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/gemma_7b/2024-05-10_gemma_7b_2.pth.\n",
      "[2024-05-10 12:06:05,869] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2380 is ready now!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_engine.save_16bit_model(f\"./models/{model_name}/\", f\"{today}_{model_name}_2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "125176c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.embed_tokens.weight',\n",
       "              tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "                      [ 0.0155, -0.0138,  0.0460,  ..., -0.0035,  0.0096,  0.0147],\n",
       "                      [-0.0128,  0.0186,  0.0124,  ...,  0.0279, -0.0185,  0.0166],\n",
       "                      ...,\n",
       "                      [-0.0046, -0.0186, -0.0093,  ..., -0.0291,  0.0085, -0.0129],\n",
       "                      [-0.0070,  0.0447, -0.0192,  ...,  0.0064,  0.0377,  0.0028],\n",
       "                      [ 0.0225, -0.0199,  0.0173,  ...,  0.0483, -0.0099,  0.0029]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.1101e-02, -3.9337e-02, -6.6643e-03,  ..., -1.4275e-02,\n",
       "                        1.4923e-02,  1.0880e-02],\n",
       "                      [-1.1034e-03,  3.7048e-02, -9.9716e-03,  ...,  1.1520e-02,\n",
       "                       -3.3325e-02, -1.9180e-02],\n",
       "                      [-1.4656e-02, -2.8954e-03,  1.7107e-04,  ..., -5.4779e-03,\n",
       "                        4.6356e-02, -2.5116e-02],\n",
       "                      ...,\n",
       "                      [-6.5956e-03,  2.7267e-02, -1.2398e-03,  ...,  2.0477e-02,\n",
       "                        3.0918e-03, -1.5732e-02],\n",
       "                      [-1.3527e-02,  3.9581e-02, -1.1322e-02,  ..., -3.9551e-02,\n",
       "                        6.4969e-05, -1.1520e-03],\n",
       "                      [ 5.0240e-03, -6.5460e-03,  1.2312e-03,  ..., -1.0307e-02,\n",
       "                        2.7634e-02,  1.9562e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0156, -0.0130, -0.0102,  ..., -0.0087, -0.0282,  0.0009],\n",
       "                      [-0.0425,  0.0275,  0.0163,  ..., -0.0023, -0.0279,  0.0094],\n",
       "                      [-0.0287, -0.0203, -0.0057,  ..., -0.0265, -0.0054,  0.0079],\n",
       "                      ...,\n",
       "                      [ 0.0095, -0.0165, -0.0002,  ...,  0.0239, -0.0302, -0.0118],\n",
       "                      [ 0.0070, -0.0020, -0.0093,  ..., -0.0347,  0.0074, -0.0199],\n",
       "                      [-0.0091, -0.0241,  0.0075,  ...,  0.0233, -0.0051,  0.0322]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0163,  0.0009, -0.0180,  ..., -0.0028,  0.0018,  0.0102],\n",
       "                      [-0.0093,  0.0201,  0.0106,  ..., -0.0088, -0.0112, -0.0232],\n",
       "                      [-0.0199, -0.0055, -0.0155,  ...,  0.0181,  0.0237,  0.0149],\n",
       "                      ...,\n",
       "                      [-0.0340,  0.0366,  0.0107,  ..., -0.0091, -0.0071,  0.0223],\n",
       "                      [-0.0064,  0.0104,  0.0135,  ...,  0.0123,  0.0135, -0.0397],\n",
       "                      [-0.0050,  0.0080, -0.0202,  ...,  0.0196,  0.0084, -0.0225]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.o_proj.weight',\n",
       "              tensor([[-1.8250e-02, -2.0172e-02, -1.6327e-02,  ...,  1.6571e-02,\n",
       "                        4.6692e-03, -2.1164e-02],\n",
       "                      [ 2.0065e-02, -5.1880e-02,  4.4128e-02,  ..., -1.2947e-02,\n",
       "                        3.5278e-02, -1.2794e-02],\n",
       "                      [-3.5858e-02,  1.7303e-02, -8.0490e-03,  ...,  5.0323e-02,\n",
       "                        2.2068e-03, -2.6188e-03],\n",
       "                      ...,\n",
       "                      [ 4.7035e-03,  2.3926e-02, -6.0463e-03,  ...,  8.0490e-03,\n",
       "                       -1.2207e-02,  8.5602e-03],\n",
       "                      [ 8.7357e-03, -2.0432e-02,  9.6588e-03,  ...,  1.4473e-02,\n",
       "                        2.7299e-05,  4.0245e-03],\n",
       "                      [-2.2774e-03,  5.8441e-03,  3.1174e-02,  ..., -7.1220e-03,\n",
       "                       -5.2795e-03, -8.5754e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.gate_proj.weight',\n",
       "              tensor([[ 8.7738e-03, -6.3972e-03,  7.1678e-03,  ...,  1.7624e-02,\n",
       "                       -1.4549e-02, -1.6846e-02],\n",
       "                      [-1.0040e-02, -1.4687e-03, -1.7715e-02,  ...,  2.8849e-05,\n",
       "                       -2.6016e-02,  1.4992e-02],\n",
       "                      [-3.9978e-03,  1.0620e-02,  4.0802e-02,  ...,  5.4436e-03,\n",
       "                        1.0948e-02, -2.3499e-03],\n",
       "                      ...,\n",
       "                      [ 4.0550e-03,  2.0996e-02,  1.2329e-02,  ..., -1.0490e-02,\n",
       "                       -1.7838e-02,  1.3290e-02],\n",
       "                      [ 3.9032e-02,  1.5854e-02, -2.6779e-02,  ...,  2.1851e-02,\n",
       "                       -1.6327e-02,  3.9062e-02],\n",
       "                      [-1.1421e-02,  1.5404e-02,  1.4038e-02,  ..., -6.4201e-03,\n",
       "                       -6.6772e-02, -1.2024e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.up_proj.weight',\n",
       "              tensor([[-0.0195,  0.0100,  0.0046,  ..., -0.0088,  0.0078,  0.0067],\n",
       "                      [-0.0025, -0.0054, -0.0063,  ...,  0.0033, -0.0005, -0.0024],\n",
       "                      [-0.0102, -0.0130, -0.0120,  ...,  0.0219,  0.0205, -0.0135],\n",
       "                      ...,\n",
       "                      [ 0.0017,  0.0331,  0.0280,  ...,  0.0062, -0.0128,  0.0005],\n",
       "                      [ 0.0322, -0.0150, -0.0086,  ...,  0.0164, -0.0345,  0.0321],\n",
       "                      [-0.0178, -0.0027, -0.0088,  ...,  0.0121, -0.0090,  0.0051]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.down_proj.weight',\n",
       "              tensor([[-3.3020e-02,  2.1057e-02,  5.0049e-03,  ...,  1.5869e-03,\n",
       "                        1.5121e-02,  3.4210e-02],\n",
       "                      [ 2.4033e-02, -4.2480e-02, -1.0658e-02,  ..., -3.1982e-02,\n",
       "                       -2.3514e-02,  4.6806e-03],\n",
       "                      [ 3.1372e-02, -1.5345e-03, -3.4943e-02,  ..., -2.4124e-02,\n",
       "                        2.6901e-02, -5.9903e-05],\n",
       "                      ...,\n",
       "                      [ 5.3101e-03,  2.6764e-02, -1.9920e-04,  ..., -1.1612e-02,\n",
       "                        6.3667e-03,  7.1144e-03],\n",
       "                      [-1.0017e-02,  1.3443e-02,  9.8705e-04,  ..., -9.8877e-03,\n",
       "                       -5.1941e-02, -1.2505e-02],\n",
       "                      [ 2.8400e-03,  4.5929e-02,  3.7956e-03,  ..., -1.4954e-02,\n",
       "                       -1.7334e-02,  3.4485e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.0.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.0.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0254, -0.0063,  0.0351,  ..., -0.0269, -0.0147,  0.0165],\n",
       "                      [-0.0069, -0.0290, -0.0138,  ...,  0.0164, -0.0321,  0.0231],\n",
       "                      [-0.0020, -0.0001,  0.0113,  ..., -0.0354,  0.0078, -0.0157],\n",
       "                      ...,\n",
       "                      [ 0.0010, -0.0032, -0.0140,  ...,  0.0048, -0.0242,  0.0146],\n",
       "                      [-0.0109, -0.0271,  0.0335,  ...,  0.0031, -0.0186,  0.0071],\n",
       "                      [-0.0027, -0.0292, -0.0158,  ...,  0.0247, -0.0279, -0.0014]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0020,  0.0057, -0.0025,  ...,  0.0272,  0.0329, -0.0135],\n",
       "                      [-0.0120, -0.0504, -0.0042,  ..., -0.0087,  0.0033,  0.0060],\n",
       "                      [ 0.0163,  0.0219,  0.0310,  ...,  0.0258, -0.0199,  0.0095],\n",
       "                      ...,\n",
       "                      [ 0.0047,  0.0227, -0.0052,  ...,  0.0098, -0.0267, -0.0071],\n",
       "                      [ 0.0161,  0.0174, -0.0435,  ...,  0.0023,  0.0375,  0.0332],\n",
       "                      [-0.0517, -0.0041,  0.0189,  ...,  0.0127, -0.0135,  0.0175]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0064,  0.0210,  0.0182,  ...,  0.0129, -0.0354, -0.0244],\n",
       "                      [ 0.0109, -0.0158,  0.0012,  ...,  0.0097, -0.0208,  0.0049],\n",
       "                      [ 0.0373, -0.0052, -0.0038,  ..., -0.0078,  0.0329, -0.0193],\n",
       "                      ...,\n",
       "                      [-0.0222, -0.0096,  0.0083,  ...,  0.0245,  0.0494,  0.0285],\n",
       "                      [-0.0098, -0.0017,  0.0036,  ...,  0.0206, -0.0264,  0.0294],\n",
       "                      [-0.0051,  0.0120,  0.0174,  ..., -0.0011,  0.0230,  0.0209]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0241, -0.0121,  0.0142,  ...,  0.0214,  0.0287,  0.0086],\n",
       "                      [-0.0136,  0.0027, -0.0270,  ..., -0.0310, -0.0135, -0.0260],\n",
       "                      [-0.0057, -0.0152, -0.0011,  ..., -0.0212, -0.0312,  0.0109],\n",
       "                      ...,\n",
       "                      [-0.0257, -0.0087, -0.0019,  ..., -0.0043, -0.0148,  0.0396],\n",
       "                      [ 0.0451, -0.0297,  0.0023,  ..., -0.0106,  0.0004,  0.0115],\n",
       "                      [-0.0414, -0.0145,  0.0403,  ...,  0.0236,  0.0377, -0.0039]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0198,  0.0029,  0.0147,  ..., -0.0111,  0.0125,  0.0173],\n",
       "                      [-0.0050,  0.0172, -0.0185,  ...,  0.0438, -0.0023, -0.0179],\n",
       "                      [-0.0045, -0.0094,  0.0069,  ...,  0.0260, -0.0079,  0.0062],\n",
       "                      ...,\n",
       "                      [-0.0192,  0.0135, -0.0132,  ..., -0.0211, -0.0106, -0.0168],\n",
       "                      [ 0.0251, -0.0074, -0.0084,  ...,  0.0065, -0.0033, -0.0007],\n",
       "                      [ 0.0157,  0.0007, -0.0349,  ..., -0.0164,  0.0385, -0.0172]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0128, -0.0114, -0.0183,  ..., -0.0269, -0.0405,  0.0137],\n",
       "                      [ 0.0238,  0.0015, -0.0195,  ..., -0.0147, -0.0113,  0.0033],\n",
       "                      [ 0.0273,  0.0329,  0.0093,  ..., -0.0124,  0.0132, -0.0064],\n",
       "                      ...,\n",
       "                      [-0.0107, -0.0117, -0.0317,  ...,  0.0112, -0.0117, -0.0021],\n",
       "                      [ 0.0304, -0.0029, -0.0039,  ..., -0.0045, -0.0126,  0.0027],\n",
       "                      [-0.0180,  0.0197,  0.0010,  ..., -0.0096,  0.0095, -0.0019]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.down_proj.weight',\n",
       "              tensor([[-0.0084, -0.0140, -0.0195,  ...,  0.0298, -0.0177,  0.0021],\n",
       "                      [ 0.0176,  0.0312,  0.0176,  ..., -0.0362,  0.0039,  0.0061],\n",
       "                      [-0.0243, -0.0033, -0.0171,  ...,  0.0510, -0.0166, -0.0077],\n",
       "                      ...,\n",
       "                      [ 0.0276,  0.0004,  0.0118,  ..., -0.0250,  0.0226,  0.0119],\n",
       "                      [ 0.0082, -0.0161,  0.0327,  ..., -0.0059, -0.0303,  0.0024],\n",
       "                      [ 0.0071, -0.0291,  0.0183,  ...,  0.0588, -0.0045, -0.0229]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.1.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.5007e-02,  4.5547e-03, -1.1688e-02,  ...,  2.7725e-02,\n",
       "                        5.0201e-03,  3.2196e-02],\n",
       "                      [ 1.0176e-03, -2.0447e-02, -1.0078e-02,  ...,  2.0874e-02,\n",
       "                        1.0376e-02, -1.1292e-02],\n",
       "                      [-2.7100e-02, -5.6992e-03,  6.1691e-05,  ...,  1.5083e-02,\n",
       "                        1.1855e-04,  2.7817e-02],\n",
       "                      ...,\n",
       "                      [-1.9852e-02, -8.7967e-03, -2.3594e-03,  ..., -7.2145e-04,\n",
       "                       -1.2955e-02,  7.5760e-03],\n",
       "                      [-1.3481e-02,  3.1357e-03,  1.2255e-03,  ..., -1.0246e-02,\n",
       "                       -5.3749e-03,  1.3237e-02],\n",
       "                      [-2.2163e-03,  1.6327e-02,  3.6316e-02,  ..., -3.4637e-03,\n",
       "                        5.6213e-02,  1.6136e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0044, -0.0130, -0.0217,  ..., -0.0094,  0.0105,  0.0070],\n",
       "                      [ 0.0004, -0.0116,  0.0221,  ..., -0.0499, -0.0169, -0.0313],\n",
       "                      [-0.0200,  0.0107, -0.0316,  ..., -0.0238, -0.0448,  0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0273, -0.0053,  0.0342,  ...,  0.0074,  0.0339, -0.0345],\n",
       "                      [-0.0464, -0.0054,  0.0107,  ...,  0.0059,  0.0024, -0.0168],\n",
       "                      [ 0.0115, -0.0062,  0.0005,  ..., -0.0025,  0.0063,  0.0204]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0282, -0.0152,  0.0003,  ...,  0.0213, -0.0173,  0.0016],\n",
       "                      [-0.0172, -0.0160,  0.0066,  ..., -0.0032,  0.0434,  0.0414],\n",
       "                      [ 0.0072,  0.0132,  0.0128,  ...,  0.0234, -0.0131,  0.0152],\n",
       "                      ...,\n",
       "                      [-0.0467, -0.0039, -0.0465,  ...,  0.0049, -0.0178, -0.0220],\n",
       "                      [ 0.0148, -0.0465,  0.0048,  ...,  0.0083, -0.0039, -0.0102],\n",
       "                      [-0.0137,  0.0154,  0.0227,  ...,  0.0435, -0.0002, -0.0193]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0112,  0.0277, -0.0048,  ..., -0.0123,  0.0134, -0.0209],\n",
       "                      [ 0.0287,  0.0249,  0.0235,  ...,  0.0177,  0.0151,  0.0331],\n",
       "                      [ 0.0120,  0.0176,  0.0104,  ..., -0.0071, -0.0036, -0.0185],\n",
       "                      ...,\n",
       "                      [ 0.0580,  0.0319, -0.0447,  ...,  0.0358,  0.0214,  0.0018],\n",
       "                      [ 0.0168,  0.0073, -0.0197,  ..., -0.0104,  0.0189,  0.0308],\n",
       "                      [-0.0484, -0.0257, -0.0225,  ..., -0.0139,  0.0166, -0.0141]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0036,  0.0350, -0.0149,  ..., -0.0081, -0.0335, -0.0002],\n",
       "                      [-0.0231,  0.0178, -0.0008,  ..., -0.0352, -0.0194, -0.0193],\n",
       "                      [-0.0312, -0.0012,  0.0065,  ...,  0.0063,  0.0216,  0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0307, -0.0296,  0.0020,  ...,  0.0181,  0.0408,  0.0204],\n",
       "                      [ 0.0117, -0.0235, -0.0351,  ..., -0.0033, -0.0337,  0.0004],\n",
       "                      [ 0.0075, -0.0333, -0.0184,  ...,  0.0365, -0.0084,  0.0324]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0046,  0.0174,  0.0011,  ...,  0.0024, -0.0106,  0.0116],\n",
       "                      [-0.0097, -0.0074,  0.0180,  ..., -0.0047, -0.0213,  0.0343],\n",
       "                      [ 0.0124,  0.0506,  0.0294,  ..., -0.0058,  0.0048, -0.0215],\n",
       "                      ...,\n",
       "                      [-0.0426,  0.0137, -0.0028,  ..., -0.0249, -0.0104,  0.0017],\n",
       "                      [ 0.0242, -0.0177,  0.0075,  ..., -0.0450, -0.0041, -0.0157],\n",
       "                      [ 0.0033,  0.0158, -0.0163,  ...,  0.0160,  0.0010, -0.0009]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.down_proj.weight',\n",
       "              tensor([[-0.0136, -0.0087,  0.0174,  ...,  0.0121,  0.0179,  0.0215],\n",
       "                      [ 0.0181, -0.0020, -0.0191,  ...,  0.0040, -0.0193,  0.0068],\n",
       "                      [-0.0139,  0.0172, -0.0195,  ...,  0.0272, -0.0070, -0.0093],\n",
       "                      ...,\n",
       "                      [-0.0178,  0.0236, -0.0134,  ...,  0.0141, -0.0225, -0.0027],\n",
       "                      [ 0.0388,  0.0327, -0.0082,  ..., -0.0255, -0.0255,  0.0300],\n",
       "                      [-0.0160, -0.0127,  0.0005,  ...,  0.0134, -0.0090, -0.0390]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.2.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0403,  0.0224, -0.0263,  ...,  0.0114, -0.0056, -0.0088],\n",
       "                      [-0.0069, -0.0059, -0.0013,  ...,  0.0242,  0.0196,  0.0157],\n",
       "                      [ 0.0212,  0.0391,  0.0192,  ..., -0.0020,  0.0344,  0.0290],\n",
       "                      ...,\n",
       "                      [-0.0047,  0.0006, -0.0019,  ..., -0.0319,  0.0052,  0.0135],\n",
       "                      [-0.0116, -0.0506,  0.0004,  ..., -0.0041, -0.0221,  0.0154],\n",
       "                      [ 0.0010,  0.0040, -0.0027,  ..., -0.0559, -0.0203, -0.0355]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0048,  0.0037, -0.0064,  ..., -0.0079,  0.0273, -0.0302],\n",
       "                      [ 0.0090, -0.0365, -0.0035,  ..., -0.0419, -0.0108, -0.0011],\n",
       "                      [ 0.0041,  0.0129, -0.0408,  ..., -0.0232,  0.0386,  0.0154],\n",
       "                      ...,\n",
       "                      [-0.0187,  0.0092,  0.0023,  ..., -0.0155, -0.0363,  0.0184],\n",
       "                      [ 0.0187, -0.0200, -0.0106,  ...,  0.0248,  0.0401, -0.0120],\n",
       "                      [ 0.0268,  0.0057,  0.0098,  ...,  0.0167,  0.0208,  0.0019]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0243,  0.0063,  0.0054,  ..., -0.0096, -0.0003,  0.0117],\n",
       "                      [ 0.0205, -0.0436,  0.0320,  ..., -0.0036,  0.0143,  0.0093],\n",
       "                      [ 0.0160, -0.0265,  0.0272,  ...,  0.0147, -0.0496,  0.0062],\n",
       "                      ...,\n",
       "                      [-0.0020,  0.0227, -0.0171,  ..., -0.0115, -0.0155,  0.0334],\n",
       "                      [ 0.0419,  0.0130, -0.0211,  ...,  0.0073,  0.0252,  0.0274],\n",
       "                      [-0.0061, -0.0374, -0.0413,  ...,  0.0089, -0.0056,  0.0122]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0350, -0.0209, -0.0189,  ..., -0.0027,  0.0155, -0.0176],\n",
       "                      [-0.0176,  0.0109, -0.0006,  ..., -0.0172,  0.0187,  0.0107],\n",
       "                      [ 0.0327, -0.0111,  0.0201,  ..., -0.0026,  0.0151,  0.0035],\n",
       "                      ...,\n",
       "                      [-0.0057, -0.0105, -0.0170,  ...,  0.0394, -0.0161,  0.0177],\n",
       "                      [-0.0116,  0.0118,  0.0229,  ...,  0.0358,  0.0204,  0.0018],\n",
       "                      [-0.0049, -0.0093, -0.0045,  ...,  0.0161, -0.0332, -0.0164]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.gate_proj.weight',\n",
       "              tensor([[-6.5575e-03,  1.7166e-02,  2.3315e-02,  ..., -2.3865e-02,\n",
       "                       -3.0746e-02, -3.4912e-02],\n",
       "                      [ 2.7542e-02,  6.4087e-03,  2.8275e-02,  ...,  2.2675e-02,\n",
       "                        1.4435e-02,  2.3594e-03],\n",
       "                      [ 4.7684e-06, -1.2222e-02, -7.6561e-03,  ...,  6.3095e-03,\n",
       "                        5.2757e-03,  1.2550e-02],\n",
       "                      ...,\n",
       "                      [ 3.0106e-02,  2.6871e-02,  7.6904e-03,  ..., -1.5282e-02,\n",
       "                       -1.8250e-02,  8.3351e-04],\n",
       "                      [ 7.1564e-03, -3.4275e-03,  1.7670e-02,  ..., -1.5701e-02,\n",
       "                       -4.9988e-02,  2.7176e-02],\n",
       "                      [ 1.8372e-02,  2.1338e-05,  3.2215e-03,  ...,  7.7362e-03,\n",
       "                       -1.5030e-03,  2.2034e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.up_proj.weight',\n",
       "              tensor([[-0.0164, -0.0086,  0.0364,  ..., -0.0074,  0.0229,  0.0031],\n",
       "                      [-0.0062,  0.0088,  0.0078,  ...,  0.0022,  0.0150,  0.0120],\n",
       "                      [ 0.0134,  0.0412, -0.0225,  ..., -0.0178,  0.0209,  0.0401],\n",
       "                      ...,\n",
       "                      [-0.0302,  0.0432, -0.0156,  ...,  0.0014, -0.0008,  0.0146],\n",
       "                      [-0.0139,  0.0164,  0.0236,  ...,  0.0257,  0.0067, -0.0011],\n",
       "                      [ 0.0077,  0.0249, -0.0048,  ..., -0.0141, -0.0037,  0.0018]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0065,  0.0162,  0.0143,  ...,  0.0024, -0.0065,  0.0232],\n",
       "                      [-0.0209, -0.0049, -0.0102,  ...,  0.0109,  0.0297,  0.0051],\n",
       "                      [-0.0109, -0.0125,  0.0168,  ...,  0.0097,  0.0167, -0.0283],\n",
       "                      ...,\n",
       "                      [-0.0041,  0.0027, -0.0078,  ..., -0.0168, -0.0270,  0.0050],\n",
       "                      [ 0.0010, -0.0054, -0.0037,  ...,  0.0193, -0.0176, -0.0042],\n",
       "                      [ 0.0137,  0.0012,  0.0087,  ..., -0.0046, -0.0024, -0.0134]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.3.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.q_proj.weight',\n",
       "              tensor([[ 1.5762e-02, -8.1558e-03, -1.1654e-03,  ..., -3.2928e-02,\n",
       "                       -4.1382e-02,  1.2146e-02],\n",
       "                      [-2.3041e-02, -1.9470e-02, -4.7624e-05,  ...,  1.9989e-02,\n",
       "                       -9.5291e-03, -1.6113e-02],\n",
       "                      [-1.5354e-04,  1.3027e-03, -2.2297e-03,  ...,  1.1681e-02,\n",
       "                        1.5778e-02, -9.7351e-03],\n",
       "                      ...,\n",
       "                      [-1.5732e-02, -2.8591e-03, -2.1866e-02,  ..., -6.4945e-04,\n",
       "                       -8.0338e-03, -1.4153e-02],\n",
       "                      [ 1.8356e-02,  6.9847e-03,  2.7756e-02,  ..., -2.1454e-02,\n",
       "                        1.1873e-03,  3.5065e-02],\n",
       "                      [-3.9429e-02, -1.5701e-02, -2.6733e-02,  ..., -5.2299e-03,\n",
       "                       -1.7900e-03, -1.9135e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0115,  0.0048, -0.0050,  ...,  0.0131, -0.0053, -0.0076],\n",
       "                      [ 0.0122,  0.0274, -0.0057,  ...,  0.0278, -0.0172, -0.0135],\n",
       "                      [ 0.0248,  0.0054, -0.0272,  ..., -0.0308,  0.0063,  0.0257],\n",
       "                      ...,\n",
       "                      [-0.0075,  0.0005, -0.0128,  ..., -0.0050,  0.0521,  0.0038],\n",
       "                      [ 0.0044,  0.0035, -0.0310,  ..., -0.0170, -0.0037,  0.0157],\n",
       "                      [-0.0095,  0.0238,  0.0011,  ...,  0.0308, -0.0305,  0.0004]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0120, -0.0166,  0.0255,  ..., -0.0110,  0.0076, -0.0074],\n",
       "                      [-0.0070,  0.0204,  0.0421,  ...,  0.0250,  0.0433, -0.0028],\n",
       "                      [-0.0338,  0.0130, -0.0187,  ..., -0.0171, -0.0054,  0.0135],\n",
       "                      ...,\n",
       "                      [-0.0227, -0.0078,  0.0056,  ..., -0.0091,  0.0098,  0.0329],\n",
       "                      [-0.0085, -0.0479, -0.0632,  ..., -0.0360, -0.0008, -0.0202],\n",
       "                      [ 0.0023, -0.0294,  0.0056,  ..., -0.0101, -0.0148, -0.0017]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0264, -0.0196,  0.0103,  ..., -0.0251,  0.0166,  0.0122],\n",
       "                      [-0.0152,  0.0190, -0.0020,  ..., -0.0071, -0.0164,  0.0167],\n",
       "                      [ 0.0367,  0.0138,  0.0118,  ...,  0.0225,  0.0344, -0.0062],\n",
       "                      ...,\n",
       "                      [-0.0006,  0.0385,  0.0184,  ..., -0.0369,  0.0028,  0.0067],\n",
       "                      [-0.0007, -0.0587,  0.0411,  ...,  0.0099, -0.0275, -0.0164],\n",
       "                      [ 0.0083,  0.0211, -0.0269,  ..., -0.0193, -0.0050,  0.0145]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0104,  0.0125,  0.0083,  ...,  0.0159, -0.0243, -0.0043],\n",
       "                      [ 0.0173,  0.0087, -0.0216,  ..., -0.0083,  0.0086,  0.0077],\n",
       "                      [ 0.0074,  0.0035, -0.0012,  ..., -0.0121,  0.0059,  0.0136],\n",
       "                      ...,\n",
       "                      [-0.0057, -0.0097, -0.0498,  ..., -0.0114, -0.0228,  0.0190],\n",
       "                      [ 0.0182, -0.0038,  0.0069,  ..., -0.0285,  0.0124,  0.0143],\n",
       "                      [-0.0096, -0.0088,  0.0308,  ...,  0.0376,  0.0233,  0.0181]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.up_proj.weight',\n",
       "              tensor([[-0.0083,  0.0018, -0.0272,  ..., -0.0405,  0.0083,  0.0249],\n",
       "                      [-0.0031,  0.0090,  0.0176,  ..., -0.0128,  0.0088,  0.0188],\n",
       "                      [-0.0153, -0.0120,  0.0017,  ..., -0.0034, -0.0069, -0.0148],\n",
       "                      ...,\n",
       "                      [ 0.0147,  0.0014, -0.0136,  ..., -0.0038,  0.0367, -0.0022],\n",
       "                      [ 0.0408, -0.0405,  0.0210,  ...,  0.0249, -0.0083,  0.0153],\n",
       "                      [ 0.0075, -0.0282,  0.0098,  ..., -0.0144, -0.0262,  0.0063]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0124, -0.0060, -0.0034,  ...,  0.0111, -0.0148,  0.0160],\n",
       "                      [-0.0260,  0.0022,  0.0282,  ..., -0.0191,  0.0287,  0.0096],\n",
       "                      [-0.0058,  0.0237, -0.0003,  ...,  0.0110, -0.0066,  0.0235],\n",
       "                      ...,\n",
       "                      [ 0.0108,  0.0019,  0.0128,  ...,  0.0143, -0.0012, -0.0066],\n",
       "                      [-0.0161, -0.0468,  0.0518,  ...,  0.0211,  0.0144, -0.0099],\n",
       "                      [-0.0127,  0.0080, -0.0072,  ..., -0.0473, -0.0439,  0.0088]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.4.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0242, -0.0008, -0.0131,  ..., -0.0071, -0.0092,  0.0074],\n",
       "                      [-0.0049,  0.0467,  0.0179,  ...,  0.0373,  0.0113, -0.0104],\n",
       "                      [-0.0113, -0.0208,  0.0071,  ...,  0.0084,  0.0053,  0.0143],\n",
       "                      ...,\n",
       "                      [ 0.0333, -0.0140, -0.0089,  ...,  0.0264,  0.0057,  0.0159],\n",
       "                      [-0.0199,  0.0426,  0.0086,  ..., -0.0002,  0.0182, -0.0040],\n",
       "                      [ 0.0060, -0.0181, -0.0066,  ...,  0.0286, -0.0316,  0.0034]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.k_proj.weight',\n",
       "              tensor([[-7.5264e-03,  2.5833e-02,  2.8610e-05,  ..., -5.5466e-03,\n",
       "                       -7.0000e-04, -1.2886e-02],\n",
       "                      [ 1.6800e-02, -3.7659e-02, -2.3514e-02,  ..., -2.4506e-02,\n",
       "                        1.7563e-02,  7.5417e-03],\n",
       "                      [-2.7939e-02,  1.0345e-02,  2.9449e-02,  ...,  2.5696e-02,\n",
       "                        2.8732e-02,  4.0497e-02],\n",
       "                      ...,\n",
       "                      [-1.8600e-02,  2.3346e-02, -7.2765e-04,  ...,  1.5205e-02,\n",
       "                       -9.6500e-05, -9.3079e-03],\n",
       "                      [ 1.4969e-02, -1.5839e-02,  3.2288e-02,  ..., -5.7983e-03,\n",
       "                        4.7188e-03, -7.9422e-03],\n",
       "                      [-2.9240e-03,  1.1795e-02,  2.1805e-02,  ...,  2.4017e-02,\n",
       "                        2.0981e-02,  1.4717e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0193,  0.0130,  0.0068,  ...,  0.0187,  0.0211, -0.0055],\n",
       "                      [ 0.0011, -0.0104, -0.0106,  ...,  0.0213,  0.0107, -0.0265],\n",
       "                      [ 0.0178,  0.0070,  0.0059,  ..., -0.0006, -0.0201,  0.0349],\n",
       "                      ...,\n",
       "                      [-0.0156,  0.0134, -0.0004,  ...,  0.0134, -0.0008,  0.0206],\n",
       "                      [-0.0145,  0.0021,  0.0200,  ...,  0.0103,  0.0266,  0.0067],\n",
       "                      [-0.0086, -0.0132,  0.0259,  ...,  0.0138, -0.0065,  0.0124]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0137, -0.0139,  0.0093,  ...,  0.0057, -0.0147,  0.0288],\n",
       "                      [-0.0092, -0.0244, -0.0081,  ...,  0.0067, -0.0129, -0.0160],\n",
       "                      [-0.0137,  0.0228, -0.0094,  ..., -0.0052,  0.0205, -0.0189],\n",
       "                      ...,\n",
       "                      [-0.0208,  0.0029, -0.0032,  ..., -0.0135,  0.0390,  0.0320],\n",
       "                      [ 0.0207, -0.0043,  0.0081,  ..., -0.0086,  0.0088, -0.0146],\n",
       "                      [-0.0136, -0.0005, -0.0021,  ..., -0.0003,  0.0387,  0.0199]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0084, -0.0087, -0.0017,  ...,  0.0161,  0.0105, -0.0061],\n",
       "                      [-0.0278, -0.0240,  0.0037,  ..., -0.0049, -0.0260, -0.0038],\n",
       "                      [-0.0149,  0.0007, -0.0066,  ...,  0.0269,  0.0091, -0.0347],\n",
       "                      ...,\n",
       "                      [-0.0240, -0.0078,  0.0264,  ..., -0.0050, -0.0023, -0.0003],\n",
       "                      [ 0.0137, -0.0038,  0.0284,  ..., -0.0274, -0.0113, -0.0014],\n",
       "                      [ 0.0024,  0.0282,  0.0191,  ...,  0.0017,  0.0201, -0.0087]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.up_proj.weight',\n",
       "              tensor([[-1.1368e-02, -9.1400e-03, -1.4282e-02,  ..., -1.9882e-02,\n",
       "                        1.6113e-02, -1.5076e-02],\n",
       "                      [ 1.1501e-03, -1.3504e-02, -4.5128e-03,  ..., -1.4404e-02,\n",
       "                       -1.0063e-02, -4.8599e-03],\n",
       "                      [-1.9287e-02,  9.4070e-03,  3.5278e-02,  ..., -1.7309e-03,\n",
       "                        1.1826e-02, -9.9838e-05],\n",
       "                      ...,\n",
       "                      [ 1.2901e-02, -9.7122e-03,  4.5319e-03,  ...,  7.5264e-03,\n",
       "                       -2.0660e-02,  1.2032e-02],\n",
       "                      [ 1.4153e-03, -1.4809e-02, -1.0788e-02,  ...,  2.7771e-03,\n",
       "                       -6.6147e-03, -2.1286e-02],\n",
       "                      [-1.3885e-02, -5.3940e-03,  4.2992e-03,  ...,  4.6577e-03,\n",
       "                       -3.7994e-02, -3.7811e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.down_proj.weight',\n",
       "              tensor([[-0.0097,  0.0227,  0.0097,  ..., -0.0301,  0.0020, -0.0052],\n",
       "                      [-0.0441, -0.0067,  0.0032,  ...,  0.0054,  0.0138,  0.0012],\n",
       "                      [ 0.0009, -0.0279,  0.0156,  ..., -0.0016,  0.0100, -0.0013],\n",
       "                      ...,\n",
       "                      [-0.0027,  0.0115,  0.0012,  ..., -0.0021,  0.0144, -0.0535],\n",
       "                      [ 0.0336,  0.0129, -0.0169,  ...,  0.0220,  0.0253,  0.0029],\n",
       "                      [-0.0134,  0.0032, -0.0004,  ...,  0.0407,  0.0069, -0.0141]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.5.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0178,  0.0116, -0.0446,  ..., -0.0213, -0.0248,  0.0121],\n",
       "                      [ 0.0192, -0.0492, -0.0133,  ...,  0.0139,  0.0242, -0.0104],\n",
       "                      [-0.0144,  0.0022, -0.0040,  ..., -0.0092, -0.0015, -0.0309],\n",
       "                      ...,\n",
       "                      [-0.0008,  0.0161, -0.0103,  ..., -0.0110, -0.0284,  0.0242],\n",
       "                      [-0.0214, -0.0177,  0.0130,  ..., -0.0110, -0.0152,  0.0025],\n",
       "                      [ 0.0031,  0.0179, -0.0032,  ..., -0.0147,  0.0163, -0.0063]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0174,  0.0023,  0.0140,  ..., -0.0050, -0.0271,  0.0106],\n",
       "                      [-0.0169, -0.0005,  0.0325,  ..., -0.0422, -0.0200, -0.0064],\n",
       "                      [-0.0069, -0.0285, -0.0151,  ...,  0.0223, -0.0359,  0.0151],\n",
       "                      ...,\n",
       "                      [ 0.0176, -0.0041, -0.0211,  ...,  0.0106,  0.0168, -0.0393],\n",
       "                      [-0.0152, -0.0472,  0.0141,  ...,  0.0023, -0.0029, -0.0173],\n",
       "                      [-0.0127, -0.0321,  0.0125,  ...,  0.0294,  0.0023, -0.0138]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0145, -0.0027,  0.0117,  ...,  0.0059,  0.0254, -0.0101],\n",
       "                      [-0.0216,  0.0433, -0.0341,  ...,  0.0066, -0.0250,  0.0017],\n",
       "                      [ 0.0445,  0.0003, -0.0015,  ...,  0.0103, -0.0412,  0.0254],\n",
       "                      ...,\n",
       "                      [ 0.0296,  0.0170,  0.0139,  ...,  0.0145,  0.0155, -0.0181],\n",
       "                      [ 0.0337, -0.0063, -0.0078,  ..., -0.0111, -0.0248,  0.0059],\n",
       "                      [-0.0089, -0.0244, -0.0013,  ...,  0.0192,  0.0047,  0.0172]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0013,  0.0166, -0.0046,  ...,  0.0261, -0.0057,  0.0094],\n",
       "                      [ 0.0226, -0.0398,  0.0077,  ..., -0.0013, -0.0289,  0.0006],\n",
       "                      [-0.0249,  0.0180,  0.0025,  ..., -0.0015, -0.0128,  0.0217],\n",
       "                      ...,\n",
       "                      [ 0.0104, -0.0042, -0.0475,  ...,  0.0007, -0.0312,  0.0026],\n",
       "                      [ 0.0477, -0.0107, -0.0354,  ..., -0.0121,  0.0273, -0.0165],\n",
       "                      [-0.0075,  0.0218, -0.0102,  ..., -0.0403, -0.0140,  0.0185]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0109, -0.0277, -0.0525,  ..., -0.0224,  0.0052,  0.0063],\n",
       "                      [-0.0278,  0.0211,  0.0143,  ...,  0.0021, -0.0291, -0.0289],\n",
       "                      [-0.0349,  0.0006, -0.0335,  ...,  0.0006,  0.0096,  0.0051],\n",
       "                      ...,\n",
       "                      [-0.0262, -0.0106,  0.0018,  ..., -0.0121, -0.0072,  0.0093],\n",
       "                      [ 0.0212,  0.0285,  0.0058,  ...,  0.0171,  0.0065,  0.0303],\n",
       "                      [-0.0089, -0.0221,  0.0276,  ...,  0.0275,  0.0032, -0.0016]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0087,  0.0319,  0.0229,  ..., -0.0189,  0.0097,  0.0202],\n",
       "                      [ 0.0564,  0.0034, -0.0208,  ..., -0.0253,  0.0016,  0.0057],\n",
       "                      [ 0.0017, -0.0026,  0.0122,  ...,  0.0044,  0.0194, -0.0124],\n",
       "                      ...,\n",
       "                      [ 0.0015,  0.0022,  0.0353,  ..., -0.0201, -0.0317, -0.0325],\n",
       "                      [ 0.0381,  0.0336,  0.0094,  ..., -0.0275, -0.0192, -0.0015],\n",
       "                      [-0.0126, -0.0184,  0.0322,  ..., -0.0100, -0.0242, -0.0097]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0183, -0.0037, -0.0226,  ...,  0.0269, -0.0104,  0.0091],\n",
       "                      [-0.0169, -0.0076, -0.0078,  ...,  0.0213,  0.0067, -0.0047],\n",
       "                      [-0.0219,  0.0082, -0.0080,  ..., -0.0246, -0.0118,  0.0333],\n",
       "                      ...,\n",
       "                      [-0.0218, -0.0147,  0.0313,  ...,  0.0060,  0.0206, -0.0255],\n",
       "                      [-0.0331,  0.0266,  0.0203,  ...,  0.0177, -0.0526,  0.0377],\n",
       "                      [-0.0030, -0.0028, -0.0013,  ..., -0.0237,  0.0117,  0.0233]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.6.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.q_proj.weight',\n",
       "              tensor([[-6.8016e-03, -4.9591e-03, -1.3763e-02,  ...,  3.6377e-02,\n",
       "                        1.3573e-02,  1.2878e-02],\n",
       "                      [ 2.7161e-02,  1.2833e-02, -1.0132e-02,  ...,  2.0523e-02,\n",
       "                        2.6672e-02, -3.4523e-03],\n",
       "                      [ 3.4866e-03,  9.2850e-03, -2.6108e-02,  ...,  1.9897e-02,\n",
       "                       -3.4454e-02,  1.1384e-05],\n",
       "                      ...,\n",
       "                      [-1.0750e-02,  1.4442e-02,  1.7227e-02,  ..., -1.3618e-02,\n",
       "                       -3.3569e-02,  1.8448e-02],\n",
       "                      [-3.6652e-02,  2.5650e-02, -3.7956e-03,  ...,  7.6065e-03,\n",
       "                       -1.2947e-02,  1.5228e-02],\n",
       "                      [ 4.4250e-04,  1.6327e-02,  1.6388e-02,  ..., -1.6037e-02,\n",
       "                       -1.0223e-02, -3.4241e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0357, -0.0153,  0.0091,  ..., -0.0140,  0.0246, -0.0229],\n",
       "                      [-0.0062,  0.0115,  0.0242,  ..., -0.0014, -0.0071, -0.0183],\n",
       "                      [-0.0144, -0.0100,  0.0048,  ...,  0.0166, -0.0160, -0.0157],\n",
       "                      ...,\n",
       "                      [ 0.0067,  0.0401, -0.0547,  ..., -0.0062, -0.0370,  0.0309],\n",
       "                      [-0.0357,  0.0014, -0.0202,  ..., -0.0145,  0.0212,  0.0046],\n",
       "                      [ 0.0044,  0.0151,  0.0208,  ..., -0.0003, -0.0070,  0.0108]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0063,  0.0170, -0.0108,  ...,  0.0324, -0.0191,  0.0076],\n",
       "                      [ 0.0274,  0.0122,  0.0168,  ..., -0.0274, -0.0050, -0.0227],\n",
       "                      [-0.0059, -0.0109, -0.0046,  ...,  0.0184, -0.0109,  0.0072],\n",
       "                      ...,\n",
       "                      [ 0.0217, -0.0050,  0.0049,  ..., -0.0415,  0.0057,  0.0170],\n",
       "                      [-0.0064,  0.0062, -0.0073,  ...,  0.0431,  0.0269, -0.0023],\n",
       "                      [-0.0074,  0.0335, -0.0107,  ..., -0.0101,  0.0106, -0.0071]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0113, -0.0271, -0.0090,  ..., -0.0058,  0.0158, -0.0161],\n",
       "                      [-0.0031, -0.0089, -0.0235,  ..., -0.0260, -0.0322,  0.0035],\n",
       "                      [-0.0104,  0.0103, -0.0381,  ...,  0.0166, -0.0387,  0.0173],\n",
       "                      ...,\n",
       "                      [ 0.0228, -0.0119,  0.0208,  ..., -0.0063,  0.0041,  0.0086],\n",
       "                      [ 0.0267,  0.0228, -0.0346,  ..., -0.0012, -0.0286,  0.0256],\n",
       "                      [-0.0329,  0.0088, -0.0023,  ..., -0.0078, -0.0145,  0.0185]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0224,  0.0023,  0.0167,  ...,  0.0079, -0.0051, -0.0051],\n",
       "                      [-0.0102,  0.0063, -0.0141,  ..., -0.0151, -0.0061,  0.0127],\n",
       "                      [ 0.0041,  0.0331,  0.0190,  ...,  0.0382, -0.0072, -0.0442],\n",
       "                      ...,\n",
       "                      [ 0.0237, -0.0156,  0.0102,  ..., -0.0272,  0.0320,  0.0242],\n",
       "                      [ 0.0184,  0.0041, -0.0258,  ..., -0.0478,  0.0101, -0.0250],\n",
       "                      [ 0.0023, -0.0398, -0.0142,  ...,  0.0201,  0.0014, -0.0191]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0090,  0.0018, -0.0325,  ...,  0.0024, -0.0033,  0.0432],\n",
       "                      [ 0.0076, -0.0586,  0.0184,  ...,  0.0352,  0.0360,  0.0379],\n",
       "                      [ 0.0085, -0.0054,  0.0230,  ...,  0.0081,  0.0128,  0.0234],\n",
       "                      ...,\n",
       "                      [-0.0010, -0.0376, -0.0137,  ...,  0.0038,  0.0041,  0.0331],\n",
       "                      [-0.0124, -0.0033, -0.0249,  ..., -0.0005, -0.0050, -0.0224],\n",
       "                      [ 0.0263, -0.0027, -0.0155,  ..., -0.0118, -0.0322,  0.0044]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.down_proj.weight',\n",
       "              tensor([[ 1.3535e-02,  1.0696e-02,  2.2850e-03,  ..., -3.3760e-03,\n",
       "                       -1.6220e-02,  4.2572e-02],\n",
       "                      [-2.4780e-02,  3.1769e-02,  2.0051e-04,  ...,  2.2831e-03,\n",
       "                       -1.8707e-02, -7.0610e-03],\n",
       "                      [-1.9775e-02,  1.6266e-02,  5.3177e-03,  ..., -2.6825e-02,\n",
       "                       -2.5582e-04,  2.4662e-03],\n",
       "                      ...,\n",
       "                      [-1.8204e-02,  1.0712e-02,  3.8239e-02,  ..., -3.9935e-05,\n",
       "                       -1.2482e-02, -1.5900e-02],\n",
       "                      [-1.8799e-02, -2.4368e-02, -3.4523e-03,  ..., -7.5684e-03,\n",
       "                        1.7958e-03, -8.3828e-04],\n",
       "                      [ 7.0343e-03,  1.1368e-02, -1.2253e-02,  ..., -1.5297e-02,\n",
       "                       -8.0490e-03, -1.7059e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.7.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.7.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0103,  0.0134,  0.0342,  ...,  0.0239,  0.0067, -0.0422],\n",
       "                      [ 0.0048, -0.0198, -0.0140,  ..., -0.0153, -0.0356,  0.0174],\n",
       "                      [ 0.0326, -0.0068,  0.0047,  ..., -0.0103, -0.0005,  0.0184],\n",
       "                      ...,\n",
       "                      [-0.0093,  0.0259,  0.0286,  ..., -0.0150, -0.0164,  0.0078],\n",
       "                      [-0.0101,  0.0157, -0.0296,  ...,  0.0261,  0.0158,  0.0145],\n",
       "                      [ 0.0212, -0.0144,  0.0027,  ..., -0.0080,  0.0049, -0.0366]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0004, -0.0080,  0.0508,  ..., -0.0073, -0.0271, -0.0153],\n",
       "                      [ 0.0309,  0.0058,  0.0037,  ..., -0.0045, -0.0046,  0.0043],\n",
       "                      [-0.0277, -0.0247, -0.0084,  ..., -0.0104, -0.0021,  0.0215],\n",
       "                      ...,\n",
       "                      [-0.0016, -0.0222, -0.0005,  ...,  0.0102, -0.0111, -0.0133],\n",
       "                      [ 0.0056,  0.0006,  0.0107,  ...,  0.0136, -0.0154,  0.0057],\n",
       "                      [ 0.0146,  0.0123,  0.0053,  ...,  0.0414, -0.0142, -0.0244]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0067,  0.0214, -0.0293,  ..., -0.0031, -0.0210,  0.0209],\n",
       "                      [-0.0045, -0.0259, -0.0139,  ..., -0.0269, -0.0136, -0.0201],\n",
       "                      [ 0.0035, -0.0341,  0.0148,  ...,  0.0036,  0.0229, -0.0202],\n",
       "                      ...,\n",
       "                      [-0.0093, -0.0261, -0.0186,  ...,  0.0111, -0.0138, -0.0317],\n",
       "                      [ 0.0333, -0.0199,  0.0141,  ...,  0.0471, -0.0149, -0.0531],\n",
       "                      [ 0.0172, -0.0136,  0.0241,  ...,  0.0156,  0.0260,  0.0037]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0284, -0.0199, -0.0163,  ..., -0.0348, -0.0249, -0.0214],\n",
       "                      [-0.0038, -0.0088,  0.0104,  ...,  0.0228, -0.0002, -0.0163],\n",
       "                      [ 0.0245,  0.0147, -0.0221,  ..., -0.0097, -0.0253, -0.0024],\n",
       "                      ...,\n",
       "                      [-0.0219, -0.0166, -0.0095,  ..., -0.0275,  0.0085, -0.0424],\n",
       "                      [ 0.0106,  0.0145,  0.0201,  ...,  0.0421, -0.0172,  0.0339],\n",
       "                      [-0.0352,  0.0121,  0.0438,  ..., -0.0324, -0.0268, -0.0089]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0050,  0.0170,  0.0121,  ...,  0.0098,  0.0080,  0.0110],\n",
       "                      [ 0.0269,  0.0159,  0.0137,  ...,  0.0077, -0.0145,  0.0255],\n",
       "                      [-0.0492,  0.0500,  0.0123,  ..., -0.0153,  0.0379, -0.0037],\n",
       "                      ...,\n",
       "                      [ 0.0010,  0.0053,  0.0093,  ..., -0.0277,  0.0056, -0.0049],\n",
       "                      [ 0.0560, -0.0124,  0.0132,  ..., -0.0178,  0.0158,  0.0211],\n",
       "                      [ 0.0077,  0.0002, -0.0192,  ..., -0.0034, -0.0042,  0.0073]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.up_proj.weight',\n",
       "              tensor([[-0.0219,  0.0082, -0.0134,  ..., -0.0179,  0.0074,  0.0126],\n",
       "                      [ 0.0266,  0.0064,  0.0143,  ...,  0.0071,  0.0297, -0.0149],\n",
       "                      [-0.0253,  0.0386, -0.0038,  ...,  0.0056,  0.0679,  0.0417],\n",
       "                      ...,\n",
       "                      [ 0.0072,  0.0091, -0.0124,  ..., -0.0345,  0.0334,  0.0018],\n",
       "                      [-0.0312,  0.0060, -0.0356,  ...,  0.0252, -0.0378, -0.0211],\n",
       "                      [-0.0006, -0.0023,  0.0101,  ..., -0.0256, -0.0031,  0.0266]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.down_proj.weight',\n",
       "              tensor([[-0.0096,  0.0097, -0.0009,  ...,  0.0184,  0.0089,  0.0161],\n",
       "                      [ 0.0365, -0.0093, -0.0063,  ...,  0.0143,  0.0324, -0.0094],\n",
       "                      [-0.0434, -0.0141,  0.0003,  ..., -0.0117,  0.0018, -0.0094],\n",
       "                      ...,\n",
       "                      [-0.0069,  0.0142, -0.0030,  ..., -0.0030, -0.0194, -0.0275],\n",
       "                      [-0.0123, -0.0070,  0.0433,  ..., -0.0344,  0.0373, -0.0072],\n",
       "                      [ 0.0152,  0.0441,  0.0144,  ...,  0.0011,  0.0281, -0.0220]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.8.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0143,  0.0044,  0.0048,  ...,  0.0256,  0.0042,  0.0104],\n",
       "                      [ 0.0085, -0.0087, -0.0214,  ..., -0.0120, -0.0073, -0.0038],\n",
       "                      [ 0.0136,  0.0085,  0.0067,  ..., -0.0079,  0.0033, -0.0206],\n",
       "                      ...,\n",
       "                      [ 0.0080,  0.0073,  0.0264,  ...,  0.0404,  0.0158,  0.0129],\n",
       "                      [ 0.0413,  0.0068, -0.0169,  ...,  0.0304, -0.0410,  0.0225],\n",
       "                      [-0.0203, -0.0261, -0.0228,  ..., -0.0212,  0.0091,  0.0213]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0167, -0.0155,  0.0015,  ...,  0.0347,  0.0111,  0.0023],\n",
       "                      [-0.0089, -0.0086, -0.0057,  ..., -0.0407, -0.0164,  0.0097],\n",
       "                      [-0.0514,  0.0326,  0.0068,  ...,  0.0030, -0.0291,  0.0604],\n",
       "                      ...,\n",
       "                      [-0.0189, -0.0506,  0.0010,  ..., -0.0043, -0.0051, -0.0109],\n",
       "                      [-0.0540, -0.0026,  0.0242,  ..., -0.0121,  0.0178,  0.0030],\n",
       "                      [ 0.0129, -0.0092, -0.0223,  ...,  0.0168,  0.0053, -0.0307]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0026,  0.0116, -0.0412,  ...,  0.0221, -0.0110, -0.0046],\n",
       "                      [ 0.0236, -0.0248, -0.0033,  ...,  0.0301, -0.0142, -0.0146],\n",
       "                      [ 0.0010,  0.0198,  0.0120,  ...,  0.0452, -0.0240, -0.0129],\n",
       "                      ...,\n",
       "                      [-0.0167, -0.0029,  0.0313,  ..., -0.0084, -0.0202, -0.0208],\n",
       "                      [-0.0384,  0.0086,  0.0065,  ..., -0.0187,  0.0188,  0.0172],\n",
       "                      [-0.0045,  0.0334, -0.0182,  ..., -0.0153, -0.0134, -0.0011]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0256, -0.0161, -0.0280,  ..., -0.0054,  0.0221,  0.0298],\n",
       "                      [-0.0228,  0.0135,  0.0240,  ..., -0.0075,  0.0139,  0.0106],\n",
       "                      [-0.0116, -0.0177,  0.0181,  ...,  0.0031,  0.0019, -0.0117],\n",
       "                      ...,\n",
       "                      [-0.0009, -0.0277,  0.0083,  ..., -0.0027,  0.0029, -0.0255],\n",
       "                      [ 0.0015, -0.0194,  0.0115,  ...,  0.0186,  0.0037, -0.0121],\n",
       "                      [-0.0311, -0.0042, -0.0313,  ...,  0.0315, -0.0130, -0.0171]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0037,  0.0220,  0.0052,  ...,  0.0138,  0.0036,  0.0300],\n",
       "                      [-0.0447,  0.0161,  0.0085,  ..., -0.0079, -0.0305, -0.0152],\n",
       "                      [ 0.0137,  0.0294, -0.0102,  ..., -0.0081, -0.0161, -0.0432],\n",
       "                      ...,\n",
       "                      [ 0.0190,  0.0481,  0.0184,  ...,  0.0129, -0.0303,  0.0095],\n",
       "                      [ 0.0030,  0.0168,  0.0008,  ..., -0.0074,  0.0099,  0.0116],\n",
       "                      [ 0.0273, -0.0061,  0.0106,  ...,  0.0293, -0.0060,  0.0028]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.up_proj.weight',\n",
       "              tensor([[-0.0383,  0.0156, -0.0107,  ..., -0.0206, -0.0176, -0.0092],\n",
       "                      [-0.0063, -0.0054, -0.0036,  ..., -0.0034,  0.0373, -0.0164],\n",
       "                      [ 0.0291, -0.0328,  0.0049,  ...,  0.0378, -0.0161,  0.0189],\n",
       "                      ...,\n",
       "                      [ 0.0481, -0.0017,  0.0128,  ...,  0.0056,  0.0055,  0.0156],\n",
       "                      [ 0.0172, -0.0014,  0.0061,  ..., -0.0006, -0.0309,  0.0125],\n",
       "                      [-0.0103,  0.0055, -0.0226,  ..., -0.0152, -0.0436, -0.0144]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0122, -0.0090,  0.0615,  ..., -0.0017,  0.0106,  0.0096],\n",
       "                      [-0.0147,  0.0204,  0.0478,  ...,  0.0209, -0.0014,  0.0098],\n",
       "                      [-0.0247, -0.0578, -0.0073,  ..., -0.0243, -0.0347,  0.0383],\n",
       "                      ...,\n",
       "                      [-0.0041, -0.0086,  0.0047,  ..., -0.0074,  0.0127,  0.0084],\n",
       "                      [-0.0009, -0.0139, -0.0152,  ...,  0.0017, -0.0019, -0.0335],\n",
       "                      [-0.0240, -0.0123,  0.0245,  ...,  0.0079, -0.0014, -0.0219]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.9.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0272,  0.0332, -0.0276,  ..., -0.0146,  0.0066, -0.0417],\n",
       "                      [ 0.0109,  0.0211, -0.0073,  ...,  0.0013,  0.0188, -0.0310],\n",
       "                      [ 0.0177,  0.0082, -0.0042,  ...,  0.0175, -0.0201, -0.0108],\n",
       "                      ...,\n",
       "                      [ 0.0106, -0.0074,  0.0081,  ...,  0.0053,  0.0067,  0.0026],\n",
       "                      [ 0.0191,  0.0313, -0.0188,  ...,  0.0059,  0.0188, -0.0330],\n",
       "                      [-0.0312, -0.0022,  0.0211,  ...,  0.0180,  0.0066,  0.0246]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0037, -0.0122, -0.0061,  ..., -0.0235,  0.0246,  0.0014],\n",
       "                      [-0.0100, -0.0157,  0.0138,  ...,  0.0101,  0.0059, -0.0129],\n",
       "                      [ 0.0087,  0.0122, -0.0252,  ...,  0.0079, -0.0024, -0.0025],\n",
       "                      ...,\n",
       "                      [-0.0046, -0.0152,  0.0026,  ..., -0.0120,  0.0418,  0.0292],\n",
       "                      [ 0.0102, -0.0280,  0.0032,  ...,  0.0080,  0.0115, -0.0034],\n",
       "                      [-0.0219, -0.0080, -0.0037,  ...,  0.0049,  0.0143, -0.0134]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0173,  0.0478,  0.0019,  ..., -0.0152,  0.0027,  0.0199],\n",
       "                      [-0.0016,  0.0279, -0.0458,  ...,  0.0053, -0.0035,  0.0322],\n",
       "                      [ 0.0253,  0.0395, -0.0274,  ..., -0.0242, -0.0043,  0.0061],\n",
       "                      ...,\n",
       "                      [ 0.0334, -0.0228, -0.0053,  ..., -0.0267,  0.0287, -0.0073],\n",
       "                      [-0.0363, -0.0232,  0.0024,  ...,  0.0275, -0.0123, -0.0200],\n",
       "                      [-0.0036,  0.0301, -0.0085,  ...,  0.0066, -0.0033,  0.0179]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.o_proj.weight',\n",
       "              tensor([[-9.8572e-03, -9.5654e-04, -1.1665e-02,  ..., -2.4048e-02,\n",
       "                       -1.1879e-02, -1.9852e-02],\n",
       "                      [ 1.8387e-02, -4.7760e-03,  1.5434e-02,  ..., -1.7929e-02,\n",
       "                       -1.5671e-02, -2.3438e-02],\n",
       "                      [ 2.2049e-02,  1.1528e-02, -7.0915e-03,  ...,  1.5564e-02,\n",
       "                       -8.3685e-05,  2.6367e-02],\n",
       "                      ...,\n",
       "                      [-2.8858e-03, -2.8214e-02, -2.1255e-02,  ...,  3.8853e-03,\n",
       "                       -9.4147e-03,  1.1826e-02],\n",
       "                      [ 6.6032e-03,  4.2114e-03,  1.9007e-03,  ...,  1.0948e-02,\n",
       "                       -8.0948e-03, -1.0872e-02],\n",
       "                      [ 3.8757e-02,  1.3535e-02,  3.1372e-02,  ..., -3.5889e-02,\n",
       "                       -8.5373e-03, -7.1716e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.gate_proj.weight',\n",
       "              tensor([[ 5.6839e-03, -1.5244e-02, -6.2370e-03,  ..., -3.8171e-04,\n",
       "                       -1.0620e-02,  1.7731e-02],\n",
       "                      [-2.3758e-02, -6.5651e-03, -1.6205e-02,  ...,  6.9160e-03,\n",
       "                       -1.3565e-02,  2.3346e-02],\n",
       "                      [-2.8519e-02, -3.5492e-02, -2.3697e-02,  ..., -1.4183e-02,\n",
       "                       -2.0996e-02, -2.4915e-05],\n",
       "                      ...,\n",
       "                      [ 2.2949e-02,  2.9785e-02, -1.4969e-02,  ...,  2.3087e-02,\n",
       "                        3.7262e-02, -1.6357e-02],\n",
       "                      [-1.5175e-02,  7.3204e-03, -3.2410e-02,  ..., -1.1414e-02,\n",
       "                       -4.5371e-04, -4.7638e-02],\n",
       "                      [-4.5929e-03, -2.9404e-02, -2.3746e-03,  ...,  1.3786e-02,\n",
       "                       -3.3600e-02,  1.3626e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0082,  0.0020, -0.0065,  ..., -0.0288, -0.0066, -0.0058],\n",
       "                      [-0.0115,  0.0055, -0.0172,  ...,  0.0180,  0.0249, -0.0087],\n",
       "                      [-0.0122, -0.0005,  0.0103,  ..., -0.0398,  0.0056,  0.0162],\n",
       "                      ...,\n",
       "                      [-0.0237, -0.0165,  0.0145,  ..., -0.0035,  0.0156, -0.0046],\n",
       "                      [-0.0140, -0.0107,  0.0121,  ...,  0.0170, -0.0308, -0.0143],\n",
       "                      [-0.0240,  0.0045, -0.0162,  ..., -0.0067,  0.0341,  0.0240]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0058,  0.0014,  0.0160,  ..., -0.0079, -0.0024,  0.0238],\n",
       "                      [-0.0147,  0.0298,  0.0389,  ...,  0.0230, -0.0167,  0.0271],\n",
       "                      [ 0.0227, -0.0011,  0.0189,  ..., -0.0223, -0.0271, -0.0133],\n",
       "                      ...,\n",
       "                      [ 0.0114,  0.0121, -0.0186,  ...,  0.0015, -0.0095, -0.0139],\n",
       "                      [-0.0060,  0.0064, -0.0034,  ...,  0.0147,  0.0206, -0.0060],\n",
       "                      [-0.0367,  0.0184, -0.0063,  ..., -0.0108,  0.0110, -0.0042]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.10.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0161,  0.0145,  0.0032,  ...,  0.0011, -0.0163,  0.0126],\n",
       "                      [ 0.0103, -0.0142,  0.0056,  ..., -0.0102, -0.0359,  0.0356],\n",
       "                      [-0.0067,  0.0268,  0.0126,  ...,  0.0220,  0.0097, -0.0050],\n",
       "                      ...,\n",
       "                      [ 0.0009, -0.0238, -0.0198,  ...,  0.0139, -0.0009,  0.0222],\n",
       "                      [-0.0173, -0.0006,  0.0158,  ..., -0.0192,  0.0057, -0.0066],\n",
       "                      [-0.0186,  0.0170,  0.0282,  ...,  0.0279, -0.0027,  0.0195]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0193,  0.0074,  0.0418,  ..., -0.0069,  0.0078,  0.0350],\n",
       "                      [-0.0141, -0.0258, -0.0197,  ..., -0.0160, -0.0153,  0.0299],\n",
       "                      [-0.0246,  0.0237, -0.0249,  ..., -0.0382,  0.0118, -0.0194],\n",
       "                      ...,\n",
       "                      [ 0.0087,  0.0159,  0.0076,  ...,  0.0012, -0.0060,  0.0275],\n",
       "                      [-0.0060, -0.0401,  0.0062,  ...,  0.0168,  0.0184, -0.0238],\n",
       "                      [-0.0045, -0.0020,  0.0096,  ..., -0.0037, -0.0013,  0.0051]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0347,  0.0093,  0.0393,  ..., -0.0014,  0.0076,  0.0003],\n",
       "                      [-0.0324,  0.0182,  0.0160,  ...,  0.0256,  0.0074, -0.0134],\n",
       "                      [ 0.0254, -0.0053,  0.0228,  ..., -0.0079, -0.0428,  0.0191],\n",
       "                      ...,\n",
       "                      [ 0.0291,  0.0020, -0.0183,  ...,  0.0056,  0.0235, -0.0111],\n",
       "                      [ 0.0203, -0.0266,  0.0056,  ..., -0.0252, -0.0001, -0.0236],\n",
       "                      [ 0.0397,  0.0012,  0.0091,  ...,  0.0203,  0.0046,  0.0150]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0027,  0.0152,  0.0125,  ...,  0.0254,  0.0305, -0.0143],\n",
       "                      [-0.0259, -0.0092,  0.0339,  ...,  0.0275, -0.0370, -0.0126],\n",
       "                      [ 0.0056, -0.0101,  0.0372,  ...,  0.0063,  0.0267, -0.0085],\n",
       "                      ...,\n",
       "                      [ 0.0095, -0.0120,  0.0095,  ..., -0.0363, -0.0247, -0.0197],\n",
       "                      [-0.0026,  0.0039, -0.0318,  ...,  0.0019, -0.0176,  0.0394],\n",
       "                      [-0.0250, -0.0199, -0.0026,  ..., -0.0140,  0.0083,  0.0050]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.gate_proj.weight',\n",
       "              tensor([[-1.6391e-04,  9.7580e-03, -3.4546e-02,  ..., -1.2352e-02,\n",
       "                       -2.6505e-02, -2.5375e-02],\n",
       "                      [ 3.7415e-02, -2.6367e-02, -1.1215e-02,  ..., -1.5961e-02,\n",
       "                        4.1618e-03,  4.2419e-03],\n",
       "                      [-6.2256e-03,  2.2850e-03,  1.8051e-02,  ...,  2.4490e-02,\n",
       "                        2.8900e-02, -2.0935e-02],\n",
       "                      ...,\n",
       "                      [ 1.4961e-02,  3.0823e-02, -1.6117e-03,  ...,  2.5726e-02,\n",
       "                       -1.8417e-02,  8.3694e-03],\n",
       "                      [ 1.1511e-03, -5.8670e-03,  2.6901e-02,  ...,  1.3496e-02,\n",
       "                        1.2527e-02, -4.1351e-03],\n",
       "                      [ 2.8214e-02,  2.0996e-02, -8.3208e-05,  ..., -2.6455e-03,\n",
       "                        1.6418e-02,  3.2104e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.up_proj.weight',\n",
       "              tensor([[-0.0073,  0.0227, -0.0268,  ..., -0.0109, -0.0152,  0.0189],\n",
       "                      [-0.0112,  0.0018, -0.0019,  ..., -0.0155, -0.0090,  0.0202],\n",
       "                      [ 0.0161,  0.0199, -0.0038,  ...,  0.0275,  0.0190, -0.0323],\n",
       "                      ...,\n",
       "                      [ 0.0049,  0.0102, -0.0365,  ...,  0.0236,  0.0635, -0.0083],\n",
       "                      [ 0.0220,  0.0104,  0.0023,  ..., -0.0043,  0.0030, -0.0012],\n",
       "                      [-0.0304, -0.0094, -0.0094,  ..., -0.0133,  0.0180, -0.0128]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.down_proj.weight',\n",
       "              tensor([[-0.0012,  0.0208,  0.0130,  ..., -0.0260, -0.0159,  0.0221],\n",
       "                      [-0.0144,  0.0368, -0.0076,  ...,  0.0139, -0.0125, -0.0063],\n",
       "                      [-0.0071, -0.0334,  0.0120,  ..., -0.0151,  0.0101, -0.0098],\n",
       "                      ...,\n",
       "                      [ 0.0160,  0.0003, -0.0365,  ...,  0.0037,  0.0058, -0.0042],\n",
       "                      [ 0.0203,  0.0024, -0.0152,  ..., -0.0005, -0.0104, -0.0256],\n",
       "                      [-0.0279,  0.0155, -0.0179,  ..., -0.0020, -0.0130, -0.0113]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.11.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0036, -0.0053, -0.0108,  ...,  0.0042,  0.0147,  0.0062],\n",
       "                      [ 0.0037, -0.0043,  0.0176,  ..., -0.0207,  0.0057,  0.0255],\n",
       "                      [-0.0201, -0.0120,  0.0083,  ...,  0.0021,  0.0273, -0.0109],\n",
       "                      ...,\n",
       "                      [ 0.0124,  0.0164,  0.0070,  ..., -0.0212,  0.0177, -0.0011],\n",
       "                      [-0.0160, -0.0110, -0.0051,  ..., -0.0178,  0.0261,  0.0150],\n",
       "                      [-0.0165,  0.0077, -0.0046,  ...,  0.0332,  0.0172,  0.0046]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0260, -0.0356,  0.0078,  ...,  0.0317,  0.0129,  0.0356],\n",
       "                      [ 0.0349, -0.0213, -0.0138,  ..., -0.0151, -0.0151, -0.0351],\n",
       "                      [-0.0069,  0.0468, -0.0137,  ..., -0.0072, -0.0002,  0.0124],\n",
       "                      ...,\n",
       "                      [ 0.0002,  0.0296, -0.0124,  ..., -0.0018,  0.0183, -0.0044],\n",
       "                      [-0.0092, -0.0149,  0.0080,  ...,  0.0178,  0.0212,  0.0023],\n",
       "                      [ 0.0177,  0.0059, -0.0167,  ...,  0.0138, -0.0188,  0.0277]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0036, -0.0043, -0.0122,  ...,  0.0106, -0.0058,  0.0150],\n",
       "                      [ 0.0094,  0.0316,  0.0080,  ...,  0.0273, -0.0179, -0.0204],\n",
       "                      [ 0.0183,  0.0211, -0.0325,  ..., -0.0057,  0.0691, -0.0253],\n",
       "                      ...,\n",
       "                      [-0.0395, -0.0211, -0.0003,  ..., -0.0322,  0.0007, -0.0066],\n",
       "                      [ 0.0414,  0.0154,  0.0162,  ...,  0.0111, -0.0167, -0.0027],\n",
       "                      [ 0.0022, -0.0015, -0.0135,  ...,  0.0115,  0.0129,  0.0053]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0042,  0.0178,  0.0048,  ..., -0.0136,  0.0090, -0.0032],\n",
       "                      [-0.0055, -0.0200, -0.0230,  ...,  0.0113, -0.0211,  0.0096],\n",
       "                      [-0.0194, -0.0252, -0.0307,  ...,  0.0189, -0.0037,  0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0047, -0.0225,  0.0243,  ..., -0.0382, -0.0005, -0.0126],\n",
       "                      [ 0.0342, -0.0093,  0.0280,  ...,  0.0057,  0.0195,  0.0052],\n",
       "                      [-0.0264, -0.0092,  0.0195,  ...,  0.0125, -0.0075,  0.0219]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.gate_proj.weight',\n",
       "              tensor([[-2.7054e-02, -5.1636e-02,  1.4206e-02,  ..., -1.7426e-02,\n",
       "                       -1.4435e-02, -8.5220e-03],\n",
       "                      [-2.0813e-02, -1.7176e-03, -8.0643e-03,  ...,  1.3023e-02,\n",
       "                        1.6556e-02,  1.7014e-02],\n",
       "                      [ 3.1860e-02,  1.9760e-02, -2.4857e-02,  ...,  1.0696e-02,\n",
       "                       -2.1286e-02, -1.5930e-02],\n",
       "                      ...,\n",
       "                      [ 1.8301e-03, -3.4576e-02,  9.4295e-05,  ..., -1.0315e-02,\n",
       "                       -1.0208e-02,  6.0425e-03],\n",
       "                      [-6.8588e-03,  2.1988e-02, -4.9400e-03,  ..., -1.3573e-02,\n",
       "                        2.2278e-02,  4.2748e-04],\n",
       "                      [-1.0017e-02,  1.0384e-02, -3.3905e-02,  ...,  1.7105e-02,\n",
       "                        2.6993e-02,  2.0920e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.up_proj.weight',\n",
       "              tensor([[-0.0038, -0.0055, -0.0179,  ..., -0.0011, -0.0106,  0.0052],\n",
       "                      [-0.0128,  0.0176, -0.0049,  ...,  0.0358, -0.0108, -0.0220],\n",
       "                      [-0.0237, -0.0315,  0.0064,  ..., -0.0271,  0.0229,  0.0142],\n",
       "                      ...,\n",
       "                      [ 0.0364,  0.0179, -0.0014,  ..., -0.0376,  0.0050, -0.0106],\n",
       "                      [ 0.0095,  0.0001, -0.0121,  ..., -0.0061,  0.0261,  0.0288],\n",
       "                      [ 0.0080, -0.0014, -0.0159,  ..., -0.0055, -0.0081,  0.0029]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.down_proj.weight',\n",
       "              tensor([[-0.0126, -0.0026,  0.0101,  ..., -0.0178,  0.0193, -0.0327],\n",
       "                      [-0.0024, -0.0018,  0.0128,  ..., -0.0610,  0.0090, -0.0023],\n",
       "                      [-0.0060, -0.0114,  0.0416,  ..., -0.0114,  0.0279, -0.0135],\n",
       "                      ...,\n",
       "                      [-0.0036, -0.0056,  0.0259,  ...,  0.0057,  0.0203, -0.0101],\n",
       "                      [ 0.0204,  0.0426, -0.0034,  ...,  0.0092,  0.0042,  0.0094],\n",
       "                      [-0.0047,  0.0161, -0.0066,  ...,  0.0016, -0.0019,  0.0036]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.12.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0026, -0.0146, -0.0112,  ..., -0.0139, -0.0006, -0.0155],\n",
       "                      [-0.0087,  0.0129,  0.0035,  ..., -0.0236,  0.0190,  0.0036],\n",
       "                      [-0.0225, -0.0151,  0.0040,  ...,  0.0126, -0.0249, -0.0303],\n",
       "                      ...,\n",
       "                      [ 0.0018,  0.0265,  0.0277,  ...,  0.0271, -0.0054, -0.0275],\n",
       "                      [-0.0126, -0.0148,  0.0251,  ..., -0.0309,  0.0009,  0.0400],\n",
       "                      [-0.0039, -0.0299, -0.0121,  ..., -0.0046, -0.0127, -0.0032]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0400, -0.0288, -0.0021,  ...,  0.0499, -0.0035, -0.0002],\n",
       "                      [ 0.0021, -0.0021,  0.0222,  ...,  0.0271,  0.0123, -0.0167],\n",
       "                      [ 0.0146, -0.0108, -0.0198,  ..., -0.0400,  0.0186, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0273, -0.0097, -0.0392,  ...,  0.0066, -0.0050, -0.0098],\n",
       "                      [-0.0088, -0.0197,  0.0467,  ..., -0.0114, -0.0136,  0.0066],\n",
       "                      [-0.0044, -0.0568, -0.0125,  ..., -0.0040,  0.0219,  0.0218]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0347, -0.0051,  0.0127,  ..., -0.0162, -0.0145, -0.0190],\n",
       "                      [ 0.0024,  0.0108,  0.0054,  ...,  0.0005, -0.0243, -0.0192],\n",
       "                      [ 0.0207,  0.0047,  0.0352,  ...,  0.0091, -0.0241, -0.0087],\n",
       "                      ...,\n",
       "                      [-0.0481,  0.0077,  0.0149,  ..., -0.0037, -0.0463, -0.0218],\n",
       "                      [-0.0153,  0.0032,  0.0049,  ...,  0.0209, -0.0034, -0.0258],\n",
       "                      [-0.0378,  0.0056, -0.0073,  ..., -0.0048, -0.0090, -0.0242]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.o_proj.weight',\n",
       "              tensor([[ 8.6546e-05,  4.0512e-03,  1.0292e-02,  ..., -2.3499e-03,\n",
       "                       -2.1317e-02,  3.6011e-02],\n",
       "                      [ 1.4484e-04,  7.2250e-03, -3.2082e-03,  ..., -1.4206e-02,\n",
       "                       -3.2288e-02,  3.8223e-03],\n",
       "                      [-1.9470e-02,  3.4729e-02,  1.7151e-02,  ..., -1.8555e-02,\n",
       "                       -2.8839e-02,  2.9816e-02],\n",
       "                      ...,\n",
       "                      [ 2.5272e-04, -1.2070e-02,  3.9459e-02,  ...,  1.5839e-02,\n",
       "                        3.6373e-03,  1.6418e-02],\n",
       "                      [-2.2842e-02,  1.9470e-02,  3.8391e-02,  ...,  5.6534e-03,\n",
       "                        4.8943e-03,  3.9697e-04],\n",
       "                      [-6.7377e-04,  1.8097e-02,  5.6419e-03,  ..., -6.0333e-02,\n",
       "                        1.3367e-02, -3.1891e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0056, -0.0147, -0.0025,  ...,  0.0013, -0.0025,  0.0085],\n",
       "                      [-0.0140,  0.0188,  0.0251,  ..., -0.0049,  0.0025, -0.0054],\n",
       "                      [-0.0293, -0.0090, -0.0237,  ..., -0.0300, -0.0037, -0.0040],\n",
       "                      ...,\n",
       "                      [-0.0148, -0.0193,  0.0063,  ...,  0.0315,  0.0031,  0.0120],\n",
       "                      [-0.0045, -0.0170,  0.0065,  ...,  0.0012, -0.0123, -0.0459],\n",
       "                      [ 0.0088,  0.0241, -0.0023,  ..., -0.0174, -0.0014,  0.0304]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.up_proj.weight',\n",
       "              tensor([[-0.0362, -0.0073, -0.0269,  ..., -0.0210,  0.0166, -0.0029],\n",
       "                      [ 0.0549, -0.0268,  0.0020,  ...,  0.0134, -0.0070,  0.0001],\n",
       "                      [-0.0074,  0.0346, -0.0044,  ...,  0.0088,  0.0321,  0.0079],\n",
       "                      ...,\n",
       "                      [ 0.0164, -0.0162, -0.0085,  ...,  0.0186, -0.0163, -0.0158],\n",
       "                      [-0.0042, -0.0221,  0.0496,  ..., -0.0114,  0.0414, -0.0242],\n",
       "                      [-0.0493,  0.0186, -0.0168,  ...,  0.0137,  0.0014, -0.0299]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.down_proj.weight',\n",
       "              tensor([[-0.0166, -0.0007,  0.0119,  ...,  0.0093,  0.0197, -0.0314],\n",
       "                      [ 0.0229, -0.0281, -0.0262,  ...,  0.0379, -0.0028,  0.0245],\n",
       "                      [ 0.0231,  0.0131, -0.0079,  ...,  0.0207,  0.0151,  0.0213],\n",
       "                      ...,\n",
       "                      [-0.0166, -0.0106, -0.0108,  ..., -0.0410,  0.0030,  0.0309],\n",
       "                      [-0.0099,  0.0001,  0.0013,  ..., -0.0109,  0.0247,  0.0137],\n",
       "                      [-0.0436,  0.0167,  0.0329,  ...,  0.0117, -0.0113, -0.0006]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.13.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0221, -0.0012, -0.0156,  ...,  0.0274, -0.0262, -0.0155],\n",
       "                      [ 0.0209, -0.0265, -0.0042,  ..., -0.0256,  0.0109, -0.0052],\n",
       "                      [ 0.0549,  0.0207,  0.0079,  ..., -0.0142, -0.0014,  0.0096],\n",
       "                      ...,\n",
       "                      [ 0.0224,  0.0004, -0.0253,  ..., -0.0237, -0.0023,  0.0176],\n",
       "                      [ 0.0530, -0.0232, -0.0226,  ..., -0.0070,  0.0043, -0.0050],\n",
       "                      [ 0.0150,  0.0071,  0.0023,  ...,  0.0032,  0.0121, -0.0135]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0107,  0.0092,  0.0338,  ...,  0.0004,  0.0008, -0.0012],\n",
       "                      [-0.0085,  0.0245, -0.0219,  ..., -0.0331,  0.0020,  0.0256],\n",
       "                      [ 0.0041,  0.0183,  0.0368,  ...,  0.0056,  0.0077,  0.0104],\n",
       "                      ...,\n",
       "                      [-0.0225,  0.0309,  0.0185,  ..., -0.0162,  0.0049,  0.0212],\n",
       "                      [ 0.0369, -0.0236,  0.0017,  ..., -0.0245, -0.0024,  0.0238],\n",
       "                      [-0.0041, -0.0065, -0.0050,  ..., -0.0108, -0.0028,  0.0151]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.v_proj.weight',\n",
       "              tensor([[ 4.9133e-03, -5.2414e-03, -8.7509e-03,  ...,  1.0597e-02,\n",
       "                        1.5579e-02,  1.4221e-02],\n",
       "                      [-3.0228e-02, -1.6418e-02,  8.4839e-03,  ...,  1.9951e-03,\n",
       "                        7.1716e-03, -2.9755e-03],\n",
       "                      [-1.6754e-02, -1.5557e-05,  6.2561e-03,  ...,  6.9153e-02,\n",
       "                       -2.0447e-02, -4.6654e-03],\n",
       "                      ...,\n",
       "                      [-1.2810e-02, -4.2725e-02,  5.9166e-03,  ...,  9.9468e-04,\n",
       "                        2.3621e-02, -4.2755e-02],\n",
       "                      [-2.3788e-02,  2.1271e-02,  3.6804e-02,  ...,  1.8196e-03,\n",
       "                       -2.2888e-02, -7.8888e-03],\n",
       "                      [ 9.4452e-03, -3.4241e-02, -3.3817e-03,  ..., -2.4872e-02,\n",
       "                        1.5152e-02, -5.5084e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0252, -0.0422,  0.0133,  ..., -0.0202,  0.0268,  0.0323],\n",
       "                      [-0.0005,  0.0322, -0.0021,  ..., -0.0026,  0.0092, -0.0158],\n",
       "                      [-0.0365, -0.0067,  0.0108,  ...,  0.0630, -0.0072,  0.0158],\n",
       "                      ...,\n",
       "                      [ 0.0086,  0.0049, -0.0215,  ...,  0.0291,  0.0184,  0.0172],\n",
       "                      [-0.0081, -0.0114, -0.0010,  ...,  0.0013, -0.0134,  0.0356],\n",
       "                      [-0.0229,  0.0082,  0.0053,  ..., -0.0102,  0.0327, -0.0173]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0279,  0.0038,  0.0179,  ...,  0.0304, -0.0011, -0.0056],\n",
       "                      [ 0.0188, -0.0101, -0.0078,  ..., -0.0120, -0.0080,  0.0051],\n",
       "                      [-0.0114,  0.0018, -0.0371,  ..., -0.0020, -0.0300, -0.0185],\n",
       "                      ...,\n",
       "                      [ 0.0163,  0.0177, -0.0278,  ..., -0.0175,  0.0159,  0.0271],\n",
       "                      [ 0.0004,  0.0194,  0.0061,  ...,  0.0042, -0.0181,  0.0239],\n",
       "                      [-0.0032, -0.0106,  0.0242,  ...,  0.0094,  0.0197, -0.0216]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.up_proj.weight',\n",
       "              tensor([[ 3.3550e-03, -1.2177e-02,  7.9751e-05,  ...,  5.6839e-03,\n",
       "                        2.1851e-02,  7.0114e-03],\n",
       "                      [-1.7258e-02, -1.2871e-02,  1.7639e-02,  ...,  5.4207e-03,\n",
       "                        6.3667e-03, -3.1853e-03],\n",
       "                      [ 1.3031e-02,  9.5978e-03, -4.1313e-03,  ...,  2.2568e-02,\n",
       "                        3.4447e-03,  3.7506e-02],\n",
       "                      ...,\n",
       "                      [-5.4512e-03,  2.6611e-02,  2.2888e-02,  ..., -1.9188e-03,\n",
       "                        1.8127e-02, -2.1683e-02],\n",
       "                      [ 3.0499e-03, -1.1162e-02, -3.5877e-03,  ...,  6.4392e-03,\n",
       "                        2.7603e-02, -3.1021e-02],\n",
       "                      [ 1.8021e-02, -2.0737e-02,  4.4373e-02,  ...,  1.3878e-02,\n",
       "                        1.8127e-02, -3.2562e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0284,  0.0105, -0.0091,  ..., -0.0035,  0.0075,  0.0289],\n",
       "                      [ 0.0294,  0.0206, -0.0181,  ..., -0.0095, -0.0052,  0.0021],\n",
       "                      [-0.0037,  0.0189, -0.0181,  ..., -0.0253, -0.0209,  0.0296],\n",
       "                      ...,\n",
       "                      [ 0.0084,  0.0092, -0.0161,  ...,  0.0355,  0.0156,  0.0202],\n",
       "                      [-0.0549,  0.0052, -0.0048,  ..., -0.0002, -0.0203, -0.0082],\n",
       "                      [-0.0706, -0.0103,  0.0050,  ..., -0.0498, -0.0075, -0.0172]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.14.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0027, -0.0020,  0.0070,  ...,  0.0278, -0.0182, -0.0030],\n",
       "                      [ 0.0130, -0.0212, -0.0080,  ...,  0.0091, -0.0039, -0.0103],\n",
       "                      [ 0.0059, -0.0239, -0.0154,  ..., -0.0230,  0.0421, -0.0405],\n",
       "                      ...,\n",
       "                      [ 0.0154,  0.0234,  0.0060,  ..., -0.0545,  0.0151,  0.0013],\n",
       "                      [ 0.0061, -0.0182,  0.0168,  ...,  0.0010,  0.0169, -0.0242],\n",
       "                      [-0.0200,  0.0094,  0.0128,  ..., -0.0063,  0.0048,  0.0148]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0242,  0.0109,  0.0068,  ..., -0.0055,  0.0067, -0.0104],\n",
       "                      [-0.0121, -0.0192,  0.0100,  ..., -0.0094, -0.0115, -0.0042],\n",
       "                      [-0.0088,  0.0059,  0.0172,  ...,  0.0138,  0.0216,  0.0059],\n",
       "                      ...,\n",
       "                      [-0.0038,  0.0416, -0.0126,  ..., -0.0197, -0.0049,  0.0529],\n",
       "                      [-0.0128, -0.0333,  0.0119,  ..., -0.0486,  0.0046,  0.0096],\n",
       "                      [ 0.0418,  0.0473,  0.0272,  ..., -0.0204,  0.0112, -0.0247]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.v_proj.weight',\n",
       "              tensor([[-2.0355e-02, -3.4637e-02, -6.8855e-03,  ..., -3.9856e-02,\n",
       "                       -7.1831e-03,  1.8021e-02],\n",
       "                      [-1.2217e-03,  1.4778e-02,  1.7624e-02,  ..., -1.0239e-02,\n",
       "                        3.7170e-02,  1.5125e-03],\n",
       "                      [ 3.9520e-03,  1.7738e-03, -2.3499e-02,  ...,  2.7344e-02,\n",
       "                       -3.7975e-03, -1.1871e-02],\n",
       "                      ...,\n",
       "                      [-1.0452e-02,  4.8637e-05, -8.9569e-03,  ...,  1.4725e-02,\n",
       "                        2.4063e-02,  5.7335e-03],\n",
       "                      [ 5.3358e-04, -1.0933e-02,  9.0256e-03,  ...,  1.8509e-02,\n",
       "                        1.8845e-02,  1.8311e-02],\n",
       "                      [-1.0048e-02, -1.8265e-02, -7.4806e-03,  ...,  2.0935e-02,\n",
       "                        9.8705e-04,  1.8768e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0178,  0.0132, -0.0120,  ..., -0.0082, -0.0168,  0.0449],\n",
       "                      [ 0.0015,  0.0251,  0.0361,  ..., -0.0316,  0.0373,  0.0011],\n",
       "                      [ 0.0294, -0.0202,  0.0011,  ...,  0.0127,  0.0048,  0.0329],\n",
       "                      ...,\n",
       "                      [-0.0080,  0.0162, -0.0373,  ...,  0.0082,  0.0054,  0.0198],\n",
       "                      [-0.0227,  0.0287,  0.0056,  ...,  0.0180, -0.0093,  0.0056],\n",
       "                      [-0.0098,  0.0099, -0.0129,  ...,  0.0044, -0.0216,  0.0152]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0169,  0.0147,  0.0462,  ..., -0.0076,  0.0047,  0.0171],\n",
       "                      [ 0.0010,  0.0063,  0.0073,  ...,  0.0215, -0.0131, -0.0175],\n",
       "                      [-0.0339, -0.0278, -0.0262,  ...,  0.0087, -0.0338, -0.0078],\n",
       "                      ...,\n",
       "                      [-0.0071,  0.0153, -0.0245,  ...,  0.0190, -0.0397,  0.0153],\n",
       "                      [-0.0355, -0.0097, -0.0153,  ...,  0.0406, -0.0116, -0.0117],\n",
       "                      [ 0.0235,  0.0094, -0.0178,  ...,  0.0072,  0.0095,  0.0044]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.up_proj.weight',\n",
       "              tensor([[-0.0097,  0.0183,  0.0082,  ...,  0.0056,  0.0274,  0.0088],\n",
       "                      [ 0.0330,  0.0126, -0.0391,  ...,  0.0158, -0.0028, -0.0052],\n",
       "                      [ 0.0013, -0.0147,  0.0202,  ..., -0.0406,  0.0005,  0.0161],\n",
       "                      ...,\n",
       "                      [ 0.0044,  0.0142, -0.0178,  ...,  0.0203,  0.0079, -0.0082],\n",
       "                      [-0.0135, -0.0130,  0.0179,  ..., -0.0164, -0.0071,  0.0145],\n",
       "                      [-0.0202,  0.0235, -0.0179,  ...,  0.0295, -0.0082, -0.0443]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0125, -0.0032, -0.0098,  ...,  0.0104, -0.0082, -0.0081],\n",
       "                      [ 0.0222, -0.0114, -0.0211,  ...,  0.0166, -0.0255,  0.0055],\n",
       "                      [ 0.0254,  0.0215, -0.0349,  ..., -0.0271,  0.0374, -0.0152],\n",
       "                      ...,\n",
       "                      [-0.0220, -0.0524,  0.0328,  ..., -0.0107,  0.0383,  0.0235],\n",
       "                      [-0.0309, -0.0046,  0.0080,  ..., -0.0292, -0.0311, -0.0095],\n",
       "                      [ 0.0316, -0.0157,  0.0028,  ...,  0.0138,  0.0105, -0.0235]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.15.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0220, -0.0240, -0.0124,  ..., -0.0088, -0.0031, -0.0261],\n",
       "                      [-0.0335,  0.0239,  0.0091,  ...,  0.0068, -0.0502, -0.0266],\n",
       "                      [ 0.0178,  0.0153,  0.0109,  ...,  0.0077, -0.0111,  0.0251],\n",
       "                      ...,\n",
       "                      [-0.0051,  0.0066, -0.0184,  ..., -0.0003,  0.0316, -0.0034],\n",
       "                      [ 0.0305,  0.0082, -0.0062,  ..., -0.0184,  0.0055, -0.0011],\n",
       "                      [-0.0021,  0.0079, -0.0110,  ...,  0.0283,  0.0154, -0.0154]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0014,  0.0284,  0.0122,  ..., -0.0005, -0.0092,  0.0103],\n",
       "                      [ 0.0214,  0.0065,  0.0036,  ...,  0.0254,  0.0616, -0.0341],\n",
       "                      [-0.0103,  0.0199,  0.0112,  ..., -0.0082,  0.0107, -0.0162],\n",
       "                      ...,\n",
       "                      [ 0.0120,  0.0034,  0.0171,  ...,  0.0093,  0.0026, -0.0162],\n",
       "                      [ 0.0197,  0.0049,  0.0115,  ..., -0.0115, -0.0236, -0.0056],\n",
       "                      [-0.0135, -0.0215, -0.0226,  ...,  0.0134, -0.0210,  0.0239]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0089, -0.0201,  0.0276,  ..., -0.0312,  0.0017, -0.0015],\n",
       "                      [ 0.0020, -0.0111, -0.0032,  ..., -0.0046, -0.0053, -0.0224],\n",
       "                      [ 0.0004,  0.0375, -0.0172,  ..., -0.0069, -0.0076,  0.0219],\n",
       "                      ...,\n",
       "                      [-0.0122,  0.0228, -0.0312,  ...,  0.0023, -0.0114,  0.0023],\n",
       "                      [-0.0193,  0.0020,  0.0117,  ..., -0.0165,  0.0032,  0.0238],\n",
       "                      [-0.0014, -0.0236,  0.0022,  ..., -0.0133,  0.0115,  0.0460]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0028,  0.0012,  0.0199,  ..., -0.0381,  0.0071,  0.0032],\n",
       "                      [ 0.0075, -0.0151,  0.0228,  ...,  0.0203, -0.0206,  0.0028],\n",
       "                      [-0.0188, -0.0373, -0.0014,  ..., -0.0305, -0.0014,  0.0117],\n",
       "                      ...,\n",
       "                      [-0.0518, -0.0404,  0.0146,  ...,  0.0131, -0.0320, -0.0066],\n",
       "                      [-0.0211, -0.0402, -0.0033,  ...,  0.0221,  0.0469, -0.0120],\n",
       "                      [ 0.0017, -0.0021, -0.0209,  ..., -0.0122,  0.0199,  0.0197]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.gate_proj.weight',\n",
       "              tensor([[ 1.3451e-02, -1.1246e-02, -1.3680e-02,  ..., -7.3776e-03,\n",
       "                        1.0017e-02, -2.1805e-02],\n",
       "                      [-3.7567e-02, -1.6571e-02,  2.6291e-02,  ...,  5.0964e-03,\n",
       "                        6.0196e-03, -1.4198e-02],\n",
       "                      [-2.1790e-02,  1.7090e-03, -1.4053e-02,  ...,  2.4918e-02,\n",
       "                       -5.6686e-03,  3.4428e-04],\n",
       "                      ...,\n",
       "                      [ 3.4607e-02, -1.7029e-02,  5.6458e-03,  ...,  2.6688e-02,\n",
       "                        1.5366e-02,  7.4234e-03],\n",
       "                      [ 1.7500e-04,  1.4412e-02, -7.5054e-04,  ...,  1.0643e-02,\n",
       "                        1.2657e-02, -2.8778e-02],\n",
       "                      [-2.3518e-03, -2.8503e-02,  1.2341e-03,  ..., -8.8573e-05,\n",
       "                       -7.2937e-03, -5.2124e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0018,  0.0031,  0.0079,  ...,  0.0114, -0.0186,  0.0057],\n",
       "                      [ 0.0081, -0.0124, -0.0123,  ..., -0.0166, -0.0044, -0.0063],\n",
       "                      [ 0.0062, -0.0242,  0.0017,  ..., -0.0032, -0.0067,  0.0030],\n",
       "                      ...,\n",
       "                      [-0.0079,  0.0623, -0.0308,  ..., -0.0314, -0.0112,  0.0181],\n",
       "                      [ 0.0006,  0.0062, -0.0232,  ...,  0.0228, -0.0097,  0.0011],\n",
       "                      [-0.0101,  0.0188,  0.0095,  ...,  0.0019,  0.0132, -0.0231]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0248,  0.0161, -0.0174,  ...,  0.0315, -0.0143, -0.0102],\n",
       "                      [-0.0416,  0.0124,  0.0164,  ..., -0.0078,  0.0252, -0.0142],\n",
       "                      [-0.0177, -0.0273,  0.0027,  ..., -0.0042,  0.0219,  0.0052],\n",
       "                      ...,\n",
       "                      [-0.0009,  0.0047, -0.0181,  ..., -0.0554, -0.0080,  0.0157],\n",
       "                      [-0.0031, -0.0081, -0.0189,  ..., -0.0263,  0.0110,  0.0091],\n",
       "                      [ 0.0198,  0.0121, -0.0053,  ..., -0.0143,  0.0090,  0.0306]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.16.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0069, -0.0063, -0.0240,  ..., -0.0132, -0.0254,  0.0125],\n",
       "                      [-0.0372, -0.0054,  0.0246,  ...,  0.0318, -0.0123,  0.0158],\n",
       "                      [-0.0153,  0.0115,  0.0084,  ...,  0.0200,  0.0088,  0.0102],\n",
       "                      ...,\n",
       "                      [-0.0591, -0.0155, -0.0066,  ..., -0.0134, -0.0370,  0.0164],\n",
       "                      [ 0.0141, -0.0210, -0.0268,  ..., -0.0214,  0.0076, -0.0123],\n",
       "                      [-0.0329,  0.0071, -0.0017,  ...,  0.0124,  0.0084, -0.0299]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0064, -0.0054, -0.0315,  ..., -0.0097,  0.0129,  0.0106],\n",
       "                      [ 0.0291,  0.0043, -0.0032,  ..., -0.0264, -0.0073,  0.0062],\n",
       "                      [ 0.0253,  0.0125, -0.0151,  ..., -0.0101,  0.0053,  0.0120],\n",
       "                      ...,\n",
       "                      [ 0.0027, -0.0111,  0.0105,  ...,  0.0068, -0.0171,  0.0201],\n",
       "                      [ 0.0233, -0.0215,  0.0078,  ...,  0.0198, -0.0131,  0.0054],\n",
       "                      [-0.0218, -0.0026,  0.0096,  ...,  0.0179, -0.0082,  0.0079]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0031, -0.0327, -0.0337,  ..., -0.0138, -0.0155,  0.0004],\n",
       "                      [-0.0464, -0.0262, -0.0002,  ...,  0.0030, -0.0017, -0.0152],\n",
       "                      [ 0.0087,  0.0173,  0.0282,  ...,  0.0199, -0.0230,  0.0064],\n",
       "                      ...,\n",
       "                      [-0.0080,  0.0099, -0.0047,  ..., -0.0069, -0.0022,  0.0014],\n",
       "                      [ 0.0026, -0.0220,  0.0028,  ..., -0.0033,  0.0031, -0.0081],\n",
       "                      [-0.0036,  0.0334,  0.0240,  ...,  0.0147, -0.0017, -0.0335]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0199, -0.0102, -0.0253,  ..., -0.0258,  0.0004,  0.0246],\n",
       "                      [-0.0015, -0.0141,  0.0130,  ..., -0.0168,  0.0287, -0.0202],\n",
       "                      [ 0.0128,  0.0233,  0.0155,  ..., -0.0316,  0.0165, -0.0094],\n",
       "                      ...,\n",
       "                      [-0.0092,  0.0159,  0.0077,  ..., -0.0099,  0.0273,  0.0118],\n",
       "                      [ 0.0048, -0.0043, -0.0030,  ..., -0.0129, -0.0170,  0.0058],\n",
       "                      [-0.0457, -0.0233, -0.0128,  ..., -0.0025,  0.0197,  0.0476]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0098, -0.0031, -0.0091,  ...,  0.0274, -0.0273, -0.0036],\n",
       "                      [-0.0069, -0.0330, -0.0276,  ...,  0.0046, -0.0029,  0.0239],\n",
       "                      [-0.0102,  0.0119,  0.0264,  ...,  0.0270,  0.0068, -0.0120],\n",
       "                      ...,\n",
       "                      [ 0.0040,  0.0135,  0.0232,  ..., -0.0273, -0.0166, -0.0164],\n",
       "                      [-0.0178, -0.0388, -0.0198,  ..., -0.0376, -0.0410,  0.0244],\n",
       "                      [-0.0078,  0.0248,  0.0364,  ..., -0.0003,  0.0419, -0.0068]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.up_proj.weight',\n",
       "              tensor([[ 7.2327e-03,  7.3395e-03, -2.1706e-03,  ..., -1.2222e-02,\n",
       "                       -1.0056e-02, -3.1403e-02],\n",
       "                      [-1.8097e-02, -1.0864e-02,  1.5717e-02,  ..., -3.3951e-03,\n",
       "                        4.7226e-03,  2.5345e-02],\n",
       "                      [-6.2027e-03,  1.3306e-02,  5.1918e-03,  ..., -2.2583e-02,\n",
       "                       -1.2489e-02, -7.0953e-04],\n",
       "                      ...,\n",
       "                      [ 3.5583e-02, -1.9970e-03, -7.4158e-03,  ...,  9.3460e-05,\n",
       "                        2.9640e-03,  6.8054e-03],\n",
       "                      [ 8.7128e-03, -1.6724e-02, -9.2468e-03,  ..., -2.5589e-02,\n",
       "                       -4.2992e-03,  2.8824e-02],\n",
       "                      [ 2.7466e-03, -9.9030e-03,  1.7044e-02,  ..., -4.3823e-02,\n",
       "                       -1.5930e-02, -1.9516e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0103, -0.0119,  0.0396,  ..., -0.0241, -0.0078, -0.0227],\n",
       "                      [ 0.0178, -0.0377,  0.0036,  ...,  0.0305,  0.0045, -0.0115],\n",
       "                      [-0.0114, -0.0078, -0.0347,  ..., -0.0047,  0.0173, -0.0064],\n",
       "                      ...,\n",
       "                      [-0.0350, -0.0171, -0.0041,  ..., -0.0067, -0.0054,  0.0082],\n",
       "                      [-0.0041,  0.0104,  0.0021,  ..., -0.0012,  0.0053, -0.0093],\n",
       "                      [-0.0268, -0.0043, -0.0185,  ...,  0.0157, -0.0018, -0.0293]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.17.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0145,  0.0074, -0.0461,  ...,  0.0029,  0.0222,  0.0136],\n",
       "                      [ 0.0140,  0.0065,  0.0070,  ..., -0.0059,  0.0077,  0.0040],\n",
       "                      [ 0.0044, -0.0176,  0.0024,  ..., -0.0044, -0.0465,  0.0076],\n",
       "                      ...,\n",
       "                      [-0.0100, -0.0146,  0.0064,  ...,  0.0215, -0.0060, -0.0139],\n",
       "                      [ 0.0316,  0.0389, -0.0071,  ..., -0.0214,  0.0131,  0.0129],\n",
       "                      [-0.0076,  0.0108,  0.0115,  ...,  0.0160,  0.0027,  0.0024]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0224,  0.0268, -0.0282,  ..., -0.0025, -0.0049,  0.0046],\n",
       "                      [ 0.0385, -0.0013, -0.0391,  ..., -0.0231, -0.0173, -0.0025],\n",
       "                      [-0.0102, -0.0283,  0.0111,  ...,  0.0022,  0.0071, -0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0104, -0.0215,  0.0026,  ...,  0.0297,  0.0015, -0.0218],\n",
       "                      [-0.0128,  0.0033,  0.0053,  ..., -0.0306, -0.0461,  0.0618],\n",
       "                      [-0.0001, -0.0027,  0.0128,  ..., -0.0324,  0.0195,  0.0251]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0248,  0.0075,  0.0103,  ...,  0.0168,  0.0235, -0.0413],\n",
       "                      [-0.0037,  0.0196, -0.0006,  ...,  0.0204, -0.0096,  0.0204],\n",
       "                      [ 0.0132, -0.0158, -0.0127,  ..., -0.0260,  0.0371, -0.0444],\n",
       "                      ...,\n",
       "                      [-0.0390, -0.0154,  0.0219,  ...,  0.0058,  0.0197,  0.0156],\n",
       "                      [-0.0024,  0.0074, -0.0108,  ..., -0.0033,  0.0285,  0.0138],\n",
       "                      [ 0.0039, -0.0243, -0.0112,  ...,  0.0153,  0.0031,  0.0038]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0058,  0.0009,  0.0406,  ...,  0.0087, -0.0132, -0.0071],\n",
       "                      [ 0.0078, -0.0152,  0.0021,  ...,  0.0060,  0.0242,  0.0055],\n",
       "                      [-0.0092, -0.0591, -0.0037,  ...,  0.0110,  0.0207, -0.0062],\n",
       "                      ...,\n",
       "                      [ 0.0166, -0.0117, -0.0040,  ..., -0.0233, -0.0283, -0.0048],\n",
       "                      [ 0.0282,  0.0263,  0.0034,  ..., -0.0306,  0.0320, -0.0117],\n",
       "                      [-0.0164, -0.0060, -0.0213,  ...,  0.0323, -0.0076, -0.0044]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.gate_proj.weight',\n",
       "              tensor([[ 7.9956e-03, -1.5434e-02,  1.3054e-02,  ...,  1.1665e-02,\n",
       "                        8.9493e-03, -2.5665e-02],\n",
       "                      [-2.0996e-02, -7.7095e-03, -1.7881e-05,  ...,  1.4275e-02,\n",
       "                        2.2858e-02,  5.1460e-03],\n",
       "                      [-6.1302e-03, -1.1230e-02,  1.9007e-03,  ...,  2.5406e-02,\n",
       "                        1.3138e-02, -1.3031e-02],\n",
       "                      ...,\n",
       "                      [ 1.3214e-02, -2.5558e-02, -3.4466e-03,  ..., -1.5099e-02,\n",
       "                        1.0750e-02,  1.0292e-02],\n",
       "                      [ 5.8670e-03,  6.9809e-04,  2.7714e-03,  ...,  6.0349e-03,\n",
       "                        5.3978e-03, -2.1164e-02],\n",
       "                      [ 1.2398e-02,  6.1249e-02, -2.4612e-02,  ..., -1.3252e-02,\n",
       "                       -1.8326e-02, -4.6349e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.up_proj.weight',\n",
       "              tensor([[-0.0353,  0.0010, -0.0138,  ..., -0.0052, -0.0308,  0.0125],\n",
       "                      [-0.0078, -0.0102, -0.0088,  ...,  0.0137, -0.0183,  0.0490],\n",
       "                      [ 0.0150,  0.0316, -0.0061,  ..., -0.0092,  0.0063,  0.0375],\n",
       "                      ...,\n",
       "                      [-0.0148, -0.0078,  0.0285,  ..., -0.0208, -0.0048, -0.0047],\n",
       "                      [ 0.0158, -0.0177,  0.0121,  ..., -0.0219,  0.0050, -0.0219],\n",
       "                      [ 0.0093,  0.0136,  0.0239,  ..., -0.0031,  0.0168,  0.0039]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.down_proj.weight',\n",
       "              tensor([[-0.0147,  0.0133,  0.0035,  ...,  0.0217,  0.0059, -0.0139],\n",
       "                      [-0.0139,  0.0289, -0.0129,  ..., -0.0163,  0.0094,  0.0080],\n",
       "                      [-0.0024, -0.0017, -0.0009,  ...,  0.0005,  0.0022, -0.0121],\n",
       "                      ...,\n",
       "                      [ 0.0146,  0.0313,  0.0076,  ..., -0.0209, -0.0077,  0.0067],\n",
       "                      [ 0.0039, -0.0157,  0.0010,  ..., -0.0045,  0.0343, -0.0195],\n",
       "                      [-0.0335, -0.0281,  0.0011,  ...,  0.0041,  0.0090, -0.0077]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.18.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0224, -0.0110, -0.0131,  ...,  0.0109,  0.0278,  0.0137],\n",
       "                      [-0.0243, -0.0010, -0.0107,  ..., -0.0307,  0.0021, -0.0293],\n",
       "                      [ 0.0106, -0.0284,  0.0291,  ..., -0.0193,  0.0126, -0.0260],\n",
       "                      ...,\n",
       "                      [-0.0036,  0.0090,  0.0011,  ..., -0.0237, -0.0087,  0.0005],\n",
       "                      [-0.0268,  0.0139, -0.0117,  ..., -0.0015, -0.0224, -0.0180],\n",
       "                      [-0.0017,  0.0212, -0.0021,  ...,  0.0141, -0.0249,  0.0097]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0112,  0.0301, -0.0070,  ..., -0.0155,  0.0023, -0.0120],\n",
       "                      [-0.0107, -0.0397,  0.0038,  ...,  0.0066,  0.0211, -0.0099],\n",
       "                      [-0.0220, -0.0109, -0.0203,  ...,  0.0228, -0.0086,  0.0457],\n",
       "                      ...,\n",
       "                      [-0.0358,  0.0014, -0.0202,  ..., -0.0154, -0.0275,  0.0200],\n",
       "                      [ 0.0189,  0.0235, -0.0141,  ...,  0.0181, -0.0142, -0.0305],\n",
       "                      [ 0.0147,  0.0125,  0.0225,  ..., -0.0086, -0.0103, -0.0208]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0145, -0.0296,  0.0213,  ...,  0.0226,  0.0185, -0.0071],\n",
       "                      [-0.0080,  0.0084,  0.0063,  ...,  0.0016, -0.0102, -0.0023],\n",
       "                      [ 0.0170, -0.0027,  0.0019,  ...,  0.0268, -0.0089, -0.0073],\n",
       "                      ...,\n",
       "                      [ 0.0138, -0.0186,  0.0005,  ..., -0.0125,  0.0281, -0.0263],\n",
       "                      [-0.0334, -0.0118,  0.0196,  ...,  0.0089,  0.0094,  0.0122],\n",
       "                      [ 0.0151,  0.0039,  0.0132,  ...,  0.0116, -0.0213,  0.0194]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.o_proj.weight',\n",
       "              tensor([[ 5.9662e-03, -1.3382e-02, -2.2079e-02,  ...,  1.4771e-02,\n",
       "                        5.1613e-03, -3.5339e-02],\n",
       "                      [-3.0121e-02, -1.4160e-02, -1.6724e-02,  ...,  2.0828e-02,\n",
       "                        2.4185e-02,  6.0844e-03],\n",
       "                      [ 9.7961e-03,  2.5482e-03,  1.0672e-03,  ..., -8.7509e-03,\n",
       "                        2.2049e-02,  6.0616e-03],\n",
       "                      ...,\n",
       "                      [ 5.8708e-03, -4.5715e-02,  6.3133e-03,  ...,  2.3575e-02,\n",
       "                       -3.9642e-02,  5.5923e-03],\n",
       "                      [-9.5224e-04,  2.4612e-02,  8.9188e-03,  ..., -4.0985e-02,\n",
       "                       -4.5128e-03, -4.0092e-03],\n",
       "                      [-3.5400e-02,  1.7456e-02, -8.9722e-03,  ..., -2.5320e-04,\n",
       "                        8.2433e-05,  5.6000e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.gate_proj.weight',\n",
       "              tensor([[-2.0096e-02,  2.0981e-02, -1.9958e-02,  ...,  4.7028e-02,\n",
       "                        1.0848e-05,  2.2736e-02],\n",
       "                      [ 2.7603e-02,  5.3009e-02, -6.5689e-03,  ...,  3.3188e-03,\n",
       "                        1.1246e-02, -1.6235e-02],\n",
       "                      [-1.3542e-02,  1.3245e-02, -1.3283e-02,  ..., -3.1738e-03,\n",
       "                        1.7426e-02,  1.9928e-02],\n",
       "                      ...,\n",
       "                      [-9.0933e-04,  2.0264e-02, -4.1718e-02,  ...,  1.6815e-02,\n",
       "                       -4.2224e-04,  1.2032e-02],\n",
       "                      [-1.2611e-02, -7.2327e-03, -7.0839e-03,  ..., -2.7866e-03,\n",
       "                        9.3460e-03,  1.9211e-02],\n",
       "                      [-1.0139e-02, -6.7902e-04, -1.5762e-02,  ...,  7.4272e-03,\n",
       "                       -1.6846e-02, -5.5847e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0186,  0.0383, -0.0293,  ..., -0.0091, -0.0135,  0.0192],\n",
       "                      [-0.0314,  0.0118, -0.0091,  ..., -0.0211, -0.0137,  0.0252],\n",
       "                      [ 0.0129,  0.0066, -0.0112,  ..., -0.0064, -0.0187, -0.0256],\n",
       "                      ...,\n",
       "                      [ 0.0017,  0.0011, -0.0134,  ..., -0.0005,  0.0135,  0.0193],\n",
       "                      [ 0.0184,  0.0089, -0.0134,  ..., -0.0180, -0.0203,  0.0021],\n",
       "                      [ 0.0045, -0.0125,  0.0190,  ...,  0.0063, -0.0459, -0.0098]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.down_proj.weight',\n",
       "              tensor([[-0.0241,  0.0510,  0.0046,  ..., -0.0056, -0.0174, -0.0158],\n",
       "                      [-0.0090,  0.0187, -0.0411,  ..., -0.0226, -0.0321,  0.0234],\n",
       "                      [ 0.0079, -0.0075, -0.0087,  ...,  0.0136, -0.0261,  0.0176],\n",
       "                      ...,\n",
       "                      [-0.0380,  0.0281, -0.0098,  ...,  0.0125,  0.0211,  0.0339],\n",
       "                      [ 0.0060,  0.0199, -0.0010,  ..., -0.0175, -0.0105, -0.0119],\n",
       "                      [-0.0258,  0.0060,  0.0025,  ...,  0.0135,  0.0330,  0.0104]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.19.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.q_proj.weight',\n",
       "              tensor([[ 2.1088e-02,  8.0719e-03,  2.1103e-02,  ...,  4.5624e-03,\n",
       "                        6.1462e-02,  1.9058e-02],\n",
       "                      [-1.5541e-02,  2.6108e-02,  2.1255e-02,  ..., -4.2305e-03,\n",
       "                       -6.5575e-03,  3.1586e-02],\n",
       "                      [ 1.1349e-03, -6.5684e-05,  1.1116e-02,  ..., -3.1219e-02,\n",
       "                        7.2060e-03, -2.1210e-02],\n",
       "                      ...,\n",
       "                      [ 1.6495e-02,  7.2823e-03, -1.4824e-02,  ...,  5.9387e-02,\n",
       "                        2.2888e-02, -3.5950e-02],\n",
       "                      [-1.7792e-02,  1.3176e-02, -1.6403e-02,  ..., -1.6785e-02,\n",
       "                        1.2579e-03,  1.0513e-02],\n",
       "                      [ 1.1124e-02, -2.6688e-02,  1.7044e-02,  ...,  2.6306e-02,\n",
       "                       -1.0727e-02, -2.2736e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.k_proj.weight',\n",
       "              tensor([[-1.4732e-02, -2.8183e-02, -2.2415e-02,  ..., -2.1912e-02,\n",
       "                        1.3840e-02,  2.0767e-02],\n",
       "                      [ 1.9817e-03, -3.9756e-05, -3.0270e-03,  ..., -1.5808e-02,\n",
       "                       -3.8727e-02,  6.6490e-03],\n",
       "                      [ 2.3438e-02, -1.8021e-02,  3.8071e-03,  ...,  7.6408e-03,\n",
       "                        1.0862e-03,  3.5095e-03],\n",
       "                      ...,\n",
       "                      [ 8.8654e-03, -7.2975e-03, -1.8036e-02,  ...,  6.5956e-03,\n",
       "                       -3.1910e-03,  9.5291e-03],\n",
       "                      [-1.9745e-02, -2.4323e-02,  1.4786e-02,  ..., -1.7746e-02,\n",
       "                       -3.3112e-02, -1.8787e-03],\n",
       "                      [-1.3435e-02, -1.7242e-02, -1.9741e-03,  ...,  2.5238e-02,\n",
       "                        4.4556e-02, -1.3649e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.v_proj.weight',\n",
       "              tensor([[ 2.1915e-03, -2.8381e-02,  3.3752e-02,  ..., -1.1806e-03,\n",
       "                       -3.6678e-03,  4.6883e-03],\n",
       "                      [ 6.2370e-03, -2.1225e-02, -1.5091e-02,  ..., -8.7051e-03,\n",
       "                        3.6530e-02, -3.4199e-03],\n",
       "                      [ 9.8419e-03, -5.2185e-03,  1.1192e-02,  ...,  1.4048e-03,\n",
       "                        2.7294e-03,  1.6785e-02],\n",
       "                      ...,\n",
       "                      [-1.2054e-02,  9.5215e-03,  4.5074e-02,  ...,  3.6945e-03,\n",
       "                        3.7354e-02, -7.6532e-05],\n",
       "                      [-1.8326e-02, -1.3596e-02, -1.7715e-02,  ..., -1.5259e-02,\n",
       "                        1.0750e-02, -7.3128e-03],\n",
       "                      [ 2.9236e-02,  1.6586e-02, -1.3130e-02,  ...,  3.4237e-03,\n",
       "                       -5.7030e-03,  8.3084e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0017,  0.0080,  0.0012,  ...,  0.0146, -0.0138, -0.0100],\n",
       "                      [-0.0128,  0.0093,  0.0069,  ..., -0.0424, -0.0056, -0.0238],\n",
       "                      [ 0.0008, -0.0223, -0.0048,  ...,  0.0155,  0.0337, -0.0253],\n",
       "                      ...,\n",
       "                      [ 0.0077, -0.0128,  0.0027,  ..., -0.0144,  0.0278,  0.0116],\n",
       "                      [-0.0245,  0.0217,  0.0340,  ...,  0.0262,  0.0417, -0.0086],\n",
       "                      [-0.0014,  0.0294, -0.0343,  ...,  0.0248,  0.0071, -0.0303]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0008,  0.0132,  0.0022,  ...,  0.0312,  0.0109,  0.0015],\n",
       "                      [ 0.0002,  0.0334,  0.0273,  ...,  0.0058, -0.0052,  0.0259],\n",
       "                      [-0.0265,  0.0006, -0.0044,  ...,  0.0120, -0.0076, -0.0102],\n",
       "                      ...,\n",
       "                      [-0.0030, -0.0044,  0.0227,  ...,  0.0194, -0.0087,  0.0199],\n",
       "                      [ 0.0313, -0.0336, -0.0170,  ...,  0.0353, -0.0122, -0.0002],\n",
       "                      [-0.0042,  0.0357, -0.0007,  ..., -0.0294, -0.0380,  0.0319]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.up_proj.weight',\n",
       "              tensor([[ 6.2866e-03,  4.6611e-05, -3.6163e-03,  ...,  7.6370e-03,\n",
       "                        2.7740e-02, -1.6235e-02],\n",
       "                      [ 2.0386e-02, -1.9363e-02,  2.9964e-03,  ...,  3.6835e-02,\n",
       "                       -1.6968e-02,  2.5269e-02],\n",
       "                      [ 5.6839e-03, -5.8212e-03, -3.0518e-02,  ...,  1.2177e-02,\n",
       "                        1.2293e-03,  2.7740e-02],\n",
       "                      ...,\n",
       "                      [ 4.1580e-03, -1.2428e-02,  5.0430e-03,  ..., -1.0216e-02,\n",
       "                       -1.1925e-02, -2.3911e-02],\n",
       "                      [ 1.5244e-02,  5.0468e-03, -1.2375e-02,  ..., -3.0518e-02,\n",
       "                       -8.1863e-03,  3.1757e-03],\n",
       "                      [ 1.9226e-03,  2.9663e-02, -4.5371e-04,  ..., -1.6003e-03,\n",
       "                        5.3940e-03,  9.7351e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.down_proj.weight',\n",
       "              tensor([[-0.0242, -0.0231, -0.0096,  ...,  0.0066,  0.0202, -0.0185],\n",
       "                      [-0.0089, -0.0182,  0.0153,  ..., -0.0397, -0.0089,  0.0165],\n",
       "                      [-0.0254,  0.0210, -0.0006,  ...,  0.0070, -0.0120, -0.0437],\n",
       "                      ...,\n",
       "                      [-0.0144, -0.0264,  0.0187,  ...,  0.0039, -0.0295, -0.0191],\n",
       "                      [-0.0009, -0.0143, -0.0165,  ...,  0.0169, -0.0278,  0.0131],\n",
       "                      [-0.0051,  0.0140,  0.0010,  ...,  0.0186,  0.0226, -0.0016]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.20.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0284,  0.0263, -0.0116,  ..., -0.0074,  0.0036,  0.0076],\n",
       "                      [-0.0096,  0.0114,  0.0014,  ..., -0.0183,  0.0032,  0.0105],\n",
       "                      [-0.0087,  0.0023,  0.0072,  ..., -0.0051,  0.0509, -0.0048],\n",
       "                      ...,\n",
       "                      [ 0.0210, -0.0006, -0.0509,  ...,  0.0312, -0.0112, -0.0202],\n",
       "                      [ 0.0099,  0.0040, -0.0213,  ..., -0.0337, -0.0069, -0.0174],\n",
       "                      [ 0.0215,  0.0072,  0.0064,  ...,  0.0086, -0.0279,  0.0218]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0378, -0.0362,  0.0164,  ...,  0.0136,  0.0058, -0.0120],\n",
       "                      [ 0.0116,  0.0145, -0.0063,  ...,  0.0051, -0.0230, -0.0187],\n",
       "                      [ 0.0034,  0.0280,  0.0193,  ..., -0.0016, -0.0314, -0.0056],\n",
       "                      ...,\n",
       "                      [ 0.0009,  0.0226,  0.0018,  ...,  0.0076,  0.0149,  0.0071],\n",
       "                      [ 0.0152,  0.0024,  0.0078,  ...,  0.0217, -0.0422, -0.0150],\n",
       "                      [ 0.0118, -0.0071, -0.0041,  ...,  0.0289,  0.0220, -0.0156]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0423,  0.0295,  0.0099,  ...,  0.0054, -0.0100, -0.0266],\n",
       "                      [ 0.0088, -0.0125, -0.0214,  ...,  0.0053,  0.0023, -0.0143],\n",
       "                      [ 0.0343, -0.0123,  0.0040,  ...,  0.0225,  0.0083, -0.0088],\n",
       "                      ...,\n",
       "                      [ 0.0192, -0.0257, -0.0375,  ...,  0.0047, -0.0181, -0.0118],\n",
       "                      [-0.0102, -0.0059, -0.0479,  ...,  0.0131, -0.0430, -0.0142],\n",
       "                      [-0.0022, -0.0371, -0.0274,  ..., -0.0273,  0.0069, -0.0028]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0052,  0.0135, -0.0087,  ...,  0.0103, -0.0021, -0.0023],\n",
       "                      [-0.0287,  0.0031,  0.0205,  ..., -0.0051,  0.0174, -0.0120],\n",
       "                      [ 0.0043,  0.0105,  0.0145,  ..., -0.0028,  0.0205,  0.0098],\n",
       "                      ...,\n",
       "                      [ 0.0152,  0.0141, -0.0363,  ...,  0.0080, -0.0145,  0.0184],\n",
       "                      [-0.0277, -0.0050,  0.0387,  ..., -0.0178, -0.0026, -0.0219],\n",
       "                      [-0.0200,  0.0003, -0.0288,  ...,  0.0357,  0.0050, -0.0469]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0258, -0.0106, -0.0037,  ...,  0.0389,  0.0279, -0.0404],\n",
       "                      [ 0.0239, -0.0189,  0.0091,  ...,  0.0175, -0.0391,  0.0247],\n",
       "                      [-0.0023,  0.0096,  0.0011,  ...,  0.0280,  0.0183,  0.0163],\n",
       "                      ...,\n",
       "                      [-0.0048,  0.0183, -0.0092,  ..., -0.0309,  0.0017, -0.0141],\n",
       "                      [-0.0011, -0.0249, -0.0054,  ...,  0.0311,  0.0177, -0.0004],\n",
       "                      [-0.0158, -0.0312, -0.0286,  ...,  0.0229, -0.0079, -0.0092]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.up_proj.weight',\n",
       "              tensor([[-0.0407, -0.0015, -0.0024,  ..., -0.0013,  0.0131,  0.0154],\n",
       "                      [-0.0002,  0.0125, -0.0050,  ..., -0.0203,  0.0074,  0.0108],\n",
       "                      [-0.0117, -0.0137, -0.0200,  ..., -0.0064,  0.0079,  0.0057],\n",
       "                      ...,\n",
       "                      [ 0.0214,  0.0012,  0.0051,  ...,  0.0320,  0.0093, -0.0114],\n",
       "                      [-0.0310, -0.0216,  0.0177,  ..., -0.0019, -0.0185,  0.0115],\n",
       "                      [ 0.0177,  0.0177,  0.0109,  ...,  0.0249, -0.0074,  0.0134]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0007,  0.0207,  0.0132,  ..., -0.0240,  0.0021, -0.0049],\n",
       "                      [ 0.0265,  0.0079, -0.0262,  ...,  0.0002,  0.0118,  0.0237],\n",
       "                      [-0.0172, -0.0201, -0.0019,  ...,  0.0237, -0.0168,  0.0095],\n",
       "                      ...,\n",
       "                      [-0.0134,  0.0200, -0.0067,  ...,  0.0082,  0.0074,  0.0039],\n",
       "                      [ 0.0381,  0.0075, -0.0226,  ...,  0.0367,  0.0158, -0.0020],\n",
       "                      [ 0.0192, -0.0040, -0.0089,  ...,  0.0076, -0.0031, -0.0134]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.21.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.q_proj.weight',\n",
       "              tensor([[ 8.4877e-04,  2.2659e-02, -1.2222e-02,  ..., -1.5259e-02,\n",
       "                        3.8574e-02,  7.4863e-05],\n",
       "                      [ 4.4250e-02,  1.4801e-02,  3.1525e-02,  ...,  2.1027e-02,\n",
       "                        1.5625e-02, -1.0849e-02],\n",
       "                      [-7.5436e-04, -1.0056e-02, -4.7836e-03,  ..., -1.0803e-02,\n",
       "                       -3.6713e-02,  1.7700e-02],\n",
       "                      ...,\n",
       "                      [ 6.6605e-03, -2.0615e-02, -1.3908e-02,  ..., -2.1301e-02,\n",
       "                        2.9343e-02,  7.5531e-03],\n",
       "                      [ 1.8021e-02, -1.2741e-02,  8.9951e-03,  ..., -1.3420e-02,\n",
       "                        2.1194e-02,  1.2047e-02],\n",
       "                      [-3.0875e-04, -3.1891e-02,  1.4427e-02,  ..., -2.1042e-02,\n",
       "                        1.2650e-02, -1.6069e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0477, -0.0028,  0.0124,  ..., -0.0018,  0.0259,  0.0120],\n",
       "                      [-0.0060, -0.0016,  0.0154,  ...,  0.0144,  0.0121,  0.0109],\n",
       "                      [ 0.0058,  0.0028,  0.0373,  ..., -0.0204,  0.0036, -0.0381],\n",
       "                      ...,\n",
       "                      [ 0.0067,  0.0279, -0.0149,  ...,  0.0136, -0.0265,  0.0244],\n",
       "                      [-0.0009,  0.0075,  0.0222,  ...,  0.0268,  0.0390, -0.0103],\n",
       "                      [-0.0511, -0.0393, -0.0245,  ..., -0.0105, -0.0156, -0.0429]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0022,  0.0021, -0.0102,  ...,  0.0105,  0.0286,  0.0122],\n",
       "                      [ 0.0261, -0.0308, -0.0274,  ...,  0.0166,  0.0006,  0.0448],\n",
       "                      [ 0.0051,  0.0015, -0.0033,  ...,  0.0081,  0.0506, -0.0174],\n",
       "                      ...,\n",
       "                      [ 0.0127,  0.0110,  0.0388,  ..., -0.0535, -0.0184, -0.0280],\n",
       "                      [-0.0345, -0.0159, -0.0221,  ...,  0.0496, -0.0157, -0.0057],\n",
       "                      [-0.0021, -0.0103, -0.0139,  ..., -0.0318, -0.0206,  0.0187]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0052,  0.0269,  0.0068,  ...,  0.0244, -0.0477, -0.0366],\n",
       "                      [-0.0299, -0.0097,  0.0213,  ...,  0.0083, -0.0305,  0.0229],\n",
       "                      [-0.0155, -0.0144,  0.0349,  ...,  0.0041, -0.0472,  0.0163],\n",
       "                      ...,\n",
       "                      [ 0.0078,  0.0190,  0.0041,  ...,  0.0213,  0.0048, -0.0298],\n",
       "                      [-0.0406, -0.0171, -0.0065,  ...,  0.0118, -0.0079, -0.0039],\n",
       "                      [ 0.0178,  0.0027,  0.0177,  ...,  0.0045, -0.0004, -0.0163]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0054,  0.0019, -0.0096,  ...,  0.0079, -0.0290,  0.0214],\n",
       "                      [ 0.0104, -0.0046, -0.0208,  ..., -0.0177, -0.0062,  0.0544],\n",
       "                      [-0.0194, -0.0038, -0.0327,  ...,  0.0260, -0.0005, -0.0182],\n",
       "                      ...,\n",
       "                      [-0.0191, -0.0126, -0.0129,  ...,  0.0212,  0.0574, -0.0024],\n",
       "                      [ 0.0105,  0.0242,  0.0280,  ...,  0.0069,  0.0114, -0.0044],\n",
       "                      [ 0.0209,  0.0021, -0.0018,  ...,  0.0123,  0.0245, -0.0322]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.up_proj.weight',\n",
       "              tensor([[-3.4882e-02,  2.3178e-02,  7.5150e-03,  ...,  2.5299e-02,\n",
       "                       -2.3331e-02,  4.6372e-05],\n",
       "                      [ 1.2421e-02,  5.6572e-03, -1.1177e-02,  ...,  8.6746e-03,\n",
       "                       -1.7960e-02,  9.0637e-03],\n",
       "                      [ 3.2654e-02, -6.6795e-03, -2.1103e-02,  ..., -9.7322e-04,\n",
       "                        4.2992e-03,  1.5320e-02],\n",
       "                      ...,\n",
       "                      [ 1.0040e-02, -3.0518e-02, -1.2222e-02,  ...,  5.1849e-02,\n",
       "                       -2.3972e-02, -4.9019e-03],\n",
       "                      [-1.8265e-02,  1.9882e-02, -4.3845e-04,  ...,  1.7715e-02,\n",
       "                        1.2039e-02,  2.1332e-02],\n",
       "                      [ 1.6830e-02,  3.1708e-02, -1.1444e-02,  ..., -2.7908e-02,\n",
       "                        2.3880e-02,  8.8577e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0056, -0.0293,  0.0012,  ...,  0.0127,  0.0023,  0.0293],\n",
       "                      [ 0.0401,  0.0068,  0.0182,  ...,  0.0295,  0.0514, -0.0017],\n",
       "                      [ 0.0108,  0.0193,  0.0317,  ..., -0.0469,  0.0123, -0.0033],\n",
       "                      ...,\n",
       "                      [ 0.0093,  0.0184,  0.0085,  ...,  0.0135, -0.0222,  0.0237],\n",
       "                      [ 0.0201, -0.0325, -0.0068,  ...,  0.0490,  0.0073, -0.0015],\n",
       "                      [-0.0127, -0.0023,  0.0222,  ...,  0.0139, -0.0247,  0.0026]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.22.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0041,  0.0193,  0.0008,  ...,  0.0312,  0.0147, -0.0473],\n",
       "                      [-0.0050, -0.0373, -0.0031,  ..., -0.0137, -0.0032,  0.0386],\n",
       "                      [-0.0049, -0.0053, -0.0013,  ...,  0.0086,  0.0162,  0.0046],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0040, -0.0149,  ...,  0.0272,  0.0269,  0.0193],\n",
       "                      [-0.0015,  0.0201, -0.0123,  ..., -0.0006,  0.0125,  0.0252],\n",
       "                      [-0.0079,  0.0197, -0.0047,  ..., -0.0241,  0.0400,  0.0152]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.k_proj.weight',\n",
       "              tensor([[-1.1654e-03,  9.5978e-03, -6.2561e-03,  ...,  2.9053e-02,\n",
       "                        2.0004e-02, -3.5515e-03],\n",
       "                      [ 1.0643e-02, -9.2926e-03,  3.8666e-02,  ...,  1.2197e-03,\n",
       "                        1.0834e-03, -1.2894e-02],\n",
       "                      [ 1.5053e-02, -1.2733e-02, -3.1531e-05,  ...,  2.3972e-02,\n",
       "                       -1.2558e-02,  2.1347e-02],\n",
       "                      ...,\n",
       "                      [ 6.7863e-03,  6.1111e-03, -2.5024e-02,  ...,  1.7868e-02,\n",
       "                       -2.5085e-02,  2.0340e-02],\n",
       "                      [-9.7504e-03,  1.8127e-02,  5.5351e-03,  ...,  3.6583e-03,\n",
       "                        2.9526e-03,  1.3527e-02],\n",
       "                      [ 5.7411e-03, -2.1629e-03,  2.1935e-03,  ..., -3.5706e-03,\n",
       "                       -9.8114e-03,  6.4774e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.v_proj.weight',\n",
       "              tensor([[ 0.0088,  0.0231,  0.0398,  ...,  0.0233,  0.0221,  0.0173],\n",
       "                      [-0.0155,  0.0054, -0.0359,  ..., -0.0125, -0.0138,  0.0217],\n",
       "                      [-0.0195, -0.0135,  0.0135,  ..., -0.0395,  0.0336, -0.0043],\n",
       "                      ...,\n",
       "                      [-0.0164,  0.0014,  0.0332,  ..., -0.0009,  0.0345,  0.0365],\n",
       "                      [ 0.0097, -0.0212, -0.0310,  ...,  0.0023,  0.0076, -0.0267],\n",
       "                      [ 0.0037, -0.0112, -0.0398,  ...,  0.0046,  0.0085,  0.0007]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.o_proj.weight',\n",
       "              tensor([[ 5.0640e-04, -3.8300e-02,  2.3987e-02,  ..., -4.5868e-02,\n",
       "                        3.1223e-03,  1.9638e-02],\n",
       "                      [ 3.0914e-02, -1.4183e-02, -2.6138e-02,  ..., -2.7237e-03,\n",
       "                        4.1542e-03,  9.5139e-03],\n",
       "                      [-2.1942e-02,  3.3712e-04, -8.8501e-03,  ...,  4.0710e-02,\n",
       "                        1.8036e-02, -2.1042e-02],\n",
       "                      ...,\n",
       "                      [ 6.1340e-03, -2.2491e-02,  2.4200e-02,  ..., -1.9684e-02,\n",
       "                        3.2349e-03, -3.4466e-03],\n",
       "                      [-9.5901e-03, -4.3831e-03, -2.9877e-02,  ...,  1.3596e-02,\n",
       "                        3.0701e-02,  4.7913e-02],\n",
       "                      [ 2.0081e-02,  1.6571e-02,  1.3695e-02,  ...,  1.3832e-02,\n",
       "                       -1.2993e-02, -6.9141e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.gate_proj.weight',\n",
       "              tensor([[ 0.0326, -0.0271,  0.0197,  ...,  0.0075, -0.0075,  0.0027],\n",
       "                      [-0.0023,  0.0334,  0.0088,  ...,  0.0428, -0.0147, -0.0185],\n",
       "                      [-0.0172,  0.0402,  0.0096,  ..., -0.0145, -0.0132,  0.0472],\n",
       "                      ...,\n",
       "                      [-0.0042, -0.0107,  0.0034,  ...,  0.0109,  0.0113,  0.0567],\n",
       "                      [ 0.0382, -0.0181,  0.0124,  ...,  0.0179,  0.0085, -0.0170],\n",
       "                      [ 0.0292,  0.0172,  0.0069,  ...,  0.0181,  0.0013,  0.0045]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.up_proj.weight',\n",
       "              tensor([[-0.0428, -0.0182, -0.0264,  ..., -0.0066,  0.0250, -0.0168],\n",
       "                      [ 0.0323,  0.0080,  0.0044,  ...,  0.0154, -0.0111,  0.0281],\n",
       "                      [-0.0125, -0.0242,  0.0179,  ...,  0.0139, -0.0169,  0.0275],\n",
       "                      ...,\n",
       "                      [-0.0297, -0.0026,  0.0017,  ...,  0.0005,  0.0214,  0.0028],\n",
       "                      [-0.0090, -0.0190, -0.0296,  ...,  0.0237, -0.0202, -0.0391],\n",
       "                      [ 0.0057, -0.0074,  0.0106,  ...,  0.0445,  0.0229, -0.0160]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.down_proj.weight',\n",
       "              tensor([[-0.0161,  0.0154,  0.0132,  ..., -0.0309,  0.0082,  0.0307],\n",
       "                      [-0.0198, -0.0149, -0.0012,  ...,  0.0260,  0.0155,  0.0059],\n",
       "                      [-0.0605,  0.0022,  0.0065,  ..., -0.0157, -0.0314, -0.0020],\n",
       "                      ...,\n",
       "                      [-0.0231, -0.0278,  0.0051,  ...,  0.0283,  0.0093,  0.0045],\n",
       "                      [-0.0048, -0.0118, -0.0124,  ...,  0.0099,  0.0307, -0.0009],\n",
       "                      [ 0.0220, -0.0111, -0.0168,  ..., -0.0126, -0.0110, -0.0139]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.23.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0020, -0.0047,  0.0333,  ...,  0.0044, -0.0217,  0.0046],\n",
       "                      [-0.0258,  0.0114, -0.0446,  ..., -0.0147,  0.0043, -0.0034],\n",
       "                      [ 0.0021,  0.0052, -0.0588,  ..., -0.0515,  0.0154, -0.0260],\n",
       "                      ...,\n",
       "                      [ 0.0020,  0.0131, -0.0219,  ...,  0.0044,  0.0323, -0.0126],\n",
       "                      [ 0.0125,  0.0041,  0.0213,  ...,  0.0490, -0.0006, -0.0226],\n",
       "                      [-0.0251, -0.0420, -0.0020,  ...,  0.0360, -0.0100,  0.0115]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.2596e-02,  1.1909e-02,  1.9714e-02,  ..., -2.8656e-02,\n",
       "                        2.6657e-02,  3.4241e-02],\n",
       "                      [ 2.6077e-02,  3.8223e-03,  5.3310e-04,  ...,  1.0315e-02,\n",
       "                       -1.2527e-02,  3.9948e-02],\n",
       "                      [-1.1101e-02,  3.1525e-02, -2.7603e-02,  ..., -3.5477e-03,\n",
       "                       -3.5767e-02,  1.6510e-02],\n",
       "                      ...,\n",
       "                      [ 6.9008e-03,  2.0660e-02, -2.1629e-03,  ...,  4.9305e-04,\n",
       "                        2.9831e-02, -1.2611e-02],\n",
       "                      [ 2.6794e-02, -3.0716e-02, -3.5763e-07,  ..., -1.5762e-02,\n",
       "                       -1.3367e-02,  3.3752e-02],\n",
       "                      [-6.0616e-03, -6.6719e-03, -1.4343e-02,  ..., -3.0258e-02,\n",
       "                       -8.3466e-03, -7.1831e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0053,  0.0176, -0.0111,  ...,  0.0004,  0.0147,  0.0150],\n",
       "                      [-0.0035, -0.0300,  0.0170,  ...,  0.0297, -0.0199,  0.0403],\n",
       "                      [ 0.0078,  0.0007,  0.0118,  ..., -0.0368,  0.0204,  0.0154],\n",
       "                      ...,\n",
       "                      [ 0.0052, -0.0183, -0.0017,  ..., -0.0113,  0.0074, -0.0349],\n",
       "                      [ 0.0044, -0.0059,  0.0120,  ..., -0.0108,  0.0047, -0.0071],\n",
       "                      [-0.0066,  0.0064,  0.0015,  ..., -0.0620, -0.0028,  0.0109]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.o_proj.weight',\n",
       "              tensor([[-0.0182, -0.0303,  0.0081,  ..., -0.0182,  0.0152,  0.0124],\n",
       "                      [ 0.0066, -0.0146, -0.0234,  ..., -0.0023, -0.0310, -0.0060],\n",
       "                      [ 0.0399, -0.0146, -0.0200,  ...,  0.0159,  0.0087,  0.0326],\n",
       "                      ...,\n",
       "                      [ 0.0186, -0.0172,  0.0021,  ...,  0.0154, -0.0268,  0.0464],\n",
       "                      [-0.0002, -0.0015, -0.0086,  ...,  0.0208,  0.0107, -0.0227],\n",
       "                      [ 0.0164, -0.0234, -0.0168,  ..., -0.0156,  0.0053, -0.0104]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0033,  0.0122,  0.0169,  ...,  0.0032, -0.0003, -0.0120],\n",
       "                      [ 0.0187,  0.0279,  0.0041,  ...,  0.0347, -0.0213,  0.0403],\n",
       "                      [-0.0041,  0.0230,  0.0083,  ..., -0.0048,  0.0012,  0.0062],\n",
       "                      ...,\n",
       "                      [-0.0068, -0.0065, -0.0253,  ...,  0.0096,  0.0213, -0.0002],\n",
       "                      [-0.0273,  0.0023, -0.0138,  ..., -0.0012,  0.0174, -0.0101],\n",
       "                      [-0.0126, -0.0415, -0.0486,  ..., -0.0278,  0.0092, -0.0143]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.up_proj.weight',\n",
       "              tensor([[-0.0051,  0.0120, -0.0102,  ..., -0.0354, -0.0074, -0.0215],\n",
       "                      [ 0.0024,  0.0040, -0.0144,  ...,  0.0052, -0.0367,  0.0202],\n",
       "                      [-0.0235, -0.0270, -0.0296,  ...,  0.0052,  0.0126, -0.0042],\n",
       "                      ...,\n",
       "                      [ 0.0074,  0.0095, -0.0025,  ..., -0.0118,  0.0330, -0.0088],\n",
       "                      [ 0.0293,  0.0173,  0.0267,  ..., -0.0291,  0.0018,  0.0180],\n",
       "                      [ 0.0279, -0.0278,  0.0197,  ..., -0.0169,  0.0087,  0.0371]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0408, -0.0163, -0.0195,  ...,  0.0112, -0.0163,  0.0465],\n",
       "                      [ 0.0025,  0.0065, -0.0306,  ..., -0.0264,  0.0002,  0.0083],\n",
       "                      [-0.0250, -0.0016, -0.0095,  ..., -0.0020,  0.0023, -0.0163],\n",
       "                      ...,\n",
       "                      [ 0.0146, -0.0059, -0.0116,  ..., -0.0013, -0.0129, -0.0050],\n",
       "                      [-0.0298, -0.0052,  0.0144,  ...,  0.0067,  0.0032, -0.0001],\n",
       "                      [-0.0021,  0.0133, -0.0191,  ...,  0.0362,  0.0050, -0.0206]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.24.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0014, -0.0020, -0.0020,  ..., -0.0081,  0.0017,  0.0045],\n",
       "                      [-0.0117,  0.0020,  0.0313,  ..., -0.0009, -0.0251,  0.0237],\n",
       "                      [-0.0413,  0.0025, -0.0283,  ..., -0.0021,  0.0219, -0.0014],\n",
       "                      ...,\n",
       "                      [ 0.0236, -0.0044, -0.0037,  ..., -0.0037, -0.0265, -0.0023],\n",
       "                      [-0.0334, -0.0235,  0.0014,  ..., -0.0195, -0.0115, -0.0341],\n",
       "                      [ 0.0074, -0.0063, -0.0055,  ...,  0.0108,  0.0102, -0.0109]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.k_proj.weight',\n",
       "              tensor([[ 1.3680e-02, -1.5915e-02,  1.0979e-02,  ..., -5.9357e-03,\n",
       "                       -2.4902e-02, -4.4220e-02],\n",
       "                      [-4.1084e-03,  3.5675e-02,  2.2202e-02,  ..., -7.0114e-03,\n",
       "                        2.3441e-03, -2.2461e-02],\n",
       "                      [-2.7176e-02, -6.0234e-03, -3.7781e-02,  ...,  2.6749e-02,\n",
       "                        1.8112e-02, -5.7716e-03],\n",
       "                      ...,\n",
       "                      [ 1.5327e-02, -2.9709e-02,  4.5395e-03,  ...,  6.4373e-06,\n",
       "                        1.0138e-03, -9.9258e-03],\n",
       "                      [ 1.3329e-02,  5.6000e-03, -1.8143e-02,  ...,  1.1398e-02,\n",
       "                       -3.4485e-02, -1.3542e-02],\n",
       "                      [ 1.1391e-02,  2.1439e-02,  1.0231e-02,  ...,  2.2411e-03,\n",
       "                        3.1090e-03,  8.2245e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0077, -0.0440, -0.0054,  ...,  0.0080,  0.0208, -0.0068],\n",
       "                      [-0.0093,  0.0473,  0.0136,  ...,  0.0220, -0.0037, -0.0007],\n",
       "                      [ 0.0106, -0.0673,  0.0192,  ..., -0.0004, -0.0188,  0.0341],\n",
       "                      ...,\n",
       "                      [ 0.0345,  0.0082,  0.0084,  ..., -0.0106, -0.0006,  0.0059],\n",
       "                      [-0.0462, -0.0328,  0.0016,  ..., -0.0323,  0.0121, -0.0062],\n",
       "                      [ 0.0237, -0.0433,  0.0389,  ..., -0.0116, -0.0148, -0.0005]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0005, -0.0009, -0.0135,  ..., -0.0115,  0.0058,  0.0073],\n",
       "                      [-0.0114, -0.0213, -0.0157,  ...,  0.0208, -0.0235,  0.0265],\n",
       "                      [ 0.0120, -0.0038,  0.0312,  ..., -0.0041,  0.0204,  0.0185],\n",
       "                      ...,\n",
       "                      [ 0.0605, -0.0023, -0.0188,  ..., -0.0237, -0.0020,  0.0678],\n",
       "                      [-0.0047, -0.0090, -0.0173,  ...,  0.0076,  0.0199,  0.0086],\n",
       "                      [ 0.0086, -0.0067, -0.0277,  ...,  0.0117, -0.0377,  0.0029]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0222,  0.0243, -0.0240,  ...,  0.0135,  0.0309, -0.0172],\n",
       "                      [ 0.0201, -0.0030, -0.0393,  ..., -0.0123,  0.0551, -0.0316],\n",
       "                      [ 0.0064,  0.0003, -0.0165,  ..., -0.0117,  0.0046, -0.0462],\n",
       "                      ...,\n",
       "                      [ 0.0084, -0.0168,  0.0181,  ..., -0.0257, -0.0292, -0.0042],\n",
       "                      [-0.0055, -0.0275,  0.0057,  ...,  0.0211, -0.0164,  0.0455],\n",
       "                      [-0.0308, -0.0016, -0.0114,  ..., -0.0045, -0.0190, -0.0035]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0260, -0.0196, -0.0031,  ...,  0.0013, -0.0124,  0.0108],\n",
       "                      [-0.0056, -0.0449,  0.0098,  ..., -0.0129, -0.0233, -0.0035],\n",
       "                      [ 0.0282,  0.0179,  0.0015,  ...,  0.0064, -0.0049, -0.0198],\n",
       "                      ...,\n",
       "                      [-0.0038,  0.0181, -0.0033,  ...,  0.0318, -0.0097,  0.0147],\n",
       "                      [-0.0093,  0.0013, -0.0160,  ..., -0.0205, -0.0183, -0.0135],\n",
       "                      [ 0.0022,  0.0025,  0.0059,  ...,  0.0271,  0.0156, -0.0112]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0105,  0.0143, -0.0156,  ...,  0.0045, -0.0154, -0.0294],\n",
       "                      [-0.0164,  0.0171,  0.0143,  ..., -0.0125, -0.0141,  0.0199],\n",
       "                      [-0.0068,  0.0134,  0.0007,  ...,  0.0087,  0.0287,  0.0267],\n",
       "                      ...,\n",
       "                      [ 0.0204, -0.0020, -0.0067,  ...,  0.0372, -0.0131, -0.0174],\n",
       "                      [-0.0037,  0.0039,  0.0184,  ...,  0.0230,  0.0127,  0.0146],\n",
       "                      [-0.0283, -0.0173,  0.0172,  ...,  0.0161, -0.0137, -0.0190]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.25.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.q_proj.weight',\n",
       "              tensor([[-0.0097,  0.0003,  0.0012,  ..., -0.0172,  0.0050, -0.0187],\n",
       "                      [-0.0139, -0.0143,  0.0086,  ..., -0.0290,  0.0110, -0.0299],\n",
       "                      [ 0.0274, -0.0244,  0.0134,  ...,  0.0095, -0.0027, -0.0141],\n",
       "                      ...,\n",
       "                      [ 0.0016,  0.0340,  0.0457,  ..., -0.0040,  0.0073, -0.0067],\n",
       "                      [-0.0125,  0.0212,  0.0241,  ..., -0.0241,  0.0190,  0.0237],\n",
       "                      [ 0.0086, -0.0162,  0.0325,  ...,  0.0004,  0.0198, -0.0208]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.k_proj.weight',\n",
       "              tensor([[ 0.0185,  0.0081,  0.0043,  ..., -0.0156,  0.0067,  0.0087],\n",
       "                      [-0.0236, -0.0143,  0.0226,  ..., -0.0471, -0.0086,  0.0071],\n",
       "                      [ 0.0158, -0.0157, -0.0247,  ...,  0.0005, -0.0246, -0.0342],\n",
       "                      ...,\n",
       "                      [-0.0136,  0.0038,  0.0236,  ...,  0.0082, -0.0177, -0.0038],\n",
       "                      [ 0.0415, -0.0256,  0.0125,  ..., -0.0086,  0.0007,  0.0098],\n",
       "                      [-0.0208, -0.0103,  0.0182,  ..., -0.0132,  0.0185, -0.0289]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.v_proj.weight',\n",
       "              tensor([[-0.0077, -0.0013,  0.0030,  ...,  0.0139,  0.0467,  0.0181],\n",
       "                      [ 0.0138, -0.0214, -0.0271,  ..., -0.0044, -0.0112, -0.0074],\n",
       "                      [ 0.0125,  0.0193, -0.0003,  ...,  0.0100, -0.0401, -0.0508],\n",
       "                      ...,\n",
       "                      [-0.0224, -0.0027, -0.0407,  ..., -0.0335, -0.0246,  0.0198],\n",
       "                      [ 0.0052, -0.0106,  0.0259,  ...,  0.0044, -0.0143,  0.0114],\n",
       "                      [ 0.0093,  0.0070,  0.0141,  ..., -0.0137, -0.0534, -0.0013]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.o_proj.weight',\n",
       "              tensor([[ 1.6754e-02, -2.9621e-03, -7.4425e-03,  ..., -6.0463e-03,\n",
       "                       -2.1271e-02,  4.4373e-02],\n",
       "                      [-3.3600e-02, -4.3726e-04, -4.6356e-02,  ...,  9.0256e-03,\n",
       "                       -2.2018e-02,  5.4817e-03],\n",
       "                      [-1.7349e-02, -1.2159e-03, -1.0185e-02,  ..., -2.2629e-02,\n",
       "                        2.0889e-02, -1.2337e-02],\n",
       "                      ...,\n",
       "                      [-1.0025e-02, -2.7599e-03,  1.0133e-05,  ...,  1.0735e-02,\n",
       "                       -1.7700e-03,  2.3605e-02],\n",
       "                      [-3.1311e-02, -1.7166e-02, -7.5302e-03,  ..., -1.4259e-02,\n",
       "                       -1.0979e-02, -1.0918e-02],\n",
       "                      [-1.1765e-02,  2.5497e-02, -4.8676e-03,  ...,  2.0187e-02,\n",
       "                        2.4557e-04, -1.8906e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0118,  0.0098, -0.0052,  ..., -0.0003, -0.0392, -0.0597],\n",
       "                      [ 0.0277, -0.0063,  0.0494,  ...,  0.0123, -0.0307,  0.0159],\n",
       "                      [ 0.0072,  0.0181, -0.0083,  ...,  0.0040,  0.0128, -0.0295],\n",
       "                      ...,\n",
       "                      [-0.0001,  0.0192,  0.0082,  ..., -0.0249, -0.0112, -0.0110],\n",
       "                      [ 0.0050, -0.0070,  0.0034,  ...,  0.0192,  0.0085,  0.0008],\n",
       "                      [-0.0174, -0.0137, -0.0145,  ..., -0.0032,  0.0050,  0.0310]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.up_proj.weight',\n",
       "              tensor([[ 0.0516, -0.0016, -0.0011,  ...,  0.0084, -0.0075,  0.0077],\n",
       "                      [-0.0161,  0.0058,  0.0047,  ..., -0.0120,  0.0129, -0.0063],\n",
       "                      [ 0.0198,  0.0225,  0.0146,  ..., -0.0081, -0.0103, -0.0004],\n",
       "                      ...,\n",
       "                      [ 0.0236, -0.0075,  0.0012,  ...,  0.0235,  0.0040, -0.0117],\n",
       "                      [ 0.0188, -0.0190,  0.0168,  ...,  0.0287,  0.0082,  0.0150],\n",
       "                      [-0.0031, -0.0147, -0.0127,  ..., -0.0177,  0.0280,  0.0354]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.down_proj.weight',\n",
       "              tensor([[ 0.0257, -0.0311,  0.0177,  ...,  0.0198,  0.0105,  0.0258],\n",
       "                      [-0.0031,  0.0087,  0.0179,  ..., -0.0113,  0.0051,  0.0301],\n",
       "                      [-0.0061, -0.0127,  0.0002,  ...,  0.0101, -0.0228,  0.0069],\n",
       "                      ...,\n",
       "                      [-0.0033, -0.0174, -0.0056,  ..., -0.0226,  0.0050, -0.0070],\n",
       "                      [ 0.0242, -0.0091,  0.0316,  ...,  0.0082,  0.0118, -0.0317],\n",
       "                      [-0.0284, -0.0026,  0.0235,  ..., -0.0136, -0.0020, -0.0032]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.26.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.q_proj.weight',\n",
       "              tensor([[ 0.0240,  0.0248,  0.0071,  ...,  0.0425,  0.0348,  0.0055],\n",
       "                      [-0.0038, -0.0022,  0.0098,  ..., -0.0091,  0.0178, -0.0076],\n",
       "                      [-0.0046, -0.0084,  0.0073,  ...,  0.0077,  0.0325,  0.0235],\n",
       "                      ...,\n",
       "                      [ 0.0043, -0.0016, -0.0042,  ...,  0.0250,  0.0261, -0.0064],\n",
       "                      [ 0.0096, -0.0149, -0.0005,  ...,  0.0242, -0.0211, -0.0125],\n",
       "                      [-0.0141, -0.0050, -0.0298,  ..., -0.0473,  0.0440, -0.0026]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.k_proj.weight',\n",
       "              tensor([[-0.0041,  0.0106, -0.0020,  ..., -0.0124, -0.0063,  0.0108],\n",
       "                      [-0.0093, -0.0139,  0.0026,  ..., -0.0237, -0.0301, -0.0130],\n",
       "                      [ 0.0016, -0.0112, -0.0214,  ...,  0.0257, -0.0223,  0.0286],\n",
       "                      ...,\n",
       "                      [ 0.0009, -0.0057, -0.0160,  ..., -0.0232, -0.0431,  0.0151],\n",
       "                      [ 0.0068, -0.0137, -0.0282,  ..., -0.0115,  0.0226,  0.0410],\n",
       "                      [-0.0051,  0.0254, -0.0271,  ..., -0.0002,  0.0244,  0.0125]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.v_proj.weight',\n",
       "              tensor([[-5.0850e-03,  1.8036e-02,  3.5614e-02,  ...,  1.5900e-02,\n",
       "                        2.0050e-02, -6.4545e-03],\n",
       "                      [ 1.5884e-02,  3.0060e-02,  1.3390e-03,  ..., -3.9597e-03,\n",
       "                       -1.3590e-03, -2.7832e-02],\n",
       "                      [ 1.4114e-02,  1.3185e-04, -1.6129e-02,  ..., -2.1179e-02,\n",
       "                       -1.9043e-02,  1.7029e-02],\n",
       "                      ...,\n",
       "                      [ 1.2405e-02, -6.1874e-03, -2.6215e-02,  ..., -1.0742e-02,\n",
       "                       -1.1055e-02,  7.9811e-05],\n",
       "                      [ 1.0880e-02, -9.7733e-03,  1.0536e-02,  ..., -2.4399e-02,\n",
       "                       -2.2766e-02, -1.1841e-02],\n",
       "                      [ 1.1589e-02, -1.5480e-02, -1.4015e-02,  ..., -6.8436e-03,\n",
       "                       -2.1207e-04,  1.4664e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.o_proj.weight',\n",
       "              tensor([[ 0.0316,  0.0205, -0.0089,  ...,  0.0116,  0.0259, -0.0026],\n",
       "                      [-0.0065, -0.0203, -0.0323,  ..., -0.0190, -0.0109,  0.0307],\n",
       "                      [-0.0213,  0.0020,  0.0061,  ...,  0.0182,  0.0358,  0.0122],\n",
       "                      ...,\n",
       "                      [ 0.0067, -0.0057,  0.0133,  ...,  0.0120, -0.0059,  0.0215],\n",
       "                      [ 0.0177,  0.0533,  0.0150,  ...,  0.0152,  0.0031, -0.0113],\n",
       "                      [-0.0170, -0.0468, -0.0829,  ..., -0.0143, -0.0448, -0.0065]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.gate_proj.weight',\n",
       "              tensor([[-0.0117, -0.0346,  0.0134,  ...,  0.0240,  0.0310, -0.0169],\n",
       "                      [ 0.0203,  0.0047, -0.0103,  ..., -0.0172,  0.0303, -0.0219],\n",
       "                      [-0.0188, -0.0113,  0.0520,  ...,  0.0073,  0.0032,  0.0101],\n",
       "                      ...,\n",
       "                      [-0.0059,  0.0011, -0.0290,  ...,  0.0248,  0.0030,  0.0026],\n",
       "                      [ 0.0165, -0.0023,  0.0162,  ...,  0.0074, -0.0030, -0.0038],\n",
       "                      [-0.0078,  0.0134, -0.0154,  ...,  0.0030,  0.0197, -0.0119]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.up_proj.weight',\n",
       "              tensor([[-1.3008e-02,  1.0536e-02, -2.9358e-02,  ...,  1.9897e-02,\n",
       "                       -3.5324e-03,  7.8506e-03],\n",
       "                      [ 2.0020e-02,  1.7502e-02, -1.3161e-02,  ..., -1.8673e-03,\n",
       "                        1.0864e-02,  1.1597e-02],\n",
       "                      [-3.1708e-02,  4.4952e-02,  1.0201e-02,  ...,  5.1727e-02,\n",
       "                        2.6062e-02, -3.9459e-02],\n",
       "                      ...,\n",
       "                      [-3.3203e-02, -1.5259e-05, -1.8860e-02,  ..., -9.2545e-03,\n",
       "                        3.2501e-03,  1.1772e-02],\n",
       "                      [-1.7868e-02,  1.4519e-02, -5.7068e-03,  ..., -1.8646e-02,\n",
       "                       -3.9597e-03,  2.3632e-03],\n",
       "                      [ 3.5919e-02,  4.5166e-02, -2.7069e-02,  ...,  3.0670e-02,\n",
       "                        2.3239e-02, -2.1759e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.down_proj.weight',\n",
       "              tensor([[-0.0032,  0.0009,  0.0032,  ...,  0.0115,  0.0145, -0.0125],\n",
       "                      [ 0.0044,  0.0172, -0.0207,  ..., -0.0195,  0.0094, -0.0060],\n",
       "                      [ 0.0058, -0.0325,  0.0101,  ...,  0.0039, -0.0068,  0.0032],\n",
       "                      ...,\n",
       "                      [-0.0047, -0.0209,  0.0335,  ...,  0.0060,  0.0022, -0.0073],\n",
       "                      [-0.0134, -0.0443,  0.0005,  ...,  0.0029, -0.0226,  0.0189],\n",
       "                      [-0.0129,  0.0247, -0.0130,  ...,  0.0146, -0.0024,  0.0452]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.input_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.27.post_attention_layernorm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.norm.weight',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('score.0.weight',\n",
       "              tensor([[-0.0153,  0.0210,  0.0081,  ...,  0.0084,  0.0273, -0.0172],\n",
       "                      [ 0.0421,  0.0087,  0.0286,  ...,  0.0347,  0.0292,  0.0057],\n",
       "                      [ 0.0094,  0.0507,  0.0385,  ...,  0.0156, -0.0120, -0.0367],\n",
       "                      ...,\n",
       "                      [-0.0075,  0.0128, -0.0320,  ...,  0.0006,  0.0100,  0.0066],\n",
       "                      [-0.0174, -0.0557,  0.0177,  ..., -0.0142,  0.0043, -0.0300],\n",
       "                      [-0.0039,  0.0131,  0.0046,  ..., -0.0025,  0.0233,  0.0319]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.0.bias',\n",
       "              tensor([-0.0314,  0.0136,  0.0528,  ...,  0.0363, -0.0226, -0.0289],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.2.weight',\n",
       "              tensor([[ 0.0064,  0.0036, -0.0275,  ..., -0.0108,  0.0107, -0.0027],\n",
       "                      [ 0.0043,  0.0058,  0.0253,  ..., -0.0009,  0.0017, -0.0096]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.2.bias',\n",
       "              tensor([-0.0677,  0.0637], dtype=torch.float16))])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_state_dict = torch.load(f\"/home/it/environments/Genety/models/{model_name}/{today}_{model_name}.pth\", mmap=True)\n",
    "loaded_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "36649d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3aa2a9a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9f78db46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(loaded_state_dict, assign=True, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b3a496a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"./models/{model_name}/{today}_{model_name}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a49ff7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Genety (conda)",
   "language": "python",
   "name": "genety-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
