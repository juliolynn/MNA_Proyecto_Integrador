{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "682cc4d4-ebb1-4e48-b84e-1e62006da233",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f61331a7-f1dd-442e-b58a-a5adb337fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873ec5c8-c16a-4697-86b5-e22292d3266a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:55:40,277] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_symbind_alt@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__nptl_change_stack_perm@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_find_dso_for_object@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_fatal_printf@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_exception_create@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `__tunable_get_val@GLIBC_PRIVATE'\n",
      "/home/it/anaconda3/envs/genety/compiler_compat/ld: /lib/x86_64-linux-gnu/libc.so.6: undefined reference to `_dl_audit_preinit@GLIBC_PRIVATE'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.3.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# DeepSpeed ZeRO-3\n",
    "import deepspeed\n",
    "from deepspeed.accelerator import get_accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd5b450-0317-4c55-847e-7723042940f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e0d556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free(GB): 23.25750732421875, Global(GB): 23.64971923828125, Free(%): 0.9834157898404292\n"
     ]
    }
   ],
   "source": [
    "(free_memory, global_memory) = torch.cuda.mem_get_info()\n",
    "print(f\"Free(GB): {free_memory/1024/1024/1024}, Global(GB): {global_memory/1024/1024/1024}, Free(%): {free_memory/global_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c439d2-2280-4823-90dd-0867012b907c",
   "metadata": {},
   "source": [
    "# Load Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a715f751-ab65-45a3-ae17-c795b8c78d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Identity</th>\n",
       "      <th>Text</th>\n",
       "      <th>A2-Unambiguous</th>\n",
       "      <th>A4-Tolerances</th>\n",
       "      <th>A5-Sources specified</th>\n",
       "      <th>E1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GSS_CONNECTIVITY</td>\n",
       "      <td>SRD_GSS_FUNC_61</td>\n",
       "      <td>The User and Rights Administration HMI shall p...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR2146</td>\n",
       "      <td>The Network Function shall support WiFi 802.11...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR797</td>\n",
       "      <td>The PwrCon software shall monitor the output v...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR3013</td>\n",
       "      <td>When prompted, the TETRA Software shall place ...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cobham_ATR</td>\n",
       "      <td>SHLR-ATR3198</td>\n",
       "      <td>The TETRA software shall allow users to select...</td>\n",
       "      <td>1</td>\n",
       "      <td>na</td>\n",
       "      <td>na</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Type         Identity  \\\n",
       "0  GSS_CONNECTIVITY  SRD_GSS_FUNC_61   \n",
       "1        Cobham_ATR     SHLR-ATR2146   \n",
       "2        Cobham_ATR      SHLR-ATR797   \n",
       "3        Cobham_ATR     SHLR-ATR3013   \n",
       "4        Cobham_ATR     SHLR-ATR3198   \n",
       "\n",
       "                                                Text A2-Unambiguous  \\\n",
       "0  The User and Rights Administration HMI shall p...              1   \n",
       "1  The Network Function shall support WiFi 802.11...              1   \n",
       "2  The PwrCon software shall monitor the output v...              1   \n",
       "3  When prompted, the TETRA Software shall place ...              1   \n",
       "4  The TETRA software shall allow users to select...              1   \n",
       "\n",
       "  A4-Tolerances A5-Sources specified  E1  \n",
       "0            na                   na   1  \n",
       "1            na                   na   1  \n",
       "2            na                   na   1  \n",
       "3            na                   na   1  \n",
       "4            na                   na   1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "master_df = pd.read_excel('./DATASETS/Training_Dataset.xlsx')\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70bdf637-9e84-4cee-a95c-e5c7bd8075d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E1</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The User and Rights Administration HMI shall p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The Network Function shall support WiFi 802.11...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The PwrCon software shall monitor the output v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>When prompted, the TETRA Software shall place ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The TETRA software shall allow users to select...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   E1                                               Text\n",
       "0   1  The User and Rights Administration HMI shall p...\n",
       "1   1  The Network Function shall support WiFi 802.11...\n",
       "2   1  The PwrCon software shall monitor the output v...\n",
       "3   1  When prompted, the TETRA Software shall place ...\n",
       "4   1  The TETRA software shall allow users to select..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = master_df[['E1','Text']].copy()\n",
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9296d035-a173-4c8e-81d9-a30a5021dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3255 entries, 0 to 3254\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   E1      3255 non-null   int64 \n",
      " 1   Text    3255 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 51.0+ KB\n"
     ]
    }
   ],
   "source": [
    "model_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c738bf70-5be7-4b1e-9d88-1e09f2f497a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2127"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df_label1 = model_df.query('E1 == 1')\n",
    "len(model_df_label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a967f2ce-ce0f-4aa7-8b6e-759011eef775",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1128"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df_label0 = model_df.query('E1 == 0')\n",
    "len(model_df_label0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fceb9f5-cd13-4779-b8db-57ff9cda386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat([model_df_label1[:1500],model_df_label0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7716b-cbb9-40df-8796-9e2718512f6b",
   "metadata": {},
   "source": [
    "# Data process and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4e0084e-559e-46cc-b1a4-892e5e0fcb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection  import train_test_split\n",
    "train_df, test_df = train_test_split(model_df, test_size=0.1, shuffle=True)\n",
    "\n",
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "test_iter = iter(list(test_df.itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6daca09-ad60-40b6-90f2-e91c79aabb0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2365"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6424905-7a3d-4fd5-a22d-f2be29be1f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\",\n",
    "    \"tokenizer\",\n",
    "    \"microsoft/Phi-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc4e1ac2-7bff-4984-8bf5-bde7daceef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeGenTokenizerFast(name_or_path='microsoft/Phi-2', vocab_size=50257, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t50257: AddedToken(\"                               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50258: AddedToken(\"                              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50259: AddedToken(\"                             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50260: AddedToken(\"                            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50261: AddedToken(\"                           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50262: AddedToken(\"                          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50263: AddedToken(\"                         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50264: AddedToken(\"                        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50265: AddedToken(\"                       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50266: AddedToken(\"                      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50267: AddedToken(\"                     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50268: AddedToken(\"                    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50269: AddedToken(\"                   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50270: AddedToken(\"                  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50271: AddedToken(\"                 \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50272: AddedToken(\"                \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50273: AddedToken(\"               \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50274: AddedToken(\"              \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50275: AddedToken(\"             \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50276: AddedToken(\"            \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50277: AddedToken(\"           \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50278: AddedToken(\"          \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50279: AddedToken(\"         \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50280: AddedToken(\"        \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50281: AddedToken(\"       \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50282: AddedToken(\"      \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50283: AddedToken(\"     \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50284: AddedToken(\"    \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50285: AddedToken(\"   \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50286: AddedToken(\"  \", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50287: AddedToken(\"\t\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50288: AddedToken(\"\t\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50289: AddedToken(\"\t\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50290: AddedToken(\"\t\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50291: AddedToken(\"\t\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50292: AddedToken(\"\t\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50293: AddedToken(\"\t\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "\t50294: AddedToken(\"\t\t\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=False),\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24d1986-12ed-45c3-8a18-bd1ada1e9440",
   "metadata": {},
   "source": [
    "# Dataset iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "880ea8d4-bae1-48bc-996e-d352db324265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "test_iter = iter(list(test_df.itertuples(index=False, name=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78c4e14f-9c36-4343-ba96-f05c6eb3196e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " 'When an AAMF_GAMF_XFER_SESSION(IRS_108) with Opcode END(0x6E) message is received the GAMF director shall create a session data available to the GCS API with revision, last published date and type. If the session is already available to the GCS API increment revision, update the published date and add type if is not already added.')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "092203c5-49af-4337-abbd-38851caa6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Get label and text\n",
    "    y, x = list(zip(*batch))\n",
    "\n",
    "    # Create list with indices from tokeniser\n",
    "\n",
    "    encoded_x = tokenizer(x, padding=True, truncation=True)\n",
    "    encoded_x.input_ids = torch.tensor(encoded_x.input_ids).to(device)\n",
    "    encoded_x.attention_mask = torch.tensor(encoded_x.attention_mask).to(device)  \n",
    "    \n",
    "    # Prepare the labels, by subtracting 1 to get them in the range 0-3\n",
    "    return encoded_x, torch.tensor(y, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a81c8d9-de06-4aed-9ff3-0c5f31985d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'When an AAMF_GAMF_XFER_SESSION(IRS_108) with Opcode END(0x6E) message is received the GAMF director shall create a session data available to the GCS API with revision, last published date and type. If the session is already available to the GCS API increment revision, update the published date and add type if is not already added.')\n",
      "(1, 'ITCM shall detect fault in hardwire connection with trailer brake actuator.')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_ids': [[2215, 281, 317, 2390, 37, 62, 38, 2390, 37, 62, 55, 24302, 62, 50, 47621, 7, 4663, 50, 62, 15711, 8, 351, 8670, 8189, 23578, 7, 15, 87, 21, 36, 8, 3275, 318, 2722, 262, 49965, 37, 3437, 2236, 2251, 257, 6246, 1366, 1695, 284, 262, 402, 7902, 7824, 351, 18440, 11, 938, 3199, 3128, 290, 2099, 13, 1002, 262, 6246, 318, 1541, 1695, 284, 262, 402, 7902, 7824, 18703, 18440, 11, 4296, 262, 3199, 3128, 290, 751, 2099, 611, 318, 407, 1541, 2087, 13], [2043, 24187, 2236, 4886, 8046, 287, 1327, 21809, 4637, 351, 12268, 20439, 43840, 1352, 13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]},\n",
       " tensor([1, 1], device='cuda:0'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "first = next(train_iter)\n",
    "second = next(train_iter)\n",
    "\n",
    "print(first)\n",
    "print(second)\n",
    "\n",
    "collate_batch([first, second])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf18db8-7d58-43c0-ac22-d3e225cb2ed1",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d306ecb-d8ef-4389-ab6c-8fb803124ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee45a2e90a554d8e969f4f6411baecf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PhiForSequenceClassification were not initialized from the model checkpoint at microsoft/Phi-2 and are newly initialized: ['model.layers.20.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.bias', 'model.layers.6.self_attn.query_key_value.weight', 'model.layers.30.self_attn.query_key_value.weight', 'model.layers.17.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.bias', 'model.layers.27.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.bias', 'model.layers.21.self_attn.query_key_value.bias', 'model.layers.9.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.bias', 'model.layers.17.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.bias', 'model.layers.25.self_attn.query_key_value.bias', 'model.layers.16.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.weight', 'model.layers.8.self_attn.query_key_value.bias', 'model.layers.22.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.bias', 'model.layers.30.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.weight', 'model.layers.27.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.weight', 'model.layers.18.self_attn.query_key_value.weight', 'model.layers.12.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.weight', 'model.layers.21.self_attn.query_key_value.weight', 'model.layers.6.self_attn.query_key_value.bias', 'model.layers.26.self_attn.query_key_value.weight', 'model.layers.4.self_attn.query_key_value.bias', 'model.layers.10.self_attn.query_key_value.weight', 'model.layers.2.self_attn.query_key_value.weight', 'model.layers.29.self_attn.query_key_value.weight', 'model.layers.9.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.weight', 'model.layers.15.self_attn.query_key_value.bias', 'model.layers.20.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.weight', 'model.layers.5.self_attn.query_key_value.bias', 'model.layers.1.self_attn.query_key_value.bias', 'model.layers.0.self_attn.query_key_value.weight', 'model.layers.3.self_attn.query_key_value.weight', 'model.layers.7.self_attn.query_key_value.bias', 'model.layers.8.self_attn.query_key_value.weight', 'model.layers.31.self_attn.query_key_value.weight', 'model.layers.11.self_attn.query_key_value.weight', 'model.layers.25.self_attn.query_key_value.weight', 'model.layers.19.self_attn.query_key_value.bias', 'model.layers.23.self_attn.query_key_value.weight', 'model.layers.28.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.bias', 'score.weight', 'model.layers.12.self_attn.query_key_value.bias', 'model.layers.11.self_attn.query_key_value.bias', 'model.layers.24.self_attn.query_key_value.bias', 'model.layers.29.self_attn.query_key_value.bias', 'model.layers.31.self_attn.query_key_value.bias', 'model.layers.14.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.weight', 'model.layers.13.self_attn.query_key_value.bias', 'model.layers.19.self_attn.query_key_value.weight', 'model.layers.10.self_attn.query_key_value.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\",\n",
    "    \"modelForSequenceClassification\",\n",
    "    \"microsoft/Phi-2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8ece1275-ff60-4b30-ac14-bc4febfcfdc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForSequenceClassification(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=2560, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e6724c0-3768-48ee-9176-2518cabbfe6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7e98c25-1d44-4a02-997c-8ec0c4e123c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, parameter in enumerate(model.parameters()):\n",
    "    parameter.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "41954657-e0cf-40d1-9143-75e11701c2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score = nn.Sequential(\n",
    "    nn.Linear(in_features=2560, out_features=2560),\n",
    "    nn.Dropout(),\n",
    "    nn.Linear(in_features=2560, out_features=2),\n",
    ")\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d8afdced-aa5f-4dae-9ef0-7183668838d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhiForSequenceClassification(\n",
       "  (model): PhiModel(\n",
       "    (embed_tokens): Embedding(51200, 2560)\n",
       "    (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x PhiDecoderLayer(\n",
       "        (self_attn): PhiAttention(\n",
       "          (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "          (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "          (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (rotary_emb): PhiRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): PhiMLP(\n",
       "          (activation_fn): NewGELUActivation()\n",
       "          (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "          (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "        )\n",
       "        (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Sequential(\n",
       "    (0): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "    (1): Dropout(p=0.5, inplace=False)\n",
       "    (2): Linear(in_features=2560, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1421e4-51bc-490c-b47d-01d83e0a28cf",
   "metadata": {},
   "source": [
    "# Train and eval functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a28ea92-68d0-44a7-9a5d-313ee209533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import time\n",
    "\n",
    "def train(model, dataloader, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 5\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "\n",
    "    for idx, (data, label) in enumerate(dataloader):         \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "        predicted_label = outputs.logits\n",
    "        loss = criterion(predicted_label, label)\n",
    "        \n",
    "        # Deepspeed model engine, backward pass \n",
    "        model.backward(loss)\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        \n",
    "        # Deepspeed model engine, optimizer step\n",
    "        model.step()\n",
    "        \n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Deepspeed model engine, empty cache\n",
    "        model.empty_partition_cache()\n",
    "        \n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "        \n",
    "\n",
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, label) in enumerate(dataloader):      \n",
    "            outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "            predicted_label = outputs.logits\n",
    "            loss = criterion(predicted_label, label)\n",
    "            \n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count, loss.item() / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc0036",
   "metadata": {},
   "source": [
    "# Deepspeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ef3a1130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:55:46,870] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.2, git-hash=unknown, git-branch=unknown\n",
      "[2024-05-08 08:55:46,870] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-05-08 08:55:46,871] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-05-08 08:55:47,147] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.1.1.204, master_port=29500\n",
      "[2024-05-08 08:55:47,148] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:55:48,504] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-05-08 08:55:49,565] [WARNING] [cpu_adam.py:84:__init__] FP16 params for CPUAdam may not work on AMD CPUs\n",
      "Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/it/.cache/torch_extensions/py311_cu121 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/it/.cache/torch_extensions/py311_cu121/cpu_adam/build.ninja...\n",
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module cpu_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "Time to load cpu_adam op: 2.1964612007141113 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module cpu_adam...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:55:52,121] [INFO] [logging.py:96:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2024-05-08 08:55:52,121] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-05-08 08:55:52,127] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
      "[2024-05-08 08:55:52,127] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
      "[2024-05-08 08:55:52,127] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\n",
      "[2024-05-08 08:55:52,127] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.float16 ZeRO stage 3 optimizer\n",
      "[2024-05-08 08:55:52,223] [INFO] [utils.py:779:see_memory_usage] Stage 3 initialize beginning\n",
      "[2024-05-08 08:55:52,224] [INFO] [utils.py:780:see_memory_usage] MA 4.97 GB         Max_MA 4.97 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:52,224] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 2.92 GB, percent = 9.6%\n",
      "[2024-05-08 08:55:52,225] [INFO] [stage3.py:130:__init__] Reduce bucket size 200000000\n",
      "[2024-05-08 08:55:52,225] [INFO] [stage3.py:131:__init__] Prefetch bucket size 0\n",
      "[2024-05-08 08:55:52,302] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\n",
      "[2024-05-08 08:55:52,303] [INFO] [utils.py:780:see_memory_usage] MA 4.97 GB         Max_MA 4.97 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:52,303] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 2.92 GB, percent = 9.6%\n",
      "Parameter Offload: Total persistent parameters: 913922 in 197 params\n",
      "[2024-05-08 08:55:54,048] [INFO] [utils.py:779:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\n",
      "[2024-05-08 08:55:54,049] [INFO] [utils.py:780:see_memory_usage] MA 0.01 GB         Max_MA 4.97 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,063] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.83 GB, percent = 25.6%\n",
      "[2024-05-08 08:55:54,169] [INFO] [utils.py:779:see_memory_usage] Before creating fp16 partitions\n",
      "[2024-05-08 08:55:54,169] [INFO] [utils.py:780:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,170] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.83 GB, percent = 25.6%\n",
      "[2024-05-08 08:55:54,267] [INFO] [utils.py:779:see_memory_usage] After creating fp16 partitions: 1\n",
      "[2024-05-08 08:55:54,267] [INFO] [utils.py:780:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,268] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.85 GB, percent = 25.7%\n",
      "[2024-05-08 08:55:54,349] [INFO] [utils.py:779:see_memory_usage] Before creating fp32 partitions\n",
      "[2024-05-08 08:55:54,350] [INFO] [utils.py:780:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,350] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.85 GB, percent = 25.7%\n",
      "[2024-05-08 08:55:54,444] [INFO] [utils.py:779:see_memory_usage] After creating fp32 partitions\n",
      "[2024-05-08 08:55:54,444] [INFO] [utils.py:780:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,444] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.87 GB, percent = 25.8%\n",
      "[2024-05-08 08:55:54,526] [INFO] [utils.py:779:see_memory_usage] Before initializing optimizer states\n",
      "[2024-05-08 08:55:54,526] [INFO] [utils.py:780:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,527] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.87 GB, percent = 25.8%\n",
      "[2024-05-08 08:55:54,621] [INFO] [utils.py:779:see_memory_usage] After initializing optimizer states\n",
      "[2024-05-08 08:55:54,621] [INFO] [utils.py:780:see_memory_usage] MA 0.01 GB         Max_MA 0.01 GB         CA 5.02 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,622] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.9 GB, percent = 25.8%\n",
      "[2024-05-08 08:55:54,622] [INFO] [stage3.py:486:_setup_for_real_optimizer] optimizer state initialized\n",
      "[2024-05-08 08:55:54,720] [INFO] [utils.py:779:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-05-08 08:55:54,721] [INFO] [utils.py:780:see_memory_usage] MA 0.38 GB         Max_MA 0.41 GB         CA 5.39 GB         Max_CA 5 GB \n",
      "[2024-05-08 08:55:54,721] [INFO] [utils.py:787:see_memory_usage] CPU Virtual Memory:  used = 7.91 GB, percent = 25.9%\n",
      "[2024-05-08 08:55:54,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2024-05-08 08:55:54,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2024-05-08 08:55:54,721] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x77c6351061d0>\n",
      "[2024-05-08 08:55:54,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:55:54,722] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-05-08 08:55:54,722] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   bfloat16_enabled ............. False\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-05-08 08:55:54,723] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x77c64c6d1850>\n",
      "[2024-05-08 08:55:54,724] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-05-08 08:55:54,724] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-05-08 08:55:54,724] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:55:54,724] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-05-08 08:55:54,724] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-05-08 08:55:54,724] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-05-08 08:55:54,725] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-05-08 08:55:54,726] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-05-08 08:55:54,726] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-05-08 08:55:54,726] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-05-08 08:55:54,726] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-05-08 08:55:54,726] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-05-08 08:55:54,726] [INFO] [config.py:1000:print]   fp16_auto_cast ............... False\n",
      "[2024-05-08 08:55:54,726] [INFO] [config.py:1000:print]   fp16_enabled ................. True\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-05-08 08:55:54,727] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 65536\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   loss_scale ................... 0\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-05-08 08:55:54,728] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-05-08 08:55:54,729] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-05-08 08:55:54,729] [INFO] [config.py:1000:print]   optimizer_name ............... adam\n",
      "[2024-05-08 08:55:54,729] [INFO] [config.py:1000:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
      "[2024-05-08 08:55:54,729] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-05-08 08:55:54,729] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-05-08 08:55:54,729] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-05-08 08:55:54,729] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   scheduler_name ............... WarmupLR\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   steps_per_print .............. 10\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   train_batch_size ............. 16\n",
      "[2024-05-08 08:55:54,730] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  16\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  False\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='cpu', nvme_path=None, buffer_count=5, buffer_size=100,000,000, max_in_cpu=1,000,000,000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='cpu', nvme_path=None, buffer_count=4, pin_memory=False, pipeline=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=0 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=True stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   zero_enabled ................. True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:55:54,731] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-05-08 08:55:54,732] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 3\n",
      "[2024-05-08 08:55:54,732] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.001, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"train_batch_size\": 16, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"cpu\"\n",
      "        }, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"load_from_fp32_weights\": true, \n",
      "        \"gather_16bit_weights_on_model_save\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"stage3_prefetch_bucket_size\": 0\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DeepSpeedEngine(\n",
       "  (module): PhiForSequenceClassification(\n",
       "    (model): PhiModel(\n",
       "      (embed_tokens): Embedding(51200, 2560)\n",
       "      (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-31): 32 x PhiDecoderLayer(\n",
       "          (self_attn): PhiAttention(\n",
       "            (query_key_value): Linear(in_features=2560, out_features=7680, bias=True)\n",
       "            (dense): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "            (attention_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (rotary_emb): PhiRotaryEmbedding()\n",
       "          )\n",
       "          (mlp): PhiMLP(\n",
       "            (activation_fn): NewGELUActivation()\n",
       "            (fc1): Linear(in_features=2560, out_features=10240, bias=True)\n",
       "            (fc2): Linear(in_features=10240, out_features=2560, bias=True)\n",
       "          )\n",
       "          (input_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (final_layernorm): LayerNorm((2560,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (score): Sequential(\n",
       "      (0): Linear(in_features=2560, out_features=2560, bias=True)\n",
       "      (1): Dropout(p=0.5, inplace=False)\n",
       "      (2): Linear(in_features=2560, out_features=2, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepspeed_config = {\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"Adam\",\n",
    "        \"params\": {\n",
    "            \"lr\": 0.001,\n",
    "            \"betas\": [\n",
    "                0.8,\n",
    "                0.999\n",
    "            ],\n",
    "            \"eps\": 1e-8,\n",
    "            \"weight_decay\": 3e-7,\n",
    "        },\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"type\": \"WarmupLR\",\n",
    "        \"params\": {\n",
    "            \"warmup_min_lr\": 0,\n",
    "            \"warmup_max_lr\": 0.001,\n",
    "            \"warmup_num_steps\": 1000,\n",
    "        },\n",
    "    },\n",
    "    \"train_batch_size\": 16,\n",
    "    # \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    # \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"fp16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 3,\n",
    "        \"offload_optimizer\": {\n",
    "            \"device\": \"cpu\",\n",
    "        },\n",
    "        \"offload_param\": {\n",
    "            \"device\": \"cpu\",\n",
    "        },\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 2e8,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 2e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"load_from_fp32_weights\": True,\n",
    "        \"gather_16bit_weights_on_model_save\": True,\n",
    "        \"contiguous_gradients\": True,\n",
    "        \"stage3_prefetch_bucket_size\": 0,\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"train_batch_size\": 16,\n",
    "}\n",
    "\n",
    "# Initialize DeepSpeed Engine\n",
    "model_engine, optimizer, _, lr_scheduler = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    config=deepspeed_config,\n",
    ")\n",
    "model_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb7f01e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = get_accelerator().device_name(model_engine.local_rank)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab0ddbc-f1e1-4662-a3d2-c09beafd25ed",
   "metadata": {},
   "source": [
    "# Split the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ab8adb52-47f3-4740-952f-639d21a70d6c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/it/anaconda3/envs/genety/lib/python3.11/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "BATCH_SIZE = 16  # batch size for training\n",
    "\n",
    "train_iter = iter(list(train_df.itertuples(index=False, name=None)))\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "test_dataset = to_map_style_dataset(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.8)\n",
    "split_train_, split_valid_ = random_split(\n",
    "    train_dataset, [num_train, len(train_dataset) - num_train]\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    split_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    split_valid_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2abb8af-8fe2-4382-8adf-7d2af9cd8e09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today = date.today().isoformat()\n",
    "model_name = \"phi\"\n",
    "checkpoint_path = f\"./models/{model_name}\"\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 20  # epoch\n",
    "# LR = 0.1 # learning rate\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "    \n",
    "def train_with_hist(model, checkpoint_path):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    total_accu = None\n",
    "\n",
    "    loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid = [], [], [], []\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        accu_train, loss_train = train(model, train_dataloader, epoch)\n",
    "        accu_val, loss_val = evaluate(model, valid_dataloader)\n",
    "        \n",
    "        print({\n",
    "            \"epoch\": epoch,\n",
    "            \"loss_train\": loss_train,\n",
    "            \"loss_val\": loss_val,\n",
    "            \"accuracy_train\": accu_train,\n",
    "            \"accuracy_val\": accu_val,\n",
    "        })\n",
    "        \n",
    "        loss_hist_train.append(loss_train)\n",
    "        loss_hist_valid.append(loss_val)\n",
    "        accuracy_hist_train.append(accu_train)\n",
    "        accuracy_hist_valid.append(accu_val)\n",
    "        \n",
    "    return loss_hist_train, loss_hist_valid, accuracy_hist_train, accuracy_hist_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00c4e1f4-9821-42a5-929f-e297c8cd9430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0040570e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Free(GB): 22.61102294921875, Global(GB): 23.64971923828125, Free(%): 0.9560799737790888\n"
     ]
    }
   ],
   "source": [
    "(free_memory, global_memory) = torch.cuda.mem_get_info()\n",
    "print(f\"Free(GB): {free_memory/1024/1024/1024}, Global(GB): {global_memory/1024/1024/1024}, Free(%): {free_memory/global_memory}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "97e7023b-7888-4afe-906a-76ac56aacab8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam Optimizer #0 is created with AVX512 arithmetic capability.\n",
      "Config: alpha=0.001000, betas=(0.800000, 0.999000), weight_decay=0.000000, adam_w=1\n",
      "[2024-05-08 08:55:55,848] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4294967296, reducing to 2147483648\n",
      "[2024-05-08 08:55:56,506] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2147483648, reducing to 1073741824\n",
      "[2024-05-08 08:55:57,018] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1073741824, reducing to 536870912\n",
      "[2024-05-08 08:55:57,514] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 536870912, reducing to 268435456\n",
      "[2024-05-08 08:55:57,993] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 268435456, reducing to 134217728\n",
      "[2024-05-08 08:55:58,516] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 134217728, reducing to 67108864\n",
      "[2024-05-08 08:55:59,008] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 67108864, reducing to 33554432\n",
      "[2024-05-08 08:55:59,514] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 33554432, reducing to 16777216\n",
      "[2024-05-08 08:55:59,997] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16777216, reducing to 8388608\n",
      "[2024-05-08 08:56:00,591] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8388608, reducing to 4194304\n",
      "[2024-05-08 08:56:00,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=10, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:00,591] [INFO] [timer.py:260:stop] epoch=0/micro_step=10/global_step=10, RunningAvgSamplesPerSec=41.427164991446745, CurrSamplesPerSec=33.89617321838779, MemAllocated=2.22GB, MaxMemAllocated=3.2GB\n",
      "[2024-05-08 08:56:01,129] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4194304, reducing to 2097152\n",
      "[2024-05-08 08:56:01,648] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2097152, reducing to 1048576\n",
      "[2024-05-08 08:56:02,171] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 1048576, reducing to 524288\n",
      "[2024-05-08 08:56:02,656] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 524288, reducing to 262144\n",
      "[2024-05-08 08:56:03,151] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 262144, reducing to 131072\n",
      "[2024-05-08 08:56:03,630] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 131072, reducing to 65536\n",
      "[2024-05-08 08:56:04,105] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 32768\n",
      "[2024-05-08 08:56:05,230] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 32768, reducing to 16384\n",
      "[2024-05-08 08:56:05,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=18, lr=[0.00010034333188799373], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:05,780] [INFO] [timer.py:260:stop] epoch=0/micro_step=20/global_step=20, RunningAvgSamplesPerSec=41.147263356481034, CurrSamplesPerSec=38.971511000593495, MemAllocated=1.59GB, MaxMemAllocated=3.68GB\n",
      "[2024-05-08 08:56:10,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=18, lr=[0.00035972708201587495], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:10,899] [INFO] [timer.py:260:stop] epoch=0/micro_step=30/global_step=30, RunningAvgSamplesPerSec=41.22427452951453, CurrSamplesPerSec=35.36814588935185, MemAllocated=2.04GB, MaxMemAllocated=3.68GB\n",
      "[2024-05-08 08:56:16,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=18, lr=[0.00044747422694073547], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:16,310] [INFO] [timer.py:260:stop] epoch=0/micro_step=40/global_step=40, RunningAvgSamplesPerSec=40.686145914069726, CurrSamplesPerSec=45.908815654274974, MemAllocated=0.79GB, MaxMemAllocated=3.68GB\n",
      "[2024-05-08 08:56:17,770] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 16384, reducing to 8192\n",
      "[2024-05-08 08:56:21,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=19, lr=[0.0004971205646114242], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:21,443] [INFO] [timer.py:260:stop] epoch=0/micro_step=50/global_step=50, RunningAvgSamplesPerSec=40.73374887447859, CurrSamplesPerSec=43.30780424876692, MemAllocated=1.05GB, MaxMemAllocated=4.36GB\n",
      "[2024-05-08 08:56:26,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=19, lr=[0.0005375946189065786], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:26,587] [INFO] [timer.py:260:stop] epoch=0/micro_step=60/global_step=60, RunningAvgSamplesPerSec=40.7862885551918, CurrSamplesPerSec=43.86980169023513, MemAllocated=0.92GB, MaxMemAllocated=4.36GB\n",
      "[2024-05-08 08:56:31,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=19, lr=[0.0005691900586993121], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:31,805] [INFO] [timer.py:260:stop] epoch=0/micro_step=70/global_step=70, RunningAvgSamplesPerSec=40.7572839913927, CurrSamplesPerSec=40.022438231349234, MemAllocated=1.47GB, MaxMemAllocated=4.36GB\n",
      "[2024-05-08 08:56:36,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=19, lr=[0.0005951099450035891], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:36,996] [INFO] [timer.py:260:stop] epoch=0/micro_step=80/global_step=80, RunningAvgSamplesPerSec=40.768622727814375, CurrSamplesPerSec=41.67719161399225, MemAllocated=1.3GB, MaxMemAllocated=4.36GB\n",
      "[2024-05-08 08:56:42,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=19, lr=[0.0006170861162396918], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:42,289] [INFO] [timer.py:260:stop] epoch=0/micro_step=90/global_step=90, RunningAvgSamplesPerSec=40.670929300613224, CurrSamplesPerSec=42.215010203234336, MemAllocated=1.16GB, MaxMemAllocated=4.36GB\n",
      "[2024-05-08 08:56:48,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=19, lr=[0.00063616167295955], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:48,177] [INFO] [timer.py:260:stop] epoch=0/micro_step=100/global_step=100, RunningAvgSamplesPerSec=40.05788160314614, CurrSamplesPerSec=27.229706404343844, MemAllocated=3.4GB, MaxMemAllocated=4.78GB\n",
      "[2024-05-08 08:56:53,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=19, lr=[0.0006530137974403645], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:56:53,577] [INFO] [timer.py:260:stop] epoch=0/micro_step=110/global_step=110, RunningAvgSamplesPerSec=39.941525984582356, CurrSamplesPerSec=43.29352747259836, MemAllocated=1.05GB, MaxMemAllocated=4.85GB\n",
      "{'epoch': 1, 'loss_train': 0.07380223022202861, 'loss_val': 0.003594493261099366, 'accuracy_train': 0.6469344608879493, 'accuracy_val': 0.6849894291754757}\n",
      "[2024-05-08 08:57:09,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=19, lr=[0.0006681071245942143], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:09,634] [INFO] [timer.py:260:stop] epoch=0/micro_step=120/global_step=120, RunningAvgSamplesPerSec=40.132952440436576, CurrSamplesPerSec=43.25142256472194, MemAllocated=1.24GB, MaxMemAllocated=4.85GB\n",
      "[2024-05-08 08:57:10,819] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 8192, reducing to 4096\n",
      "[2024-05-08 08:57:15,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=20, lr=[0.0006804642283860751], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:15,129] [INFO] [timer.py:260:stop] epoch=0/micro_step=130/global_step=130, RunningAvgSamplesPerSec=40.058871562841624, CurrSamplesPerSec=39.54838527179294, MemAllocated=1.47GB, MaxMemAllocated=4.85GB\n",
      "[2024-05-08 08:57:20,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=20, lr=[0.0006930604153492083], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:57:20,281] [INFO] [timer.py:260:stop] epoch=0/micro_step=140/global_step=140, RunningAvgSamplesPerSec=40.13198685315636, CurrSamplesPerSec=43.55947777072437, MemAllocated=1.08GB, MaxMemAllocated=4.85GB\n",
      "[2024-05-08 08:57:25,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=20, lr=[0.000704647784102279], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:25,380] [INFO] [timer.py:260:stop] epoch=0/micro_step=150/global_step=150, RunningAvgSamplesPerSec=40.24791248318872, CurrSamplesPerSec=42.335356456361545, MemAllocated=1.21GB, MaxMemAllocated=4.85GB\n",
      "[2024-05-08 08:57:30,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=20, lr=[0.000715376011892746], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:30,832] [INFO] [timer.py:260:stop] epoch=0/micro_step=160/global_step=160, RunningAvgSamplesPerSec=40.15848447874006, CurrSamplesPerSec=42.40949139346386, MemAllocated=1.17GB, MaxMemAllocated=4.85GB\n",
      "[2024-05-08 08:57:36,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=20, lr=[0.0007253637530185604], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:36,445] [INFO] [timer.py:260:stop] epoch=0/micro_step=170/global_step=170, RunningAvgSamplesPerSec=39.96009661716508, CurrSamplesPerSec=38.904207998135625, MemAllocated=1.6GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:57:41,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=20, lr=[0.0007347066608853083], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:41,562] [INFO] [timer.py:260:stop] epoch=0/micro_step=180/global_step=180, RunningAvgSamplesPerSec=40.029727933966875, CurrSamplesPerSec=44.777930798876895, MemAllocated=0.9GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:57:46,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=20, lr=[0.0007434829737927581], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:46,738] [INFO] [timer.py:260:stop] epoch=0/micro_step=190/global_step=190, RunningAvgSamplesPerSec=40.05106192086833, CurrSamplesPerSec=33.51277561990446, MemAllocated=2.22GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:57:52,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=20, lr=[0.000751757501701102], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:52,051] [INFO] [timer.py:260:stop] epoch=0/micro_step=200/global_step=200, RunningAvgSamplesPerSec=40.040972471331536, CurrSamplesPerSec=42.15452397747952, MemAllocated=1.22GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:57:57,251] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=20, lr=[0.0007595845336509431], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:57:57,252] [INFO] [timer.py:260:stop] epoch=0/micro_step=210/global_step=210, RunningAvgSamplesPerSec=40.07459845370057, CurrSamplesPerSec=45.691172555691956, MemAllocated=0.84GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:02,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=20, lr=[0.0007670099985546603], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:02,812] [INFO] [timer.py:260:stop] epoch=0/micro_step=220/global_step=220, RunningAvgSamplesPerSec=39.959952872068314, CurrSamplesPerSec=43.14852899667524, MemAllocated=1.03GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:07,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=20, lr=[0.0007740730982446398], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:07,921] [INFO] [timer.py:260:stop] epoch=0/micro_step=230/global_step=230, RunningAvgSamplesPerSec=40.029893222535904, CurrSamplesPerSec=42.89389031352507, MemAllocated=1.15GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 2, 'loss_train': 0.0831272052660049, 'loss_val': 0.0015092329545454545, 'accuracy_train': 0.7071881606765328, 'accuracy_val': 0.828752642706131}\n",
      "[2024-05-08 08:58:24,085] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 4096, reducing to 2048\n",
      "[2024-05-08 08:58:24,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=21, lr=[0.0007801480382800396], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:24,086] [INFO] [timer.py:260:stop] epoch=0/micro_step=240/global_step=240, RunningAvgSamplesPerSec=40.04466271789593, CurrSamplesPerSec=41.99905624126178, MemAllocated=1.3GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:29,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=21, lr=[0.0007866118274466294], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:29,235] [INFO] [timer.py:260:stop] epoch=0/micro_step=250/global_step=250, RunningAvgSamplesPerSec=40.095565191282134, CurrSamplesPerSec=45.41243662722871, MemAllocated=0.79GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:34,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=21, lr=[0.000792799300316046], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:34,599] [INFO] [timer.py:260:stop] epoch=0/micro_step=260/global_step=260, RunningAvgSamplesPerSec=40.07494860090756, CurrSamplesPerSec=42.869448788315495, MemAllocated=1.15GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:39,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=21, lr=[0.0007987331156985788], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:39,692] [INFO] [timer.py:260:stop] epoch=0/micro_step=270/global_step=270, RunningAvgSamplesPerSec=40.129333475279196, CurrSamplesPerSec=39.113085999539564, MemAllocated=1.47GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:45,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=21, lr=[0.0008044332546937506], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:45,091] [INFO] [timer.py:260:stop] epoch=0/micro_step=280/global_step=280, RunningAvgSamplesPerSec=40.10302848339601, CurrSamplesPerSec=43.39837746427867, MemAllocated=1.04GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:50,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=21, lr=[0.0008099174266674693], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:50,301] [INFO] [timer.py:260:stop] epoch=0/micro_step=290/global_step=290, RunningAvgSamplesPerSec=40.104273673423315, CurrSamplesPerSec=46.05990290948949, MemAllocated=0.77GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:58:55,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=21, lr=[0.0008152014010911992], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:58:55,396] [INFO] [timer.py:260:stop] epoch=0/micro_step=300/global_step=300, RunningAvgSamplesPerSec=40.12495578411954, CurrSamplesPerSec=33.694516954237244, MemAllocated=2.17GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:00,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=21, lr=[0.0008202992809188493], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:00,784] [INFO] [timer.py:260:stop] epoch=0/micro_step=310/global_step=310, RunningAvgSamplesPerSec=40.089086981882645, CurrSamplesPerSec=45.26617107543208, MemAllocated=0.85GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:05,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=21, lr=[0.0008252237294414767], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:05,997] [INFO] [timer.py:260:stop] epoch=0/micro_step=320/global_step=320, RunningAvgSamplesPerSec=40.094339342809555, CurrSamplesPerSec=27.59656762137082, MemAllocated=3.21GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:11,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=21, lr=[0.0008299861598082783], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:11,364] [INFO] [timer.py:260:stop] epoch=0/micro_step=330/global_step=330, RunningAvgSamplesPerSec=40.069308359101655, CurrSamplesPerSec=39.74188596828886, MemAllocated=1.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:16,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=21, lr=[0.0008345968943523937], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:16,912] [INFO] [timer.py:260:stop] epoch=0/micro_step=340/global_step=340, RunningAvgSamplesPerSec=40.0072456960601, CurrSamplesPerSec=39.727111385789875, MemAllocated=1.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:22,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=21, lr=[0.0008390652993166581], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:22,075] [INFO] [timer.py:260:stop] epoch=0/micro_step=350/global_step=350, RunningAvgSamplesPerSec=40.04354399754771, CurrSamplesPerSec=42.075604451274295, MemAllocated=1.21GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 3, 'loss_train': 0.069564077365222, 'loss_val': 0.00026839984143763214, 'accuracy_train': 0.7050739957716702, 'accuracy_val': 0.8414376321353065}\n",
      "[2024-05-08 08:59:38,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=21, lr=[0.0008433998994010274], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 08:59:38,237] [INFO] [timer.py:260:stop] epoch=0/micro_step=360/global_step=360, RunningAvgSamplesPerSec=40.04226159484723, CurrSamplesPerSec=39.176516809526284, MemAllocated=1.59GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:43,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=21, lr=[0.00084760847565306], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:43,634] [INFO] [timer.py:260:stop] epoch=0/micro_step=370/global_step=370, RunningAvgSamplesPerSec=40.01249631064841, CurrSamplesPerSec=31.32608245093518, MemAllocated=2.59GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:49,229] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=21, lr=[0.0008516981495261064], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:49,229] [INFO] [timer.py:260:stop] epoch=0/micro_step=380/global_step=380, RunningAvgSamplesPerSec=39.939556164850465, CurrSamplesPerSec=42.082860303357904, MemAllocated=1.26GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:54,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=21, lr=[0.0008556754553863535], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:54,249] [INFO] [timer.py:260:stop] epoch=0/micro_step=390/global_step=390, RunningAvgSamplesPerSec=39.98454752559303, CurrSamplesPerSec=42.81002323939125, MemAllocated=1.2GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 08:59:59,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=21, lr=[0.0008595464033226909], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 08:59:59,477] [INFO] [timer.py:260:stop] epoch=0/micro_step=400/global_step=400, RunningAvgSamplesPerSec=39.985410859170415, CurrSamplesPerSec=39.352282482748265, MemAllocated=1.52GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:04,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=21, lr=[0.0008633165337752359], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:04,466] [INFO] [timer.py:260:stop] epoch=0/micro_step=410/global_step=410, RunningAvgSamplesPerSec=40.043299796458676, CurrSamplesPerSec=44.349792356718204, MemAllocated=0.95GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:09,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=21, lr=[0.0008669909652289162], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:09,742] [INFO] [timer.py:260:stop] epoch=0/micro_step=420/global_step=420, RunningAvgSamplesPerSec=40.035988655657015, CurrSamplesPerSec=41.422437626110344, MemAllocated=1.27GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:14,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=21, lr=[0.0008705744360024473], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:14,748] [INFO] [timer.py:260:stop] epoch=0/micro_step=430/global_step=430, RunningAvgSamplesPerSec=40.08425892628768, CurrSamplesPerSec=44.33631424282785, MemAllocated=0.93GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:20,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=21, lr=[0.000874071340988765], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:20,147] [INFO] [timer.py:260:stop] epoch=0/micro_step=440/global_step=440, RunningAvgSamplesPerSec=40.05771064455953, CurrSamplesPerSec=33.53970806815808, MemAllocated=2.22GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:25,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=21, lr=[0.0008774857640615748], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:25,618] [INFO] [timer.py:260:stop] epoch=0/micro_step=450/global_step=450, RunningAvgSamplesPerSec=40.02908972914518, CurrSamplesPerSec=44.712653542027006, MemAllocated=0.92GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:30,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=21, lr=[0.0008808215067473738], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:30,611] [INFO] [timer.py:260:stop] epoch=0/micro_step=460/global_step=460, RunningAvgSamplesPerSec=40.077888727972585, CurrSamplesPerSec=41.75379094255288, MemAllocated=1.24GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:36,177] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=21, lr=[0.0008840821136677744], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:36,177] [INFO] [timer.py:260:stop] epoch=0/micro_step=470/global_step=470, RunningAvgSamplesPerSec=40.02306514929246, CurrSamplesPerSec=39.24660219611586, MemAllocated=1.59GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 4, 'loss_train': 0.07690732927705471, 'loss_val': 0.012445494186046511, 'accuracy_train': 0.7077167019027484, 'accuracy_val': 0.6194503171247357}\n",
      "[2024-05-08 09:00:52,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=21, lr=[0.0008872708951790871], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:52,449] [INFO] [timer.py:260:stop] epoch=0/micro_step=480/global_step=480, RunningAvgSamplesPerSec=39.999825117245344, CurrSamplesPerSec=39.13258654973147, MemAllocated=1.65GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:00:58,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=21, lr=[0.0008903909475716945], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:00:58,003] [INFO] [timer.py:260:stop] epoch=0/micro_step=490/global_step=490, RunningAvgSamplesPerSec=39.94700584104857, CurrSamplesPerSec=29.2772714918486, MemAllocated=2.9GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:03,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=21, lr=[0.0008934451711381878], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:03,176] [INFO] [timer.py:260:stop] epoch=0/micro_step=500/global_step=500, RunningAvgSamplesPerSec=39.97002763719529, CurrSamplesPerSec=34.78116364164067, MemAllocated=2.09GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:08,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=21, lr=[0.0008964362863745402], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:08,549] [INFO] [timer.py:260:stop] epoch=0/micro_step=510/global_step=510, RunningAvgSamplesPerSec=39.958034007837455, CurrSamplesPerSec=43.72730536588926, MemAllocated=0.92GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:13,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=21, lr=[0.00089936684854113], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:13,743] [INFO] [timer.py:260:stop] epoch=0/micro_step=520/global_step=520, RunningAvgSamplesPerSec=39.969102688620495, CurrSamplesPerSec=37.927855036391236, MemAllocated=1.71GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:19,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=21, lr=[0.0009022392607789196], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:19,040] [INFO] [timer.py:260:stop] epoch=0/micro_step=530/global_step=530, RunningAvgSamplesPerSec=39.96823559486615, CurrSamplesPerSec=35.534842049692436, MemAllocated=1.98GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:24,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=21, lr=[0.000905055785949486], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:24,253] [INFO] [timer.py:260:stop] epoch=0/micro_step=540/global_step=540, RunningAvgSamplesPerSec=39.98507178694098, CurrSamplesPerSec=44.42453923075599, MemAllocated=0.97GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:29,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=21, lr=[0.000907818557345062], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:29,608] [INFO] [timer.py:260:stop] epoch=0/micro_step=550/global_step=550, RunningAvgSamplesPerSec=39.97403132476923, CurrSamplesPerSec=39.19209767413067, MemAllocated=1.59GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:34,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=21, lr=[0.0009105295883955796], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:34,732] [INFO] [timer.py:260:stop] epoch=0/micro_step=560/global_step=560, RunningAvgSamplesPerSec=40.002420171909044, CurrSamplesPerSec=45.131131323660455, MemAllocated=0.84GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:40,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=21, lr=[0.0009131907814833639], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:40,047] [INFO] [timer.py:260:stop] epoch=0/micro_step=570/global_step=570, RunningAvgSamplesPerSec=39.99573136943948, CurrSamplesPerSec=41.737327148815275, MemAllocated=1.28GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:45,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=21, lr=[0.0009158039359621412], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:01:45,362] [INFO] [timer.py:260:stop] epoch=0/micro_step=580/global_step=580, RunningAvgSamplesPerSec=39.98222545627402, CurrSamplesPerSec=44.68356694225662, MemAllocated=0.96GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:01:50,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=21, lr=[0.0009183707554650238], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:01:50,338] [INFO] [timer.py:260:stop] epoch=0/micro_step=590/global_step=590, RunningAvgSamplesPerSec=40.017291147798005, CurrSamplesPerSec=44.02160250266817, MemAllocated=0.9GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 5, 'loss_train': 0.060538100389799156, 'loss_val': 0.0009915309526955603, 'accuracy_train': 0.7404862579281184, 'accuracy_val': 0.7843551797040169}\n",
      "[2024-05-08 09:02:06,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=21, lr=[0.0009208928545758122], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:06,146] [INFO] [timer.py:260:stop] epoch=0/micro_step=600/global_step=600, RunningAvgSamplesPerSec=40.07354508294816, CurrSamplesPerSec=43.60430969395247, MemAllocated=1.08GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:11,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=21, lr=[0.0009233717649290339], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:11,562] [INFO] [timer.py:260:stop] epoch=0/micro_step=610/global_step=610, RunningAvgSamplesPerSec=40.04682934085954, CurrSamplesPerSec=30.64016893241562, MemAllocated=2.62GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:16,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=21, lr=[0.0009258089407964371], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:16,944] [INFO] [timer.py:260:stop] epoch=0/micro_step=620/global_step=620, RunningAvgSamplesPerSec=40.03601697153546, CurrSamplesPerSec=43.107702348192284, MemAllocated=1.03GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:22,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=21, lr=[0.0009282057642109585], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:22,165] [INFO] [timer.py:260:stop] epoch=0/micro_step=630/global_step=630, RunningAvgSamplesPerSec=40.043043152948265, CurrSamplesPerSec=45.40398731021306, MemAllocated=0.84GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:27,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=21, lr=[0.0009305635496733727], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:27,554] [INFO] [timer.py:260:stop] epoch=0/micro_step=640/global_step=640, RunningAvgSamplesPerSec=40.02481936076822, CurrSamplesPerSec=31.255088876614128, MemAllocated=2.59GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:32,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=21, lr=[0.0009328835484817563], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:32,667] [INFO] [timer.py:260:stop] epoch=0/micro_step=650/global_step=650, RunningAvgSamplesPerSec=40.05127839763579, CurrSamplesPerSec=41.5500543918055, MemAllocated=1.34GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:38,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=21, lr=[0.0009351669527194668], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:38,196] [INFO] [timer.py:260:stop] epoch=0/micro_step=660/global_step=660, RunningAvgSamplesPerSec=40.01581814404826, CurrSamplesPerSec=41.30134614757342, MemAllocated=1.15GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:43,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=21, lr=[0.0009374148989334566], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:43,252] [INFO] [timer.py:260:stop] epoch=0/micro_step=670/global_step=670, RunningAvgSamplesPerSec=40.04153408250397, CurrSamplesPerSec=31.45958539774243, MemAllocated=2.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:48,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=21, lr=[0.0009396284715313367], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:48,842] [INFO] [timer.py:260:stop] epoch=0/micro_step=680/global_step=680, RunningAvgSamplesPerSec=40.001259044452155, CurrSamplesPerSec=25.866705005490648, MemAllocated=3.56GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:54,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=21, lr=[0.0009418087059226078], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:54,113] [INFO] [timer.py:260:stop] epoch=0/micro_step=690/global_step=690, RunningAvgSamplesPerSec=40.00039536290271, CurrSamplesPerSec=44.53834503831384, MemAllocated=1.02GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:02:59,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=21, lr=[0.000943956591426834], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:02:59,320] [INFO] [timer.py:260:stop] epoch=0/micro_step=700/global_step=700, RunningAvgSamplesPerSec=40.007456438086145, CurrSamplesPerSec=41.85624062101218, MemAllocated=1.29GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:04,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=21, lr=[0.0009460730739692088], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:04,497] [INFO] [timer.py:260:stop] epoch=0/micro_step=710/global_step=710, RunningAvgSamplesPerSec=40.01382627968873, CurrSamplesPerSec=42.954043963513726, MemAllocated=1.15GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 6, 'loss_train': 0.09223305700193248, 'loss_val': 0.0016403359540169134, 'accuracy_train': 0.7071881606765328, 'accuracy_val': 0.7991543340380549}\n",
      "[2024-05-08 09:03:20,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=21, lr=[0.0009481590585818939], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:20,546] [INFO] [timer.py:260:stop] epoch=0/micro_step=720/global_step=720, RunningAvgSamplesPerSec=40.01448557970023, CurrSamplesPerSec=31.536861298432765, MemAllocated=2.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:26,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=21, lr=[0.0009502154117276889], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:26,048] [INFO] [timer.py:260:stop] epoch=0/micro_step=730/global_step=730, RunningAvgSamplesPerSec=39.99492769197246, CurrSamplesPerSec=43.55730079061312, MemAllocated=1.08GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:31,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=21, lr=[0.0009522429634609609], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:31,538] [INFO] [timer.py:260:stop] epoch=0/micro_step=740/global_step=740, RunningAvgSamplesPerSec=39.97340881229826, CurrSamplesPerSec=35.766805646678094, MemAllocated=1.98GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:36,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=21, lr=[0.0009542425094393249], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:36,561] [INFO] [timer.py:260:stop] epoch=0/micro_step=750/global_step=750, RunningAvgSamplesPerSec=40.00567095229963, CurrSamplesPerSec=39.560974804579274, MemAllocated=1.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:41,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=21, lr=[0.0009562148127982752], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:41,970] [INFO] [timer.py:260:stop] epoch=0/micro_step=760/global_step=760, RunningAvgSamplesPerSec=39.98826179613999, CurrSamplesPerSec=39.18822990116644, MemAllocated=1.52GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:47,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=21, lr=[0.0009581606058998222], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:47,048] [INFO] [timer.py:260:stop] epoch=0/micro_step=770/global_step=770, RunningAvgSamplesPerSec=40.00653400941299, CurrSamplesPerSec=41.49225260807118, MemAllocated=1.28GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:52,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=21, lr=[0.0009600805919651603], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:52,205] [INFO] [timer.py:260:stop] epoch=0/micro_step=780/global_step=780, RunningAvgSamplesPerSec=40.014026466682324, CurrSamplesPerSec=41.987021404921656, MemAllocated=1.24GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:03:57,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=21, lr=[0.0009619754466004771], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:03:57,471] [INFO] [timer.py:260:stop] epoch=0/micro_step=790/global_step=790, RunningAvgSamplesPerSec=40.02031255340442, CurrSamplesPerSec=39.331432858214555, MemAllocated=1.55GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:04:02,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=21, lr=[0.0009638458192241882], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:02,597] [INFO] [timer.py:260:stop] epoch=0/micro_step=800/global_step=800, RunningAvgSamplesPerSec=40.03750721161668, CurrSamplesPerSec=43.35595642494337, MemAllocated=1.09GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:04:07,920] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=21, lr=[0.00096569233440314], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:07,921] [INFO] [timer.py:260:stop] epoch=0/micro_step=810/global_step=810, RunningAvgSamplesPerSec=40.03263831152018, CurrSamplesPerSec=42.89350648686584, MemAllocated=1.15GB, MaxMemAllocated=6.59GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:04:13,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=21, lr=[0.0009675155931046639], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:13,009] [INFO] [timer.py:260:stop] epoch=0/micro_step=820/global_step=820, RunningAvgSamplesPerSec=40.051766659541016, CurrSamplesPerSec=44.11646523246832, MemAllocated=0.97GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:04:18,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=21, lr=[0.0009693161738707574], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:18,224] [INFO] [timer.py:260:stop] epoch=0/micro_step=830/global_step=830, RunningAvgSamplesPerSec=40.0578141043906, CurrSamplesPerSec=39.472133766391124, MemAllocated=1.47GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 7, 'loss_train': 0.08449622188511662, 'loss_val': 0.002120358747357294, 'accuracy_train': 0.7315010570824524, 'accuracy_val': 0.8435517970401691}\n",
      "[2024-05-08 09:04:34,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=21, lr=[0.0009710946339201396], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:34,359] [INFO] [timer.py:260:stop] epoch=0/micro_step=840/global_step=840, RunningAvgSamplesPerSec=40.068627056273286, CurrSamplesPerSec=32.943414028309924, MemAllocated=2.34GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:04:39,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=21, lr=[0.0009728515101834245], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:39,630] [INFO] [timer.py:260:stop] epoch=0/micro_step=850/global_step=850, RunningAvgSamplesPerSec=40.070773509323274, CurrSamplesPerSec=42.06033576135448, MemAllocated=1.24GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:04:44,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=21, lr=[0.0009745873202762334], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:44,774] [INFO] [timer.py:260:stop] epoch=0/micro_step=860/global_step=860, RunningAvgSamplesPerSec=40.084443906146156, CurrSamplesPerSec=44.41125077593612, MemAllocated=0.9GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:04:50,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=21, lr=[0.0009763025634146509], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:50,294] [INFO] [timer.py:260:stop] epoch=0/micro_step=870/global_step=870, RunningAvgSamplesPerSec=40.059111915395285, CurrSamplesPerSec=42.7759683078953, MemAllocated=1.15GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:04:55,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=21, lr=[0.0009779977212770808], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:04:55,394] [INFO] [timer.py:260:stop] epoch=0/micro_step=880/global_step=880, RunningAvgSamplesPerSec=40.078366332671344, CurrSamplesPerSec=41.70671930175642, MemAllocated=1.34GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:00,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=21, lr=[0.0009796732588162223], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:00,576] [INFO] [timer.py:260:stop] epoch=0/micro_step=890/global_step=890, RunningAvgSamplesPerSec=40.08858848068125, CurrSamplesPerSec=41.5704904608016, MemAllocated=1.28GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:06,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=21, lr=[0.0009813296250245906], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:06,187] [INFO] [timer.py:260:stop] epoch=0/micro_step=900/global_step=900, RunningAvgSamplesPerSec=40.055779099038105, CurrSamplesPerSec=39.78585115061372, MemAllocated=1.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:11,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=21, lr=[0.000982967253656738], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:11,573] [INFO] [timer.py:260:stop] epoch=0/micro_step=910/global_step=910, RunningAvgSamplesPerSec=40.04110290839062, CurrSamplesPerSec=39.815144487339445, MemAllocated=1.42GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:16,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=21, lr=[0.0009845865639110762], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:16,702] [INFO] [timer.py:260:stop] epoch=0/micro_step=920/global_step=920, RunningAvgSamplesPerSec=40.054733987047385, CurrSamplesPerSec=42.816960933248176, MemAllocated=1.15GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:22,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=21, lr=[0.0009861879610739892], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:22,354] [INFO] [timer.py:260:stop] epoch=0/micro_step=930/global_step=930, RunningAvgSamplesPerSec=40.02101807349828, CurrSamplesPerSec=44.325157049510146, MemAllocated=1.02GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:27,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=21, lr=[0.0009877718371287037], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:27,568] [INFO] [timer.py:260:stop] epoch=0/micro_step=940/global_step=940, RunningAvgSamplesPerSec=40.03030135427361, CurrSamplesPerSec=41.50428779761942, MemAllocated=1.28GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:32,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=21, lr=[0.0009893385713312139], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:32,454] [INFO] [timer.py:260:stop] epoch=0/micro_step=950/global_step=950, RunningAvgSamplesPerSec=40.06561017426128, CurrSamplesPerSec=44.371081017107386, MemAllocated=1.02GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 8, 'loss_train': 0.10653744590710888, 'loss_val': 0.0018891219608879492, 'accuracy_train': 0.7357293868921776, 'accuracy_val': 0.7610993657505285}\n",
      "[2024-05-08 09:05:48,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=21, lr=[0.0009908885307553703], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:48,597] [INFO] [timer.py:260:stop] epoch=0/micro_step=960/global_step=960, RunningAvgSamplesPerSec=40.07296150039388, CurrSamplesPerSec=45.03536520467581, MemAllocated=0.9GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:53,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=21, lr=[0.0009924220708090977], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:53,803] [INFO] [timer.py:260:stop] epoch=0/micro_step=970/global_step=970, RunningAvgSamplesPerSec=40.07870201001167, CurrSamplesPerSec=38.12223863300689, MemAllocated=1.71GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:05:58,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=21, lr=[0.0009939395357235546], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:05:58,839] [INFO] [timer.py:260:stop] epoch=0/micro_step=980/global_step=980, RunningAvgSamplesPerSec=40.099918291623716, CurrSamplesPerSec=44.326474535789394, MemAllocated=0.97GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:03,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=21, lr=[0.0009954412590169217], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:03,949] [INFO] [timer.py:260:stop] epoch=0/micro_step=990/global_step=990, RunningAvgSamplesPerSec=40.11542098225816, CurrSamplesPerSec=44.23723848434631, MemAllocated=1.02GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:09,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=21, lr=[0.0009969275639343793], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:09,159] [INFO] [timer.py:260:stop] epoch=0/micro_step=1000/global_step=1000, RunningAvgSamplesPerSec=40.11923305674298, CurrSamplesPerSec=41.67180861567274, MemAllocated=1.21GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:14,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=21, lr=[0.0009983987638657265], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:14,705] [INFO] [timer.py:260:stop] epoch=0/micro_step=1010/global_step=1010, RunningAvgSamplesPerSec=40.094444635689584, CurrSamplesPerSec=37.1621111714708, MemAllocated=1.83GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:19,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=21, lr=[0.0009998551627419942], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:19,847] [INFO] [timer.py:260:stop] epoch=0/micro_step=1020/global_step=1020, RunningAvgSamplesPerSec=40.1071473936865, CurrSamplesPerSec=42.2954936354005, MemAllocated=1.21GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:25,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:25,290] [INFO] [timer.py:260:stop] epoch=0/micro_step=1030/global_step=1030, RunningAvgSamplesPerSec=40.09234263193711, CurrSamplesPerSec=37.76404696078966, MemAllocated=1.72GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:30,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:06:30,581] [INFO] [timer.py:260:stop] epoch=0/micro_step=1040/global_step=1040, RunningAvgSamplesPerSec=40.087133939763454, CurrSamplesPerSec=25.866714975659207, MemAllocated=3.57GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:35,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:35,697] [INFO] [timer.py:260:stop] epoch=0/micro_step=1050/global_step=1050, RunningAvgSamplesPerSec=40.099309409408775, CurrSamplesPerSec=41.595097013482224, MemAllocated=1.34GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:41,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:41,023] [INFO] [timer.py:260:stop] epoch=0/micro_step=1060/global_step=1060, RunningAvgSamplesPerSec=40.0949840917856, CurrSamplesPerSec=38.109314423918356, MemAllocated=1.68GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:06:46,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:06:46,280] [INFO] [timer.py:260:stop] epoch=0/micro_step=1070/global_step=1070, RunningAvgSamplesPerSec=40.09131937114366, CurrSamplesPerSec=38.831159422805634, MemAllocated=1.58GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 9, 'loss_train': 0.1399682591379823, 'loss_val': 0.0002542056190539112, 'accuracy_train': 0.7093023255813954, 'accuracy_val': 0.8076109936575053}\n",
      "[2024-05-08 09:07:02,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:02,266] [INFO] [timer.py:260:stop] epoch=0/micro_step=1080/global_step=1080, RunningAvgSamplesPerSec=40.109652592718355, CurrSamplesPerSec=43.5072056179768, MemAllocated=1.04GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:07,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:07,431] [INFO] [timer.py:260:stop] epoch=0/micro_step=1090/global_step=1090, RunningAvgSamplesPerSec=40.118801366255184, CurrSamplesPerSec=42.581793516366446, MemAllocated=1.11GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:13,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:13,049] [INFO] [timer.py:260:stop] epoch=0/micro_step=1100/global_step=1100, RunningAvgSamplesPerSec=40.090533444878645, CurrSamplesPerSec=27.10031567091882, MemAllocated=3.4GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:18,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:18,580] [INFO] [timer.py:260:stop] epoch=0/micro_step=1110/global_step=1110, RunningAvgSamplesPerSec=40.0682237836002, CurrSamplesPerSec=44.82210379588399, MemAllocated=1.01GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:23,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:23,795] [INFO] [timer.py:260:stop] epoch=0/micro_step=1120/global_step=1120, RunningAvgSamplesPerSec=40.066691161657005, CurrSamplesPerSec=41.98363292508444, MemAllocated=1.2GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:29,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:29,155] [INFO] [timer.py:260:stop] epoch=0/micro_step=1130/global_step=1130, RunningAvgSamplesPerSec=40.05811144759301, CurrSamplesPerSec=38.717865963875404, MemAllocated=1.6GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:34,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:34,423] [INFO] [timer.py:260:stop] epoch=0/micro_step=1140/global_step=1140, RunningAvgSamplesPerSec=40.05831189176721, CurrSamplesPerSec=42.463402457490005, MemAllocated=1.21GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:39,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:39,730] [INFO] [timer.py:260:stop] epoch=0/micro_step=1150/global_step=1150, RunningAvgSamplesPerSec=40.056025853333175, CurrSamplesPerSec=41.4047267957468, MemAllocated=1.28GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:44,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:44,997] [INFO] [timer.py:260:stop] epoch=0/micro_step=1160/global_step=1160, RunningAvgSamplesPerSec=40.05876422426388, CurrSamplesPerSec=42.37540151861967, MemAllocated=1.21GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:50,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:50,014] [INFO] [timer.py:260:stop] epoch=0/micro_step=1170/global_step=1170, RunningAvgSamplesPerSec=40.079982678494446, CurrSamplesPerSec=42.1436437442225, MemAllocated=1.21GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:07:55,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=21, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:07:55,118] [INFO] [timer.py:260:stop] epoch=0/micro_step=1180/global_step=1180, RunningAvgSamplesPerSec=40.09333037423747, CurrSamplesPerSec=44.18990324940555, MemAllocated=0.97GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:00,307] [INFO] [loss_scaler.py:183:update_scale] [deepspeed] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 2048, reducing to 1024\n",
      "[2024-05-08 09:08:00,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:00,308] [INFO] [timer.py:260:stop] epoch=0/micro_step=1190/global_step=1190, RunningAvgSamplesPerSec=40.09950847164281, CurrSamplesPerSec=49.7987638774385, MemAllocated=0.54GB, MaxMemAllocated=6.59GB\n",
      "{'epoch': 10, 'loss_train': 0.1121029359845732, 'loss_val': 0.003004013609936575, 'accuracy_train': 0.7278012684989429, 'accuracy_val': 0.7949260042283298}\n",
      "[2024-05-08 09:08:16,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:16,448] [INFO] [timer.py:260:stop] epoch=0/micro_step=1200/global_step=1200, RunningAvgSamplesPerSec=40.10127194934205, CurrSamplesPerSec=45.35219266797186, MemAllocated=0.86GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:21,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:21,744] [INFO] [timer.py:260:stop] epoch=0/micro_step=1210/global_step=1210, RunningAvgSamplesPerSec=40.099923331753594, CurrSamplesPerSec=42.759369819058094, MemAllocated=1.11GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:26,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:26,916] [INFO] [timer.py:260:stop] epoch=0/micro_step=1220/global_step=1220, RunningAvgSamplesPerSec=40.107875055016976, CurrSamplesPerSec=39.76958259898711, MemAllocated=1.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:32,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:32,185] [INFO] [timer.py:260:stop] epoch=0/micro_step=1230/global_step=1230, RunningAvgSamplesPerSec=40.109885284593595, CurrSamplesPerSec=43.14453438666669, MemAllocated=1.11GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:37,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:37,451] [INFO] [timer.py:260:stop] epoch=0/micro_step=1240/global_step=1240, RunningAvgSamplesPerSec=40.11021639860347, CurrSamplesPerSec=38.84026143972423, MemAllocated=1.59GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:42,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:42,868] [INFO] [timer.py:260:stop] epoch=0/micro_step=1250/global_step=1250, RunningAvgSamplesPerSec=40.100484241339394, CurrSamplesPerSec=33.83997369806426, MemAllocated=2.21GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:48,075] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:48,076] [INFO] [timer.py:260:stop] epoch=0/micro_step=1260/global_step=1260, RunningAvgSamplesPerSec=40.10555692569376, CurrSamplesPerSec=39.83558801077496, MemAllocated=1.46GB, MaxMemAllocated=6.59GB\n",
      "[2024-05-08 09:08:53,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:08:53,826] [INFO] [timer.py:260:stop] epoch=0/micro_step=1270/global_step=1270, RunningAvgSamplesPerSec=40.06939601032175, CurrSamplesPerSec=36.367651697023234, MemAllocated=1.98GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:08:59,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:08:59,247] [INFO] [timer.py:260:stop] epoch=0/micro_step=1280/global_step=1280, RunningAvgSamplesPerSec=40.05419811869049, CurrSamplesPerSec=33.76718213101647, MemAllocated=2.17GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:04,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:04,162] [INFO] [timer.py:260:stop] epoch=0/micro_step=1290/global_step=1290, RunningAvgSamplesPerSec=40.07795439881548, CurrSamplesPerSec=43.029287468846356, MemAllocated=1.15GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:09,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:09,150] [INFO] [timer.py:260:stop] epoch=0/micro_step=1300/global_step=1300, RunningAvgSamplesPerSec=40.09698445081985, CurrSamplesPerSec=44.20192407757961, MemAllocated=0.96GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 11, 'loss_train': 0.12284084908805992, 'loss_val': 0.0008717833311310782, 'accuracy_train': 0.7362579281183932, 'accuracy_val': 0.8118393234672304}\n",
      "[2024-05-08 09:09:25,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:25,109] [INFO] [timer.py:260:stop] epoch=0/micro_step=1310/global_step=1310, RunningAvgSamplesPerSec=40.1050619757561, CurrSamplesPerSec=43.09521757641648, MemAllocated=1.27GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:30,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:30,372] [INFO] [timer.py:260:stop] epoch=0/micro_step=1320/global_step=1320, RunningAvgSamplesPerSec=40.11097512924782, CurrSamplesPerSec=42.78886896731132, MemAllocated=1.16GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:35,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:35,676] [INFO] [timer.py:260:stop] epoch=0/micro_step=1330/global_step=1330, RunningAvgSamplesPerSec=40.10777702100837, CurrSamplesPerSec=31.452567029409188, MemAllocated=2.49GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:40,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:40,937] [INFO] [timer.py:260:stop] epoch=0/micro_step=1340/global_step=1340, RunningAvgSamplesPerSec=40.110109543200174, CurrSamplesPerSec=35.754362148643914, MemAllocated=1.98GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:46,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:46,425] [INFO] [timer.py:260:stop] epoch=0/micro_step=1350/global_step=1350, RunningAvgSamplesPerSec=40.097668530824336, CurrSamplesPerSec=39.151832454624, MemAllocated=1.47GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:51,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:51,875] [INFO] [timer.py:260:stop] epoch=0/micro_step=1360/global_step=1360, RunningAvgSamplesPerSec=40.08926705091632, CurrSamplesPerSec=39.08829934770187, MemAllocated=1.59GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:09:56,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:09:56,857] [INFO] [timer.py:260:stop] epoch=0/micro_step=1370/global_step=1370, RunningAvgSamplesPerSec=40.10883851625969, CurrSamplesPerSec=45.614152521463346, MemAllocated=0.83GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:02,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:02,666] [INFO] [timer.py:260:stop] epoch=0/micro_step=1380/global_step=1380, RunningAvgSamplesPerSec=40.073202106620116, CurrSamplesPerSec=39.74741749946546, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:07,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:07,849] [INFO] [timer.py:260:stop] epoch=0/micro_step=1390/global_step=1390, RunningAvgSamplesPerSec=40.077636428247054, CurrSamplesPerSec=42.79713711577743, MemAllocated=1.16GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:12,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:12,909] [INFO] [timer.py:260:stop] epoch=0/micro_step=1400/global_step=1400, RunningAvgSamplesPerSec=40.087887739968004, CurrSamplesPerSec=39.31581013264844, MemAllocated=1.59GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:18,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:18,198] [INFO] [timer.py:260:stop] epoch=0/micro_step=1410/global_step=1410, RunningAvgSamplesPerSec=40.084438158555606, CurrSamplesPerSec=39.15212939615672, MemAllocated=1.65GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:23,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:23,322] [INFO] [timer.py:260:stop] epoch=0/micro_step=1420/global_step=1420, RunningAvgSamplesPerSec=40.09131613365611, CurrSamplesPerSec=33.63566212402953, MemAllocated=2.22GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 12, 'loss_train': 0.1584159092973705, 'loss_val': 0.0038401823467230445, 'accuracy_train': 0.7008456659619451, 'accuracy_val': 0.7124735729386892}\n",
      "[2024-05-08 09:10:39,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:39,270] [INFO] [timer.py:260:stop] epoch=0/micro_step=1430/global_step=1430, RunningAvgSamplesPerSec=40.101862198706755, CurrSamplesPerSec=41.65150549373479, MemAllocated=1.28GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:44,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:44,756] [INFO] [timer.py:260:stop] epoch=0/micro_step=1440/global_step=1440, RunningAvgSamplesPerSec=40.0896639427694, CurrSamplesPerSec=44.30540414923733, MemAllocated=0.97GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:50,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:50,163] [INFO] [timer.py:260:stop] epoch=0/micro_step=1450/global_step=1450, RunningAvgSamplesPerSec=40.08321728228174, CurrSamplesPerSec=37.96207574781351, MemAllocated=1.71GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:10:55,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:10:55,683] [INFO] [timer.py:260:stop] epoch=0/micro_step=1460/global_step=1460, RunningAvgSamplesPerSec=40.06634538385904, CurrSamplesPerSec=27.297514179816012, MemAllocated=3.4GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:00,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:00,740] [INFO] [timer.py:260:stop] epoch=0/micro_step=1470/global_step=1470, RunningAvgSamplesPerSec=40.08189293302869, CurrSamplesPerSec=41.700473557873046, MemAllocated=1.3GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:06,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:06,049] [INFO] [timer.py:260:stop] epoch=0/micro_step=1480/global_step=1480, RunningAvgSamplesPerSec=40.08112857834693, CurrSamplesPerSec=39.67035041849352, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:11,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:11,291] [INFO] [timer.py:260:stop] epoch=0/micro_step=1490/global_step=1490, RunningAvgSamplesPerSec=40.0827795471028, CurrSamplesPerSec=39.922345529555585, MemAllocated=1.37GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:16,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:16,941] [INFO] [timer.py:260:stop] epoch=0/micro_step=1500/global_step=1500, RunningAvgSamplesPerSec=40.05787964895511, CurrSamplesPerSec=33.9061746558801, MemAllocated=2.21GB, MaxMemAllocated=6.9GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:11:22,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:22,038] [INFO] [timer.py:260:stop] epoch=0/micro_step=1510/global_step=1510, RunningAvgSamplesPerSec=40.066300270905586, CurrSamplesPerSec=45.55924839205921, MemAllocated=0.85GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:27,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:27,293] [INFO] [timer.py:260:stop] epoch=0/micro_step=1520/global_step=1520, RunningAvgSamplesPerSec=40.06642763965864, CurrSamplesPerSec=43.10845000305124, MemAllocated=1.11GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:32,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:32,537] [INFO] [timer.py:260:stop] epoch=0/micro_step=1530/global_step=1530, RunningAvgSamplesPerSec=40.06895511496044, CurrSamplesPerSec=42.858251167270076, MemAllocated=1.11GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:37,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:37,581] [INFO] [timer.py:260:stop] epoch=0/micro_step=1540/global_step=1540, RunningAvgSamplesPerSec=40.081451823076726, CurrSamplesPerSec=41.59563842730991, MemAllocated=1.34GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 13, 'loss_train': 0.10188565647375256, 'loss_val': 0.00435220665961945, 'accuracy_train': 0.757399577167019, 'accuracy_val': 0.8097251585623678}\n",
      "[2024-05-08 09:11:53,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:53,510] [INFO] [timer.py:260:stop] epoch=0/micro_step=1550/global_step=1550, RunningAvgSamplesPerSec=40.094448603249795, CurrSamplesPerSec=45.25579227974771, MemAllocated=0.84GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:11:58,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:11:58,788] [INFO] [timer.py:260:stop] epoch=0/micro_step=1560/global_step=1560, RunningAvgSamplesPerSec=40.09498620022265, CurrSamplesPerSec=39.916337841519166, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:03,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:03,959] [INFO] [timer.py:260:stop] epoch=0/micro_step=1570/global_step=1570, RunningAvgSamplesPerSec=40.100388636936565, CurrSamplesPerSec=39.50294319862117, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:09,255] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:09,256] [INFO] [timer.py:260:stop] epoch=0/micro_step=1580/global_step=1580, RunningAvgSamplesPerSec=40.096713245392976, CurrSamplesPerSec=31.20129289262408, MemAllocated=2.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:14,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:14,282] [INFO] [timer.py:260:stop] epoch=0/micro_step=1590/global_step=1590, RunningAvgSamplesPerSec=40.11198939603817, CurrSamplesPerSec=45.92028169842913, MemAllocated=0.77GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:19,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:19,800] [INFO] [timer.py:260:stop] epoch=0/micro_step=1600/global_step=1600, RunningAvgSamplesPerSec=40.09868012929817, CurrSamplesPerSec=42.23626527164842, MemAllocated=1.22GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:25,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:25,284] [INFO] [timer.py:260:stop] epoch=0/micro_step=1610/global_step=1610, RunningAvgSamplesPerSec=40.08777899063765, CurrSamplesPerSec=42.027410025626445, MemAllocated=1.24GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:31,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:31,071] [INFO] [timer.py:260:stop] epoch=0/micro_step=1620/global_step=1620, RunningAvgSamplesPerSec=40.05940003111098, CurrSamplesPerSec=37.37033565934338, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:36,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:36,190] [INFO] [timer.py:260:stop] epoch=0/micro_step=1630/global_step=1630, RunningAvgSamplesPerSec=40.06504736909637, CurrSamplesPerSec=43.44403128074991, MemAllocated=1.08GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:41,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:41,311] [INFO] [timer.py:260:stop] epoch=0/micro_step=1640/global_step=1640, RunningAvgSamplesPerSec=40.07185652603977, CurrSamplesPerSec=38.783292445769796, MemAllocated=1.65GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:46,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:46,503] [INFO] [timer.py:260:stop] epoch=0/micro_step=1650/global_step=1650, RunningAvgSamplesPerSec=40.07587867098949, CurrSamplesPerSec=44.14275631483639, MemAllocated=1.02GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:12:51,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:12:51,522] [INFO] [timer.py:260:stop] epoch=0/micro_step=1660/global_step=1660, RunningAvgSamplesPerSec=40.089569364499056, CurrSamplesPerSec=45.58948659160001, MemAllocated=0.72GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 14, 'loss_train': 0.11774104241336879, 'loss_val': 0.0013110299947145878, 'accuracy_train': 0.741014799154334, 'accuracy_val': 0.7991543340380549}\n",
      "[2024-05-08 09:13:07,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:07,741] [INFO] [timer.py:260:stop] epoch=0/micro_step=1670/global_step=1670, RunningAvgSamplesPerSec=40.08723727964697, CurrSamplesPerSec=27.177799925726525, MemAllocated=3.4GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:13,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:13,029] [INFO] [timer.py:260:stop] epoch=0/micro_step=1680/global_step=1680, RunningAvgSamplesPerSec=40.08765599107735, CurrSamplesPerSec=39.90423297664097, MemAllocated=1.42GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:18,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:18,048] [INFO] [timer.py:260:stop] epoch=0/micro_step=1690/global_step=1690, RunningAvgSamplesPerSec=40.100036786055455, CurrSamplesPerSec=42.62937290375038, MemAllocated=1.15GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:23,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:23,279] [INFO] [timer.py:260:stop] epoch=0/micro_step=1700/global_step=1700, RunningAvgSamplesPerSec=40.100066296017175, CurrSamplesPerSec=44.64385363273805, MemAllocated=1.02GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:28,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:28,708] [INFO] [timer.py:260:stop] epoch=0/micro_step=1710/global_step=1710, RunningAvgSamplesPerSec=40.09103399509197, CurrSamplesPerSec=39.146670959981755, MemAllocated=1.65GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:33,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:33,846] [INFO] [timer.py:260:stop] epoch=0/micro_step=1720/global_step=1720, RunningAvgSamplesPerSec=40.09806106426917, CurrSamplesPerSec=43.409887220355316, MemAllocated=1.03GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:39,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:39,208] [INFO] [timer.py:260:stop] epoch=0/micro_step=1730/global_step=1730, RunningAvgSamplesPerSec=40.09217358282826, CurrSamplesPerSec=31.377038127650742, MemAllocated=2.59GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:44,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:13:44,668] [INFO] [timer.py:260:stop] epoch=0/micro_step=1740/global_step=1740, RunningAvgSamplesPerSec=40.08432526787691, CurrSamplesPerSec=43.131362963857995, MemAllocated=0.96GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:49,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:49,738] [INFO] [timer.py:260:stop] epoch=0/micro_step=1750/global_step=1750, RunningAvgSamplesPerSec=40.093963365161, CurrSamplesPerSec=39.433330336561035, MemAllocated=1.47GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:13:55,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:13:55,240] [INFO] [timer.py:260:stop] epoch=0/micro_step=1760/global_step=1760, RunningAvgSamplesPerSec=40.082801881477906, CurrSamplesPerSec=35.70782829304007, MemAllocated=1.98GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:00,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:00,593] [INFO] [timer.py:260:stop] epoch=0/micro_step=1770/global_step=1770, RunningAvgSamplesPerSec=40.07948136549217, CurrSamplesPerSec=41.95292515799035, MemAllocated=1.27GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:05,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:05,731] [INFO] [timer.py:260:stop] epoch=0/micro_step=1780/global_step=1780, RunningAvgSamplesPerSec=40.08555476572559, CurrSamplesPerSec=43.414408574604046, MemAllocated=1.08GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 15, 'loss_train': 0.14387031539045983, 'loss_val': 0.02209963002114165, 'accuracy_train': 0.7325581395348837, 'accuracy_val': 0.6807610993657506}\n",
      "[2024-05-08 09:14:21,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:21,793] [INFO] [timer.py:260:stop] epoch=0/micro_step=1790/global_step=1790, RunningAvgSamplesPerSec=40.090580068575626, CurrSamplesPerSec=41.92380745333078, MemAllocated=1.24GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:26,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:26,996] [INFO] [timer.py:260:stop] epoch=0/micro_step=1800/global_step=1800, RunningAvgSamplesPerSec=40.09572434620341, CurrSamplesPerSec=43.19830164910747, MemAllocated=1.15GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:32,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:32,541] [INFO] [timer.py:260:stop] epoch=0/micro_step=1810/global_step=1810, RunningAvgSamplesPerSec=40.08167762305777, CurrSamplesPerSec=35.99297398389594, MemAllocated=1.71GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:37,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:37,591] [INFO] [timer.py:260:stop] epoch=0/micro_step=1820/global_step=1820, RunningAvgSamplesPerSec=40.09057421167503, CurrSamplesPerSec=41.8786515732062, MemAllocated=1.23GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:42,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:42,736] [INFO] [timer.py:260:stop] epoch=0/micro_step=1830/global_step=1830, RunningAvgSamplesPerSec=40.093453565071975, CurrSamplesPerSec=44.406137427775114, MemAllocated=0.9GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:48,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:48,284] [INFO] [timer.py:260:stop] epoch=0/micro_step=1840/global_step=1840, RunningAvgSamplesPerSec=40.07982215336785, CurrSamplesPerSec=39.985309276200624, MemAllocated=1.42GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:53,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:53,306] [INFO] [timer.py:260:stop] epoch=0/micro_step=1850/global_step=1850, RunningAvgSamplesPerSec=40.09172891972031, CurrSamplesPerSec=41.77437595707332, MemAllocated=1.3GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:14:58,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:14:58,790] [INFO] [timer.py:260:stop] epoch=0/micro_step=1860/global_step=1860, RunningAvgSamplesPerSec=40.07999044405705, CurrSamplesPerSec=35.46429043267061, MemAllocated=2.02GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:04,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:04,094] [INFO] [timer.py:260:stop] epoch=0/micro_step=1870/global_step=1870, RunningAvgSamplesPerSec=40.07956898955059, CurrSamplesPerSec=43.275464504941844, MemAllocated=1.09GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:09,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:09,362] [INFO] [timer.py:260:stop] epoch=0/micro_step=1880/global_step=1880, RunningAvgSamplesPerSec=40.08028123820577, CurrSamplesPerSec=44.27123666678321, MemAllocated=0.99GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:14,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:14,585] [INFO] [timer.py:260:stop] epoch=0/micro_step=1890/global_step=1890, RunningAvgSamplesPerSec=40.082306038386406, CurrSamplesPerSec=45.479373999128484, MemAllocated=0.77GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:20,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:20,029] [INFO] [timer.py:260:stop] epoch=0/micro_step=1900/global_step=1900, RunningAvgSamplesPerSec=40.07336666961856, CurrSamplesPerSec=44.48632309111929, MemAllocated=0.9GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 16, 'loss_train': 0.11735327992328377, 'loss_val': 0.001067405523255814, 'accuracy_train': 0.7605708245243129, 'accuracy_val': 0.8266384778012685}\n",
      "[2024-05-08 09:15:36,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:36,401] [INFO] [timer.py:260:stop] epoch=0/micro_step=1910/global_step=1910, RunningAvgSamplesPerSec=40.06664851551584, CurrSamplesPerSec=33.70123457782099, MemAllocated=2.17GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:41,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:41,548] [INFO] [timer.py:260:stop] epoch=0/micro_step=1920/global_step=1920, RunningAvgSamplesPerSec=40.07336744629968, CurrSamplesPerSec=42.447018759610195, MemAllocated=1.22GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:46,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:46,787] [INFO] [timer.py:260:stop] epoch=0/micro_step=1930/global_step=1930, RunningAvgSamplesPerSec=40.0758525568005, CurrSamplesPerSec=39.629026819579465, MemAllocated=1.52GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:51,893] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:51,894] [INFO] [timer.py:260:stop] epoch=0/micro_step=1940/global_step=1940, RunningAvgSamplesPerSec=40.08483483026537, CurrSamplesPerSec=43.56321024294804, MemAllocated=1.05GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:15:56,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:15:56,901] [INFO] [timer.py:260:stop] epoch=0/micro_step=1950/global_step=1950, RunningAvgSamplesPerSec=40.09556789776773, CurrSamplesPerSec=39.57256892294865, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:16:02,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:02,146] [INFO] [timer.py:260:stop] epoch=0/micro_step=1960/global_step=1960, RunningAvgSamplesPerSec=40.09710256023573, CurrSamplesPerSec=41.45129806958822, MemAllocated=1.28GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:16:07,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:07,649] [INFO] [timer.py:260:stop] epoch=0/micro_step=1970/global_step=1970, RunningAvgSamplesPerSec=40.08781987332519, CurrSamplesPerSec=42.17908470276787, MemAllocated=1.27GB, MaxMemAllocated=6.9GB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:16:12,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:12,933] [INFO] [timer.py:260:stop] epoch=0/micro_step=1980/global_step=1980, RunningAvgSamplesPerSec=40.08613003862605, CurrSamplesPerSec=27.684603865431818, MemAllocated=3.21GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:16:18,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:18,444] [INFO] [timer.py:260:stop] epoch=0/micro_step=1990/global_step=1990, RunningAvgSamplesPerSec=40.0770054256987, CurrSamplesPerSec=39.74812376321776, MemAllocated=1.47GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:16:23,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:23,649] [INFO] [timer.py:260:stop] epoch=0/micro_step=2000/global_step=2000, RunningAvgSamplesPerSec=40.08032049518351, CurrSamplesPerSec=42.65690327632937, MemAllocated=1.15GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:16:29,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:29,112] [INFO] [timer.py:260:stop] epoch=0/micro_step=2010/global_step=2010, RunningAvgSamplesPerSec=40.072530032757854, CurrSamplesPerSec=31.427129597542358, MemAllocated=2.58GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:16:34,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:34,452] [INFO] [timer.py:260:stop] epoch=0/micro_step=2020/global_step=2020, RunningAvgSamplesPerSec=40.07092461488494, CurrSamplesPerSec=43.136796934156614, MemAllocated=1.05GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 17, 'loss_train': 0.11925699670521452, 'loss_val': 0.00013252242170983086, 'accuracy_train': 0.742600422832981, 'accuracy_val': 0.7991543340380549}\n",
      "[2024-05-08 09:16:50,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:50,708] [INFO] [timer.py:260:stop] epoch=0/micro_step=2030/global_step=2030, RunningAvgSamplesPerSec=40.0709291301662, CurrSamplesPerSec=39.518482313840124, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:16:56,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:16:56,163] [INFO] [timer.py:260:stop] epoch=0/micro_step=2040/global_step=2040, RunningAvgSamplesPerSec=40.06340928714948, CurrSamplesPerSec=27.243855921366187, MemAllocated=3.4GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:01,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:01,432] [INFO] [timer.py:260:stop] epoch=0/micro_step=2050/global_step=2050, RunningAvgSamplesPerSec=40.06464660455178, CurrSamplesPerSec=38.80760389782747, MemAllocated=1.65GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:06,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:06,769] [INFO] [timer.py:260:stop] epoch=0/micro_step=2060/global_step=2060, RunningAvgSamplesPerSec=40.06261501179248, CurrSamplesPerSec=40.31933017470203, MemAllocated=1.4GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:11,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:11,812] [INFO] [timer.py:260:stop] epoch=0/micro_step=2070/global_step=2070, RunningAvgSamplesPerSec=40.072249449551016, CurrSamplesPerSec=42.61886871057799, MemAllocated=1.11GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:17,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:17,198] [INFO] [timer.py:260:stop] epoch=0/micro_step=2080/global_step=2080, RunningAvgSamplesPerSec=40.06791495498499, CurrSamplesPerSec=41.48748152319932, MemAllocated=1.34GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:22,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:22,370] [INFO] [timer.py:260:stop] epoch=0/micro_step=2090/global_step=2090, RunningAvgSamplesPerSec=40.072907402376025, CurrSamplesPerSec=44.80696092027084, MemAllocated=0.96GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:27,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:27,527] [INFO] [timer.py:260:stop] epoch=0/micro_step=2100/global_step=2100, RunningAvgSamplesPerSec=40.077627211554514, CurrSamplesPerSec=38.19988137401226, MemAllocated=1.66GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:32,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:32,872] [INFO] [timer.py:260:stop] epoch=0/micro_step=2110/global_step=2110, RunningAvgSamplesPerSec=40.07549412256091, CurrSamplesPerSec=41.7516608246948, MemAllocated=1.21GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:38,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:38,285] [INFO] [timer.py:260:stop] epoch=0/micro_step=2120/global_step=2120, RunningAvgSamplesPerSec=40.06717174118508, CurrSamplesPerSec=33.50700283748354, MemAllocated=2.22GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:43,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:43,602] [INFO] [timer.py:260:stop] epoch=0/micro_step=2130/global_step=2130, RunningAvgSamplesPerSec=40.063182879095805, CurrSamplesPerSec=39.0693432909158, MemAllocated=1.55GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:17:48,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:17:48,831] [INFO] [timer.py:260:stop] epoch=0/micro_step=2140/global_step=2140, RunningAvgSamplesPerSec=40.064764107749, CurrSamplesPerSec=33.883508635871586, MemAllocated=2.21GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 18, 'loss_train': 0.08762515070070424, 'loss_val': 0.0020222895745243127, 'accuracy_train': 0.7658562367864693, 'accuracy_val': 0.7885835095137421}\n",
      "[2024-05-08 09:18:04,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:04,998] [INFO] [timer.py:260:stop] epoch=0/micro_step=2150/global_step=2150, RunningAvgSamplesPerSec=40.064180913563824, CurrSamplesPerSec=37.6449512222896, MemAllocated=1.71GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:10,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:10,209] [INFO] [timer.py:260:stop] epoch=0/micro_step=2160/global_step=2160, RunningAvgSamplesPerSec=40.06436578566629, CurrSamplesPerSec=43.738819233935274, MemAllocated=1.09GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:15,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:15,324] [INFO] [timer.py:260:stop] epoch=0/micro_step=2170/global_step=2170, RunningAvgSamplesPerSec=40.06859127543806, CurrSamplesPerSec=40.06953863423314, MemAllocated=1.42GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:20,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:20,511] [INFO] [timer.py:260:stop] epoch=0/micro_step=2180/global_step=2180, RunningAvgSamplesPerSec=40.070367770755006, CurrSamplesPerSec=39.90205013556581, MemAllocated=1.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:25,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:25,699] [INFO] [timer.py:260:stop] epoch=0/micro_step=2190/global_step=2190, RunningAvgSamplesPerSec=40.07225943695364, CurrSamplesPerSec=31.725579258088246, MemAllocated=2.46GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:30,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:30,827] [INFO] [timer.py:260:stop] epoch=0/micro_step=2200/global_step=2200, RunningAvgSamplesPerSec=40.07826479071055, CurrSamplesPerSec=37.85502818727134, MemAllocated=1.71GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:36,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:18:36,174] [INFO] [timer.py:260:stop] epoch=0/micro_step=2210/global_step=2210, RunningAvgSamplesPerSec=40.07590807129036, CurrSamplesPerSec=40.289580785549056, MemAllocated=1.36GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:41,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:41,421] [INFO] [timer.py:260:stop] epoch=0/micro_step=2220/global_step=2220, RunningAvgSamplesPerSec=40.078028838280254, CurrSamplesPerSec=44.18143729368809, MemAllocated=1.02GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:46,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:46,793] [INFO] [timer.py:260:stop] epoch=0/micro_step=2230/global_step=2230, RunningAvgSamplesPerSec=40.073112428604, CurrSamplesPerSec=27.59983631469528, MemAllocated=3.21GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:51,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:51,877] [INFO] [timer.py:260:stop] epoch=0/micro_step=2240/global_step=2240, RunningAvgSamplesPerSec=40.08147625229971, CurrSamplesPerSec=45.2743248202759, MemAllocated=0.84GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:18:56,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:18:56,936] [INFO] [timer.py:260:stop] epoch=0/micro_step=2250/global_step=2250, RunningAvgSamplesPerSec=40.089867047857695, CurrSamplesPerSec=43.654742451524264, MemAllocated=1.08GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:02,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:02,584] [INFO] [timer.py:260:stop] epoch=0/micro_step=2260/global_step=2260, RunningAvgSamplesPerSec=40.07667593513264, CurrSamplesPerSec=41.76072832042614, MemAllocated=1.28GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 19, 'loss_train': 0.12206176370697344, 'loss_val': 3.8421179225026425e-05, 'accuracy_train': 0.7600422832980972, 'accuracy_val': 0.8456659619450317}\n",
      "[2024-05-08 09:19:18,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:18,572] [INFO] [timer.py:260:stop] epoch=0/micro_step=2270/global_step=2270, RunningAvgSamplesPerSec=40.083238095914844, CurrSamplesPerSec=38.72742895955855, MemAllocated=1.52GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:24,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:24,078] [INFO] [timer.py:260:stop] epoch=0/micro_step=2280/global_step=2280, RunningAvgSamplesPerSec=40.07387159683617, CurrSamplesPerSec=37.67477100582282, MemAllocated=1.59GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:29,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:29,466] [INFO] [timer.py:260:stop] epoch=0/micro_step=2290/global_step=2290, RunningAvgSamplesPerSec=40.0703682092133, CurrSamplesPerSec=39.607485593278355, MemAllocated=1.47GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:34,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:34,854] [INFO] [timer.py:260:stop] epoch=0/micro_step=2300/global_step=2300, RunningAvgSamplesPerSec=40.06614835258646, CurrSamplesPerSec=33.68837698689991, MemAllocated=2.17GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:40,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:40,069] [INFO] [timer.py:260:stop] epoch=0/micro_step=2310/global_step=2310, RunningAvgSamplesPerSec=40.068929534825045, CurrSamplesPerSec=34.61035945666334, MemAllocated=2.08GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:45,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:45,217] [INFO] [timer.py:260:stop] epoch=0/micro_step=2320/global_step=2320, RunningAvgSamplesPerSec=40.074621818355475, CurrSamplesPerSec=43.55026243516995, MemAllocated=0.97GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:50,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:50,478] [INFO] [timer.py:260:stop] epoch=0/micro_step=2330/global_step=2330, RunningAvgSamplesPerSec=40.07328424737476, CurrSamplesPerSec=27.584192194088143, MemAllocated=3.21GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:19:55,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:19:55,401] [INFO] [timer.py:260:stop] epoch=0/micro_step=2340/global_step=2340, RunningAvgSamplesPerSec=40.086365648883394, CurrSamplesPerSec=42.581442273282185, MemAllocated=1.15GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:20:00,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:20:00,451] [INFO] [timer.py:260:stop] epoch=0/micro_step=2350/global_step=2350, RunningAvgSamplesPerSec=40.09434718653185, CurrSamplesPerSec=42.89835965811135, MemAllocated=1.15GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:20:05,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:20:05,994] [INFO] [timer.py:260:stop] epoch=0/micro_step=2360/global_step=2360, RunningAvgSamplesPerSec=40.083874996626214, CurrSamplesPerSec=29.61632027651193, MemAllocated=2.9GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:20:11,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:20:11,522] [INFO] [timer.py:260:stop] epoch=0/micro_step=2370/global_step=2370, RunningAvgSamplesPerSec=40.07408614033062, CurrSamplesPerSec=45.0971032130298, MemAllocated=0.89GB, MaxMemAllocated=6.9GB\n",
      "[2024-05-08 09:20:16,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=22, lr=[0.001], mom=[[0.8, 0.999]]\n",
      "[2024-05-08 09:20:16,591] [INFO] [timer.py:260:stop] epoch=0/micro_step=2380/global_step=2380, RunningAvgSamplesPerSec=40.080123104492095, CurrSamplesPerSec=48.15538573037981, MemAllocated=0.44GB, MaxMemAllocated=6.9GB\n",
      "{'epoch': 20, 'loss_train': 0.10869869879637914, 'loss_val': 0.0007953926070295983, 'accuracy_train': 0.7822410147991543, 'accuracy_val': 0.8160676532769556}\n"
     ]
    }
   ],
   "source": [
    "hist = train_with_hist(model_engine, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0d3a2a32-9601-4458-82d3-b169d7e74ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_learning_curves(hist):\n",
    "    x_arr = np.arange(len(hist[0])) + 1\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    ax = fig.add_subplot(1, 2, 1)\n",
    "    ax.plot(x_arr, hist[0], '-o', label='Train loss')\n",
    "    ax.plot(x_arr, hist[1], '--<', label='Validation loss')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax = fig.add_subplot(1, 2, 2)\n",
    "    ax.plot(x_arr, hist[2], '-o', label='Train acc.')\n",
    "    ax.plot(x_arr, hist[3], '--<', label='Validation acc.')\n",
    "    ax.legend(fontsize=15)\n",
    "    ax.set_xlabel('Epoch', size=15)\n",
    "    ax.set_ylabel('Accuracy', size=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ad6a0c1b-3a25-4cff-96c6-3266913e208e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_learning_curves(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a230c-2886-4676-9a87-6bac8b503bb4",
   "metadata": {},
   "source": [
    "### Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29c316c-7bce-4176-b1fa-00dd769a6a1a",
   "metadata": {},
   "source": [
    "Checking the results of the test dataset…\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebc0b680-b1de-4aee-bad4-baf7f50b9f0e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.844\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking the results of test dataset.\")\n",
    "accu_test, _ = evaluate(model, test_dataloader)\n",
    "print(\"test accuracy {:8.3f}\".format(accu_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ef9b09f4-0640-4e59-aefc-9d2d136d9f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(model, dataloader):\n",
    "    model.eval()\n",
    "    y_test = np.asarray([])\n",
    "    y_predict = np.asarray([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (data, label) in enumerate(dataloader):\n",
    "            outputs = model(input_ids=data.input_ids, attention_mask=data.attention_mask)\n",
    "            predicted_label = outputs.logits\n",
    "                  \n",
    "            y_test = np.concatenate((y_test, np.asarray(label.to(device='cpu', dtype=torch.long))), axis=None)\n",
    "            y_predict = np.concatenate((y_predict, np.asarray((predicted_label.argmax(1).to(device='cpu', dtype=torch.long)))), axis=None)\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_predict)\n",
    "    sns.heatmap(cm, annot=True, fmt = \"d\")\n",
    "    print(classification_report(y_test, y_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2efeb419-1df7-4e6c-8814-ddf3294b9b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.97      0.84       109\n",
      "         1.0       0.97      0.75      0.85       154\n",
      "\n",
      "    accuracy                           0.84       263\n",
      "   macro avg       0.86      0.86      0.84       263\n",
      "weighted avg       0.88      0.84      0.84       263\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a457def5-9ac2-48c9-a2ef-69ead17b1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    with torch.no_grad():\n",
    "        encoded_text = tokenizer(text)\n",
    "        encoded_text.input_ids = torch.tensor(encoded_text.input_ids).to(device).unsqueeze(0)\n",
    "        encoded_text.attention_mask = torch.tensor(encoded_text.attention_mask).to(device).unsqueeze(0)\n",
    "\n",
    "        outputs = model(input_ids=encoded_text.input_ids, attention_mask=encoded_text.attention_mask)\n",
    "        predicted_label = outputs.logits\n",
    "        return predicted_label.argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40c741cb-e91b-406f-a1f7-3c0ff292ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a 0\n"
     ]
    }
   ],
   "source": [
    "ex_text_str = 'The ePump Software shall define Fault ID 1 as follows:'\n",
    "\n",
    "print(\"This is a %s\" % predict(ex_text_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "61cc330f-c045-4453-a430-d19c5cac6bc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"The IO Service shall select the XLR-PW DEV_INFO_DATA file if HPP_XLR_WIRING is grounded (logical 1) and bits AC_TYPE_BIT1 - AC_TYPE_BIT6 do not indicate a CFM engine configuration. NOTE: HPP_XLR_WIRING and bits AC_TYPE_BIT[1-6] are discrete inputs which are received on constant pins between hardware configurations. See 282100-ICD-x for more details.\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "587df1de-6144-420e-a789-7771209aeca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"I shall like waffles\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e3f924b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"Bumblebe is red\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0c0e6726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text = \"Bumblebee is red\"\n",
    "predict(pred_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "901886b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-08 09:48:01,724] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2380 is about to be saved!\n",
      "[2024-05-08 09:48:01,733] [INFO] [engine.py:3596:save_16bit_model] Saving model weights to ./models/phi/2024-05-08_phi.pth, tag: global_step2380\n",
      "[2024-05-08 09:48:01,734] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving ./models/phi/2024-05-08_phi.pth...\n",
      "[2024-05-08 09:48:06,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved ./models/phi/2024-05-08_phi.pth.\n",
      "[2024-05-08 09:48:06,475] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2380 is ready now!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "today = date.today().isoformat()\n",
    "model_engine.save_16bit_model(\"./models/phi/\", f\"{today}_phi.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dbaabd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('model.embed_tokens.weight',\n",
       "              tensor([[ 1.6998e-02, -1.3275e-02,  2.0309e-02,  ...,  1.7822e-02,\n",
       "                        5.1346e-03, -7.7343e-04],\n",
       "                      [ 9.7351e-03,  5.1636e-02,  1.5656e-02,  ..., -6.7329e-03,\n",
       "                        6.9389e-03, -1.1322e-02],\n",
       "                      [-4.6387e-02, -9.0942e-03, -1.1349e-03,  ..., -3.0945e-02,\n",
       "                        3.8940e-02,  1.3847e-02],\n",
       "                      ...,\n",
       "                      [-5.9605e-08,  5.9605e-08, -5.9605e-08,  ...,  1.5140e-05,\n",
       "                       -1.1206e-05,  1.7762e-05],\n",
       "                      [-0.0000e+00, -5.9605e-08, -1.1921e-07,  ..., -2.5094e-05,\n",
       "                        3.7730e-05,  2.0683e-05],\n",
       "                      [ 0.0000e+00, -5.9605e-08,  5.9605e-08,  ..., -1.6034e-05,\n",
       "                       -1.5676e-05, -3.5882e-05]], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0010,  0.0288, -0.0464,  ..., -0.0098, -0.0261, -0.0083],\n",
       "                      [-0.0224,  0.0126, -0.0141,  ...,  0.0216, -0.0114, -0.0125],\n",
       "                      [ 0.0063,  0.0238, -0.0100,  ...,  0.0184, -0.0166,  0.0017],\n",
       "                      ...,\n",
       "                      [-0.0430, -0.0136, -0.0269,  ...,  0.0118, -0.0137, -0.0004],\n",
       "                      [ 0.0174, -0.0046, -0.0175,  ..., -0.0065, -0.0007,  0.0347],\n",
       "                      [ 0.0204,  0.0208, -0.0122,  ..., -0.0397,  0.0011, -0.0171]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.dense.weight',\n",
       "              tensor([[ 7.4005e-03,  6.9275e-03, -2.6108e-02,  ...,  5.9128e-03,\n",
       "                        2.4475e-02, -2.4719e-02],\n",
       "                      [ 2.9236e-02,  8.4257e-04, -1.1093e-02,  ..., -1.7303e-02,\n",
       "                       -2.2629e-02, -6.9499e-05],\n",
       "                      [ 6.9351e-03,  2.3087e-02,  2.5063e-03,  ..., -8.8806e-03,\n",
       "                        1.9196e-02, -1.5236e-02],\n",
       "                      ...,\n",
       "                      [ 2.0294e-03, -1.7138e-03,  2.8091e-02,  ...,  6.6147e-03,\n",
       "                       -1.8299e-05,  1.3466e-02],\n",
       "                      [ 1.0735e-02,  5.3772e-02,  1.0033e-02,  ...,  1.1055e-02,\n",
       "                        1.1742e-02,  2.3499e-02],\n",
       "                      [ 3.3379e-03,  8.2626e-03,  2.3865e-02,  ...,  3.4302e-02,\n",
       "                       -3.8795e-03,  3.0197e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.0.self_attn.dense.bias',\n",
       "              tensor([ 0.0291,  0.0069,  0.0309,  ..., -0.0225, -0.0260,  0.0126],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.fc1.weight',\n",
       "              tensor([[-3.9520e-02,  2.5146e-02,  2.8549e-02,  ...,  8.4915e-03,\n",
       "                        3.6240e-03, -3.0701e-02],\n",
       "                      [ 1.8740e-03,  6.1378e-03, -1.4717e-02,  ..., -2.8748e-02,\n",
       "                        3.6097e-04, -1.1101e-03],\n",
       "                      [-2.9488e-03,  2.2873e-02, -4.3945e-02,  ...,  2.8782e-03,\n",
       "                       -2.1561e-02,  1.5556e-02],\n",
       "                      ...,\n",
       "                      [-2.7728e-04, -4.5240e-05, -5.2643e-04,  ..., -6.5947e-04,\n",
       "                        1.3151e-03,  8.2302e-04],\n",
       "                      [-4.6420e-04, -5.1308e-04,  1.2708e-04,  ...,  8.3351e-04,\n",
       "                        2.2185e-04,  5.7602e-04],\n",
       "                      [-1.7624e-03, -5.2691e-04,  4.9305e-04,  ...,  8.6498e-04,\n",
       "                       -4.1032e-04, -5.0688e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.fc1.bias',\n",
       "              tensor([-0.0089, -0.0109, -0.0138,  ..., -0.0003, -0.0011, -0.0001],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.fc2.weight',\n",
       "              tensor([[ 2.2308e-02, -5.2605e-03,  3.6697e-03,  ...,  3.4070e-04,\n",
       "                       -2.3019e-04, -8.0156e-04],\n",
       "                      [ 1.9714e-02,  4.9171e-03,  3.3813e-02,  ...,  2.2030e-04,\n",
       "                       -4.4227e-04,  1.4544e-03],\n",
       "                      [-2.7828e-03, -2.3010e-02, -5.4810e-02,  ...,  5.7983e-04,\n",
       "                       -6.9237e-04, -5.9843e-04],\n",
       "                      ...,\n",
       "                      [ 1.7944e-02, -1.6647e-02, -3.0945e-02,  ..., -2.1803e-04,\n",
       "                        1.4353e-04,  1.8907e-04],\n",
       "                      [ 1.2102e-03, -5.3291e-03,  1.3626e-02,  ...,  1.5497e-06,\n",
       "                       -3.4165e-04,  5.3310e-04],\n",
       "                      [-1.6190e-02, -1.0010e-02, -8.0109e-03,  ..., -4.9305e-04,\n",
       "                       -8.4925e-04,  3.1972e-04]], dtype=torch.float16)),\n",
       "             ('model.layers.0.mlp.fc2.bias',\n",
       "              tensor([ 0.0034, -0.0016,  0.0121,  ..., -0.0100, -0.0020,  0.0042],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.input_layernorm.weight',\n",
       "              tensor([0.2576, 0.2559, 0.2637,  ..., 0.2571, 0.2544, 0.2524],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.0.input_layernorm.bias',\n",
       "              tensor([-0.0062,  0.0076,  0.0087,  ..., -0.0238, -0.0254, -0.0201],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0287,  0.0176,  0.0214,  ..., -0.0504, -0.0303,  0.0087],\n",
       "                      [-0.0001,  0.0011,  0.0186,  ..., -0.0115, -0.0286,  0.0089],\n",
       "                      [ 0.0142, -0.0096, -0.0220,  ...,  0.0145,  0.0094, -0.0085],\n",
       "                      ...,\n",
       "                      [ 0.0047,  0.0018, -0.0099,  ...,  0.0108, -0.0152,  0.0076],\n",
       "                      [-0.0104,  0.0042, -0.0233,  ...,  0.0257, -0.0278, -0.0014],\n",
       "                      [ 0.0158,  0.0079,  0.0041,  ..., -0.0301,  0.0015, -0.0040]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.dense.weight',\n",
       "              tensor([[ 0.0065,  0.0210,  0.0005,  ...,  0.0022,  0.0194,  0.0019],\n",
       "                      [ 0.0071,  0.0123, -0.0010,  ..., -0.0343, -0.0212,  0.0255],\n",
       "                      [-0.0197, -0.0363,  0.0338,  ...,  0.0112, -0.0016, -0.0487],\n",
       "                      ...,\n",
       "                      [ 0.0026,  0.0116, -0.0202,  ..., -0.0114,  0.0029,  0.0021],\n",
       "                      [ 0.0037,  0.0038, -0.0147,  ..., -0.0022,  0.0201,  0.0117],\n",
       "                      [-0.0224,  0.0134,  0.0204,  ...,  0.0121,  0.0024,  0.0111]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.self_attn.dense.bias',\n",
       "              tensor([-0.0011,  0.0139,  0.0414,  ...,  0.0316,  0.0585,  0.0575],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.fc1.weight',\n",
       "              tensor([[ 0.0145,  0.0457, -0.0235,  ..., -0.0323,  0.0764,  0.0311],\n",
       "                      [ 0.0192,  0.0392, -0.0286,  ...,  0.0237,  0.0087,  0.0057],\n",
       "                      [ 0.0122,  0.0378,  0.0417,  ...,  0.0300,  0.0389,  0.0215],\n",
       "                      ...,\n",
       "                      [ 0.0146, -0.0099,  0.0065,  ..., -0.0194,  0.0074, -0.0074],\n",
       "                      [ 0.0091, -0.0005, -0.0054,  ..., -0.0112, -0.0044, -0.0148],\n",
       "                      [-0.0257,  0.0262, -0.0061,  ..., -0.0200, -0.0267,  0.0157]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.fc1.bias',\n",
       "              tensor([-0.0759, -0.0588, -0.0675,  ...,  0.0042, -0.0092, -0.0007],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.fc2.weight',\n",
       "              tensor([[-0.0034,  0.0057,  0.0014,  ..., -0.0075, -0.0038,  0.0181],\n",
       "                      [-0.0262, -0.0056,  0.0222,  ...,  0.0089,  0.0019, -0.0166],\n",
       "                      [-0.0616, -0.0048,  0.0175,  ..., -0.0107,  0.0098,  0.0038],\n",
       "                      ...,\n",
       "                      [-0.0261, -0.0081,  0.0220,  ...,  0.0141,  0.0078,  0.0123],\n",
       "                      [ 0.0418,  0.0533, -0.0064,  ...,  0.0037, -0.0015,  0.0095],\n",
       "                      [-0.0383, -0.0220, -0.0223,  ..., -0.0018,  0.0096, -0.0063]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.mlp.fc2.bias',\n",
       "              tensor([-0.0166, -0.0105,  0.0282,  ...,  0.0156, -0.0084,  0.0034],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.input_layernorm.weight',\n",
       "              tensor([0.3474, 0.3384, 0.2546,  ..., 0.4014, 0.4192, 0.3977],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.1.input_layernorm.bias',\n",
       "              tensor([-0.0181, -0.0461, -0.0308,  ..., -0.0376, -0.0509, -0.0167],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.query_key_value.weight',\n",
       "              tensor([[-0.0042,  0.0179,  0.0061,  ...,  0.0151,  0.0037, -0.0086],\n",
       "                      [-0.0080, -0.0036, -0.0018,  ..., -0.0040,  0.0196,  0.0044],\n",
       "                      [ 0.0117,  0.0226,  0.0236,  ..., -0.0140, -0.0149,  0.0210],\n",
       "                      ...,\n",
       "                      [-0.0042,  0.0208,  0.0092,  ..., -0.0316,  0.0080, -0.0124],\n",
       "                      [-0.0144,  0.0097,  0.0242,  ..., -0.0034, -0.0145, -0.0077],\n",
       "                      [-0.0068, -0.0254,  0.0077,  ...,  0.0487,  0.0088,  0.0172]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.dense.weight',\n",
       "              tensor([[-0.0277,  0.0078,  0.0099,  ...,  0.0012, -0.0095,  0.0255],\n",
       "                      [ 0.0012,  0.0010, -0.0092,  ..., -0.0061,  0.0192,  0.0274],\n",
       "                      [-0.0385,  0.0118, -0.0293,  ...,  0.0194,  0.0013,  0.0235],\n",
       "                      ...,\n",
       "                      [-0.0319,  0.0134,  0.0456,  ..., -0.0054,  0.0178,  0.0536],\n",
       "                      [ 0.0559,  0.0130, -0.0181,  ..., -0.0196,  0.0445,  0.0125],\n",
       "                      [ 0.0259,  0.0085, -0.0142,  ..., -0.0070,  0.0021, -0.0440]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.self_attn.dense.bias',\n",
       "              tensor([ 0.0322, -0.0098,  0.0215,  ..., -0.0195, -0.0062,  0.0864],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.fc1.weight',\n",
       "              tensor([[-0.0128, -0.0001,  0.0581,  ..., -0.0303,  0.0591,  0.0010],\n",
       "                      [ 0.0196, -0.0512, -0.0053,  ...,  0.0108,  0.0132,  0.0587],\n",
       "                      [-0.0120, -0.0115, -0.0007,  ..., -0.0604,  0.0105,  0.0763],\n",
       "                      ...,\n",
       "                      [-0.0143,  0.0084,  0.0149,  ...,  0.0118,  0.0342,  0.0061],\n",
       "                      [-0.0007,  0.0019,  0.0168,  ...,  0.0002,  0.0137,  0.0178],\n",
       "                      [-0.0089, -0.0052,  0.0162,  ...,  0.0095,  0.0110,  0.0101]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.fc1.bias',\n",
       "              tensor([-0.0568, -0.0500, -0.0627,  ...,  0.0137,  0.0066, -0.0010],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.fc2.weight',\n",
       "              tensor([[-0.0344,  0.0292,  0.0428,  ...,  0.0097,  0.0007,  0.0100],\n",
       "                      [-0.0337, -0.0170, -0.0245,  ..., -0.0078, -0.0090,  0.0019],\n",
       "                      [-0.0640, -0.0199, -0.0328,  ..., -0.0223, -0.0111, -0.0091],\n",
       "                      ...,\n",
       "                      [-0.0391, -0.0067, -0.0190,  ...,  0.0003,  0.0050, -0.0005],\n",
       "                      [-0.0439,  0.0329,  0.0529,  ..., -0.0179, -0.0156, -0.0078],\n",
       "                      [ 0.0080,  0.0169,  0.0174,  ..., -0.0132, -0.0107,  0.0036]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.mlp.fc2.bias',\n",
       "              tensor([-0.0198, -0.0131,  0.0277,  ...,  0.0086,  0.0075, -0.0081],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.input_layernorm.weight',\n",
       "              tensor([0.4011, 0.3906, 0.3223,  ..., 0.4294, 0.4387, 0.4277],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.2.input_layernorm.bias',\n",
       "              tensor([-0.0210, -0.0227,  0.0398,  ..., -0.0077, -0.0091, -0.0241],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.query_key_value.weight',\n",
       "              tensor([[ 1.0138e-03, -6.4819e-02,  3.5248e-02,  ..., -2.4918e-02,\n",
       "                        7.1831e-03,  1.9653e-02],\n",
       "                      [ 3.4607e-02, -2.4811e-02,  2.3773e-02,  ...,  1.0422e-02,\n",
       "                        9.0942e-03, -1.9241e-02],\n",
       "                      [ 6.5880e-03,  9.0103e-03, -1.3237e-03,  ...,  2.2369e-02,\n",
       "                       -1.9806e-02, -9.5673e-03],\n",
       "                      ...,\n",
       "                      [-9.2773e-03,  5.8136e-03,  2.9175e-02,  ...,  1.9424e-02,\n",
       "                        1.7136e-02, -5.4588e-03],\n",
       "                      [-4.4891e-02,  3.4119e-02,  1.8204e-02,  ...,  2.3186e-05,\n",
       "                        5.4398e-03, -2.5208e-02],\n",
       "                      [ 1.4320e-02, -3.2684e-02, -2.7603e-02,  ...,  1.2054e-03,\n",
       "                       -2.1057e-02, -6.8359e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.dense.weight',\n",
       "              tensor([[ 0.0069, -0.0212,  0.0083,  ...,  0.0098,  0.0467,  0.0296],\n",
       "                      [ 0.0129,  0.0085, -0.0088,  ...,  0.0091, -0.0355, -0.0037],\n",
       "                      [-0.0012, -0.0166, -0.0031,  ..., -0.0145,  0.0844, -0.0088],\n",
       "                      ...,\n",
       "                      [ 0.0324, -0.0210,  0.0197,  ..., -0.0058, -0.0108, -0.0087],\n",
       "                      [ 0.0278, -0.0111, -0.0240,  ...,  0.0191,  0.0360, -0.0418],\n",
       "                      [ 0.0324,  0.0125,  0.0234,  ..., -0.0016, -0.0232, -0.0089]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.self_attn.dense.bias',\n",
       "              tensor([-0.0014,  0.0163,  0.0272,  ..., -0.0083, -0.0203,  0.0158],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.fc1.weight',\n",
       "              tensor([[-0.0136,  0.0076, -0.0584,  ..., -0.0197,  0.0048, -0.0054],\n",
       "                      [ 0.0501, -0.0139, -0.0247,  ..., -0.0360,  0.0375,  0.0296],\n",
       "                      [ 0.0187, -0.0413, -0.0185,  ...,  0.0214,  0.0075, -0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0065, -0.0221,  0.0188,  ...,  0.0159,  0.0660, -0.0392],\n",
       "                      [-0.0259,  0.0197, -0.0145,  ...,  0.0296,  0.0262, -0.0142],\n",
       "                      [-0.0240,  0.0072,  0.0245,  ...,  0.0115,  0.0207, -0.0254]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.fc1.bias',\n",
       "              tensor([-0.0497, -0.0173, -0.0459,  ..., -0.0139, -0.0004, -0.0074],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.fc2.weight',\n",
       "              tensor([[-0.0097, -0.0242, -0.0200,  ...,  0.0008,  0.0061, -0.0003],\n",
       "                      [ 0.0173, -0.0153, -0.0400,  ..., -0.0327,  0.0004, -0.0182],\n",
       "                      [-0.0437,  0.0236,  0.0344,  ...,  0.0258,  0.0094, -0.0120],\n",
       "                      ...,\n",
       "                      [ 0.0142,  0.0021,  0.0077,  ...,  0.0037, -0.0016,  0.0104],\n",
       "                      [ 0.0040, -0.0160,  0.0112,  ...,  0.0119, -0.0133,  0.0349],\n",
       "                      [-0.0221, -0.0046,  0.0350,  ...,  0.0164, -0.0116, -0.0266]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.mlp.fc2.bias',\n",
       "              tensor([-0.0010, -0.0224,  0.0431,  ..., -0.0035,  0.0067,  0.0150],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.input_layernorm.weight',\n",
       "              tensor([0.4709, 0.4551, 0.4177,  ..., 0.4868, 0.4917, 0.4888],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.3.input_layernorm.bias',\n",
       "              tensor([-0.0213, -0.0114,  0.0718,  ..., -0.0094, -0.0103, -0.0165],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.query_key_value.weight',\n",
       "              tensor([[ 1.4915e-02,  3.9917e-02,  2.3132e-02,  ...,  7.6981e-03,\n",
       "                        4.1924e-03, -2.2263e-02],\n",
       "                      [-1.5671e-02, -1.3367e-02, -1.8509e-02,  ...,  1.1322e-02,\n",
       "                        1.3107e-02,  2.4643e-02],\n",
       "                      [-9.8495e-03, -2.7985e-02, -2.3613e-03,  ..., -3.2063e-03,\n",
       "                       -1.4313e-02,  2.4216e-02],\n",
       "                      ...,\n",
       "                      [-6.1417e-03, -5.6152e-02, -2.0638e-03,  ..., -3.0804e-03,\n",
       "                       -2.4475e-02,  4.8676e-03],\n",
       "                      [-9.5010e-05, -2.9397e-04,  1.0887e-02,  ..., -2.8896e-03,\n",
       "                        9.6817e-03,  8.3542e-03],\n",
       "                      [-2.5406e-02, -4.0527e-02, -4.1008e-03,  ..., -1.2840e-02,\n",
       "                       -3.5152e-03, -9.0103e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.dense.weight',\n",
       "              tensor([[ 0.0406, -0.0245,  0.0026,  ..., -0.0192,  0.0125, -0.0326],\n",
       "                      [-0.0013, -0.0036,  0.0081,  ...,  0.0562, -0.0142, -0.0290],\n",
       "                      [ 0.0072,  0.0330, -0.0114,  ...,  0.0099,  0.0288,  0.0043],\n",
       "                      ...,\n",
       "                      [ 0.0117, -0.0167,  0.0097,  ...,  0.0034,  0.0222,  0.0093],\n",
       "                      [-0.0118, -0.0006, -0.0161,  ..., -0.0091, -0.0087,  0.0190],\n",
       "                      [ 0.0119, -0.0100, -0.0350,  ..., -0.0270,  0.0062,  0.0164]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.self_attn.dense.bias',\n",
       "              tensor([ 0.0271, -0.0133,  0.0334,  ..., -0.0179,  0.0064, -0.0078],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.fc1.weight',\n",
       "              tensor([[ 0.0197, -0.0309,  0.0465,  ...,  0.0697, -0.0143,  0.0396],\n",
       "                      [-0.0229,  0.0220, -0.0351,  ..., -0.0112,  0.0383, -0.0137],\n",
       "                      [ 0.0039, -0.0175, -0.0025,  ...,  0.0352, -0.0125, -0.0265],\n",
       "                      ...,\n",
       "                      [ 0.0241, -0.0073, -0.0147,  ...,  0.0187, -0.0236, -0.0667],\n",
       "                      [ 0.0208, -0.0173, -0.0120,  ...,  0.0388, -0.0185, -0.0017],\n",
       "                      [ 0.0290,  0.0087, -0.0171,  ..., -0.0148, -0.0143, -0.0223]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.fc1.bias',\n",
       "              tensor([-0.0415, -0.0056, -0.0331,  ...,  0.0166, -0.0050, -0.0006],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.fc2.weight',\n",
       "              tensor([[ 8.7509e-03,  2.5955e-02,  1.0033e-02,  ..., -1.3786e-02,\n",
       "                       -1.4122e-02, -8.2245e-03],\n",
       "                      [-1.0727e-02, -3.3752e-02, -6.6147e-03,  ..., -1.4877e-02,\n",
       "                       -1.1581e-02,  1.3405e-02],\n",
       "                      [ 2.1591e-02,  1.1543e-02, -2.6901e-02,  ..., -3.5496e-03,\n",
       "                       -2.2797e-02,  4.0779e-03],\n",
       "                      ...,\n",
       "                      [-1.6632e-02,  2.0645e-02,  1.5732e-02,  ..., -1.2161e-02,\n",
       "                       -2.3636e-02, -6.0081e-04],\n",
       "                      [ 5.4016e-03, -9.9063e-05, -2.4368e-02,  ...,  3.4454e-02,\n",
       "                        8.0719e-03,  2.3376e-02],\n",
       "                      [ 3.5004e-02, -2.7649e-02, -3.1204e-03,  ...,  2.0126e-02,\n",
       "                       -6.7863e-03, -9.9106e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.4.mlp.fc2.bias',\n",
       "              tensor([ 0.0056, -0.0193,  0.0410,  ...,  0.0058,  0.0120,  0.0101],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.input_layernorm.weight',\n",
       "              tensor([0.4580, 0.4580, 0.4241,  ..., 0.4858, 0.4751, 0.4773],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.4.input_layernorm.bias',\n",
       "              tensor([-0.0183, -0.0162,  0.0682,  ..., -0.0079, -0.0137, -0.0155],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0035,  0.0053, -0.0041,  ..., -0.0189,  0.0110, -0.0348],\n",
       "                      [ 0.0193, -0.0352,  0.0080,  ..., -0.0133,  0.0300, -0.0306],\n",
       "                      [ 0.0078,  0.0077,  0.0310,  ...,  0.0180,  0.0129, -0.0061],\n",
       "                      ...,\n",
       "                      [-0.0189, -0.0094,  0.0026,  ..., -0.0136, -0.0132, -0.0052],\n",
       "                      [-0.0062, -0.0155,  0.0218,  ...,  0.0125,  0.0223,  0.0248],\n",
       "                      [ 0.0114, -0.0144,  0.0090,  ..., -0.0397, -0.0099,  0.0066]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.dense.weight',\n",
       "              tensor([[ 0.0008,  0.0495, -0.0135,  ..., -0.0310,  0.0086, -0.0061],\n",
       "                      [ 0.0007, -0.0524, -0.0187,  ..., -0.0385,  0.0170, -0.0356],\n",
       "                      [-0.0059,  0.0424, -0.0439,  ...,  0.0005, -0.0045,  0.0147],\n",
       "                      ...,\n",
       "                      [-0.0045,  0.0019,  0.0201,  ..., -0.0061,  0.0462, -0.0208],\n",
       "                      [ 0.0095, -0.0186, -0.0509,  ...,  0.0029,  0.0341,  0.0025],\n",
       "                      [ 0.0030,  0.0118,  0.0143,  ..., -0.0345,  0.0284, -0.0038]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.self_attn.dense.bias',\n",
       "              tensor([ 0.0115, -0.0013,  0.0188,  ..., -0.0446,  0.0236,  0.0071],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.fc1.weight',\n",
       "              tensor([[ 0.0388, -0.0557, -0.0576,  ..., -0.0694, -0.0350, -0.0055],\n",
       "                      [-0.0443, -0.0017, -0.0108,  ...,  0.0139,  0.0090, -0.0838],\n",
       "                      [-0.0288, -0.0045,  0.0014,  ...,  0.0199,  0.0596,  0.0303],\n",
       "                      ...,\n",
       "                      [ 0.0051, -0.0142, -0.0118,  ..., -0.0437,  0.0443, -0.0608],\n",
       "                      [ 0.0286, -0.0262, -0.0099,  ...,  0.0542, -0.0130, -0.0206],\n",
       "                      [ 0.0132,  0.0463, -0.0352,  ...,  0.0188,  0.0117, -0.0108]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.fc1.bias',\n",
       "              tensor([-3.4729e-02, -4.4342e-02, -5.1666e-02,  ..., -3.4928e-05,\n",
       "                      -3.9368e-03, -1.9608e-03], dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.fc2.weight',\n",
       "              tensor([[ 0.0151, -0.0213, -0.0046,  ..., -0.0163, -0.0030, -0.0204],\n",
       "                      [-0.0275,  0.0022,  0.0123,  ...,  0.0063,  0.0065, -0.0100],\n",
       "                      [ 0.0147, -0.0016,  0.0362,  ..., -0.0042, -0.0096,  0.0424],\n",
       "                      ...,\n",
       "                      [-0.0040, -0.0016,  0.0251,  ...,  0.0438, -0.0367, -0.0208],\n",
       "                      [-0.0213,  0.0177, -0.0056,  ..., -0.0346,  0.0172, -0.0157],\n",
       "                      [ 0.0085, -0.0095, -0.0093,  ...,  0.0515,  0.0029, -0.0012]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.mlp.fc2.bias',\n",
       "              tensor([ 0.0128, -0.0145,  0.0635,  ...,  0.0260,  0.0016, -0.0041],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.input_layernorm.weight',\n",
       "              tensor([0.4224, 0.4263, 0.4116,  ..., 0.4309, 0.4329, 0.4336],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.5.input_layernorm.bias',\n",
       "              tensor([-0.0174, -0.0139,  0.0710,  ..., -0.0078, -0.0114, -0.0136],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0008,  0.0104, -0.0179,  ...,  0.0204,  0.0067, -0.0091],\n",
       "                      [-0.0189, -0.0315,  0.0117,  ...,  0.0148,  0.0204, -0.0090],\n",
       "                      [ 0.0448, -0.0312, -0.0066,  ...,  0.0147, -0.0118,  0.0400],\n",
       "                      ...,\n",
       "                      [ 0.0249, -0.0064,  0.0215,  ..., -0.0168,  0.0133,  0.0376],\n",
       "                      [-0.0224,  0.0486, -0.0219,  ..., -0.0144,  0.0040, -0.0094],\n",
       "                      [-0.0338, -0.0198, -0.0240,  ..., -0.0043,  0.0373, -0.0210]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.dense.weight',\n",
       "              tensor([[ 1.9236e-03,  1.6113e-02,  7.8659e-03,  ...,  2.7390e-02,\n",
       "                       -4.2603e-02, -1.6205e-02],\n",
       "                      [-3.0563e-02,  3.1921e-02, -5.8105e-02,  ...,  2.3115e-04,\n",
       "                        3.2562e-02, -5.5504e-03],\n",
       "                      [-2.2598e-02, -1.3268e-02, -2.5925e-02,  ..., -2.6512e-03,\n",
       "                       -4.3945e-02, -1.5793e-02],\n",
       "                      ...,\n",
       "                      [ 3.6957e-02, -1.3947e-05, -8.4734e-04,  ...,  3.2349e-02,\n",
       "                       -6.3553e-03,  1.0521e-02],\n",
       "                      [ 1.6983e-02, -4.8103e-03,  1.4900e-02,  ...,  1.7487e-02,\n",
       "                       -3.3569e-02,  1.9867e-02],\n",
       "                      [-1.4030e-02,  7.9041e-03, -1.8677e-02,  ...,  9.1400e-03,\n",
       "                        1.2337e-02, -3.7506e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.6.self_attn.dense.bias',\n",
       "              tensor([-0.0053,  0.0113, -0.0191,  ..., -0.0374,  0.0113,  0.0245],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.fc1.weight',\n",
       "              tensor([[ 0.0190, -0.0715, -0.0313,  ..., -0.0201, -0.0688, -0.0107],\n",
       "                      [-0.0024, -0.0344, -0.0902,  ...,  0.0295,  0.0736, -0.0241],\n",
       "                      [ 0.0358,  0.0245,  0.0428,  ..., -0.0412,  0.0325,  0.0723],\n",
       "                      ...,\n",
       "                      [-0.0101,  0.0156,  0.0435,  ..., -0.0037,  0.0215, -0.0364],\n",
       "                      [-0.0298,  0.0501,  0.0007,  ...,  0.0122,  0.0212, -0.0046],\n",
       "                      [-0.0050,  0.0150,  0.0257,  ...,  0.0030,  0.0123, -0.0359]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.fc1.bias',\n",
       "              tensor([-0.0321, -0.0439, -0.0569,  ..., -0.0095, -0.0272, -0.0102],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.fc2.weight',\n",
       "              tensor([[ 0.0455,  0.0121,  0.0214,  ..., -0.0043,  0.0367,  0.0103],\n",
       "                      [-0.0326, -0.0286, -0.0068,  ..., -0.0044, -0.0086, -0.0025],\n",
       "                      [-0.0016, -0.0052, -0.0089,  ..., -0.0214,  0.0072, -0.0249],\n",
       "                      ...,\n",
       "                      [-0.0032,  0.0069, -0.0168,  ..., -0.0134, -0.0363,  0.0100],\n",
       "                      [ 0.0377,  0.0017, -0.0057,  ...,  0.0103,  0.0850,  0.0235],\n",
       "                      [-0.0102,  0.0036,  0.0182,  ...,  0.0285, -0.0326,  0.0195]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.mlp.fc2.bias',\n",
       "              tensor([ 0.0329, -0.0193,  0.0616,  ...,  0.0328,  0.0147,  0.0137],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.input_layernorm.weight',\n",
       "              tensor([0.4507, 0.4495, 0.4343,  ..., 0.4597, 0.4465, 0.4534],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.6.input_layernorm.bias',\n",
       "              tensor([-0.0160, -0.0143,  0.0925,  ..., -0.0066, -0.0086, -0.0194],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.query_key_value.weight',\n",
       "              tensor([[-0.0275,  0.0274,  0.0147,  ..., -0.0026,  0.0068, -0.0130],\n",
       "                      [-0.0164, -0.0181,  0.0278,  ...,  0.0153, -0.0281,  0.0240],\n",
       "                      [-0.0077, -0.0047,  0.0124,  ...,  0.0239, -0.0227,  0.0030],\n",
       "                      ...,\n",
       "                      [ 0.0270,  0.0290,  0.0232,  ..., -0.0287, -0.0133,  0.0353],\n",
       "                      [ 0.0365, -0.0013, -0.0194,  ..., -0.0201,  0.0209, -0.0339],\n",
       "                      [-0.0113, -0.0160,  0.0036,  ...,  0.0381, -0.0097, -0.0492]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.dense.weight',\n",
       "              tensor([[-0.0241,  0.0111, -0.0249,  ...,  0.0081, -0.0104,  0.0016],\n",
       "                      [ 0.0338,  0.0018,  0.0020,  ..., -0.0042, -0.0283, -0.0076],\n",
       "                      [ 0.0274,  0.0153,  0.0614,  ..., -0.0059,  0.0174, -0.0210],\n",
       "                      ...,\n",
       "                      [ 0.0208, -0.0172,  0.0305,  ...,  0.0131,  0.0181,  0.0230],\n",
       "                      [ 0.0282,  0.0016, -0.0722,  ..., -0.0038,  0.0732, -0.0049],\n",
       "                      [-0.0057, -0.0543, -0.0491,  ..., -0.0030,  0.0365, -0.0090]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.self_attn.dense.bias',\n",
       "              tensor([0.0299, 0.0041, 0.0151,  ..., 0.0077, 0.0111, 0.0247],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.fc1.weight',\n",
       "              tensor([[ 0.0679,  0.0486, -0.0488,  ..., -0.0202,  0.0399, -0.0545],\n",
       "                      [ 0.0184, -0.0481, -0.0619,  ..., -0.0064,  0.0127, -0.0004],\n",
       "                      [-0.0046, -0.0300, -0.0471,  ..., -0.0363, -0.0193, -0.0454],\n",
       "                      ...,\n",
       "                      [ 0.0191, -0.0437, -0.0208,  ..., -0.0125, -0.0391, -0.0143],\n",
       "                      [-0.0331,  0.0077, -0.0074,  ..., -0.0055, -0.0353,  0.0014],\n",
       "                      [ 0.0066,  0.0231, -0.0131,  ..., -0.0057, -0.0084,  0.0338]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.fc1.bias',\n",
       "              tensor([-0.0374, -0.0427, -0.0417,  ...,  0.0103,  0.0071,  0.0121],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.fc2.weight',\n",
       "              tensor([[ 0.0472,  0.0248, -0.0045,  ..., -0.0181,  0.0248,  0.0019],\n",
       "                      [ 0.0240, -0.0065,  0.0038,  ...,  0.0144,  0.0035, -0.0074],\n",
       "                      [-0.0395, -0.0118,  0.0056,  ...,  0.0212,  0.0103,  0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0361,  0.0323, -0.0271,  ...,  0.0077,  0.0173,  0.0331],\n",
       "                      [ 0.0135, -0.0148,  0.0162,  ...,  0.0408,  0.0317, -0.0139],\n",
       "                      [ 0.0319, -0.0136, -0.0302,  ..., -0.0091,  0.0057, -0.0167]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.mlp.fc2.bias',\n",
       "              tensor([ 0.0213, -0.0051,  0.0466,  ...,  0.0431, -0.0113, -0.0084],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.input_layernorm.weight',\n",
       "              tensor([0.4543, 0.4556, 0.4368,  ..., 0.4595, 0.4497, 0.4585],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.7.input_layernorm.bias',\n",
       "              tensor([-0.0157, -0.0177,  0.0913,  ..., -0.0037, -0.0090, -0.0137],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0083, -0.0046, -0.0130,  ...,  0.0085,  0.0023,  0.0075],\n",
       "                      [ 0.0012,  0.0183,  0.0040,  ..., -0.0027,  0.0073,  0.0194],\n",
       "                      [-0.0107,  0.0026, -0.0112,  ...,  0.0390, -0.0010,  0.0432],\n",
       "                      ...,\n",
       "                      [-0.0128, -0.0162, -0.0174,  ..., -0.0024,  0.0058, -0.0222],\n",
       "                      [-0.0178,  0.0074, -0.0338,  ..., -0.0097,  0.0024, -0.0152],\n",
       "                      [-0.0042, -0.0090, -0.0152,  ..., -0.0069, -0.0220, -0.0071]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.dense.weight',\n",
       "              tensor([[-0.0443,  0.0221, -0.0510,  ...,  0.0085, -0.0150, -0.0077],\n",
       "                      [-0.0072, -0.0618, -0.0271,  ..., -0.0375, -0.0024,  0.0336],\n",
       "                      [-0.0335,  0.0414,  0.0008,  ...,  0.0149, -0.0173, -0.0439],\n",
       "                      ...,\n",
       "                      [-0.0168,  0.0233,  0.0012,  ..., -0.0012, -0.0225,  0.0136],\n",
       "                      [-0.0022,  0.0287,  0.0039,  ..., -0.0615,  0.0155, -0.0057],\n",
       "                      [-0.0074,  0.0031, -0.0355,  ...,  0.0041,  0.0283,  0.0193]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.self_attn.dense.bias',\n",
       "              tensor([ 0.0104, -0.0029,  0.0104,  ..., -0.0240,  0.0224, -0.0060],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.fc1.weight',\n",
       "              tensor([[ 2.1305e-03, -3.4119e-02,  2.1057e-03,  ...,  2.2293e-02,\n",
       "                       -4.0619e-02,  4.5410e-02],\n",
       "                      [-2.3438e-02, -4.3831e-03, -2.5387e-03,  ..., -1.5266e-02,\n",
       "                       -2.9785e-02,  7.1983e-03],\n",
       "                      [-2.0996e-02, -3.5339e-02, -4.2358e-02,  ..., -5.0903e-02,\n",
       "                        2.8000e-02, -1.9388e-03],\n",
       "                      ...,\n",
       "                      [-5.4121e-04, -2.9816e-02, -2.3544e-02,  ...,  6.9336e-02,\n",
       "                        3.1776e-03, -3.0487e-02],\n",
       "                      [ 3.9368e-03,  8.9111e-03, -2.5650e-02,  ...,  5.5054e-02,\n",
       "                        7.5111e-03,  4.9286e-03],\n",
       "                      [ 5.2071e-03,  2.0096e-02,  1.6332e-05,  ..., -1.8082e-02,\n",
       "                       -6.0364e-02, -1.1070e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.fc1.bias',\n",
       "              tensor([-0.0462, -0.0233, -0.0525,  ...,  0.0061, -0.0135, -0.0105],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.fc2.weight',\n",
       "              tensor([[ 1.7838e-02, -1.0582e-02,  2.7573e-02,  ..., -1.3374e-02,\n",
       "                        7.9803e-03,  9.2697e-04],\n",
       "                      [ 8.4305e-03, -2.4490e-02,  2.1118e-02,  ...,  2.2766e-02,\n",
       "                        1.5230e-03,  2.0172e-02],\n",
       "                      [ 8.9216e-04, -1.7441e-02, -2.7191e-02,  ...,  2.0569e-02,\n",
       "                        3.7174e-03, -5.3215e-03],\n",
       "                      ...,\n",
       "                      [ 4.0100e-02, -3.6072e-02,  2.8976e-02,  ..., -3.9406e-03,\n",
       "                       -2.8976e-02,  2.7481e-02],\n",
       "                      [ 1.3748e-02, -1.0422e-02,  1.7300e-03,  ...,  1.0841e-02,\n",
       "                       -1.3985e-02,  4.1321e-02],\n",
       "                      [-7.5996e-05,  3.7323e-02, -1.5793e-02,  ..., -2.1652e-02,\n",
       "                       -4.2295e-04,  3.2867e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.8.mlp.fc2.bias',\n",
       "              tensor([ 0.0039,  0.0040,  0.0622,  ...,  0.0499, -0.0087,  0.0111],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.input_layernorm.weight',\n",
       "              tensor([0.4629, 0.4592, 0.4199,  ..., 0.4731, 0.4595, 0.4592],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.8.input_layernorm.bias',\n",
       "              tensor([-0.0170, -0.0168,  0.0950,  ..., -0.0042, -0.0135, -0.0182],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.query_key_value.weight',\n",
       "              tensor([[ 6.1836e-03, -1.5129e-02, -7.9107e-04,  ...,  1.2321e-03,\n",
       "                        2.6764e-02, -3.7781e-02],\n",
       "                      [-2.7542e-02, -1.3092e-02, -3.8376e-03,  ..., -7.3586e-03,\n",
       "                        4.6722e-02, -1.1047e-02],\n",
       "                      [-2.2308e-02, -2.2278e-02, -1.9779e-03,  ..., -1.4496e-02,\n",
       "                       -4.6692e-03, -2.0096e-02],\n",
       "                      ...,\n",
       "                      [-2.3315e-02, -1.9943e-02, -2.1866e-02,  ..., -1.4244e-02,\n",
       "                        3.6957e-02, -2.5467e-02],\n",
       "                      [ 5.1842e-03, -2.0538e-02, -1.1414e-02,  ...,  1.8143e-02,\n",
       "                        9.4910e-03, -1.7227e-02],\n",
       "                      [-1.0729e-05, -1.9226e-02,  1.1620e-02,  ...,  3.6957e-02,\n",
       "                       -1.4832e-02, -1.2314e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.dense.weight',\n",
       "              tensor([[-0.0170,  0.0572, -0.0224,  ...,  0.0083,  0.0045, -0.0320],\n",
       "                      [-0.0270, -0.0353,  0.0396,  ...,  0.0187, -0.0298, -0.0113],\n",
       "                      [-0.0391, -0.0205, -0.0341,  ...,  0.0174, -0.0256, -0.0114],\n",
       "                      ...,\n",
       "                      [ 0.0020, -0.0149, -0.0589,  ..., -0.0248,  0.0269,  0.0070],\n",
       "                      [-0.0420,  0.0545, -0.0345,  ..., -0.0034, -0.0334,  0.0248],\n",
       "                      [ 0.0287, -0.0166,  0.0124,  ...,  0.0068, -0.0373,  0.0483]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.self_attn.dense.bias',\n",
       "              tensor([ 0.0109,  0.0095, -0.0092,  ..., -0.0059,  0.0447,  0.0030],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.fc1.weight',\n",
       "              tensor([[-0.0059, -0.0485, -0.0023,  ...,  0.0387, -0.0663, -0.0262],\n",
       "                      [-0.0057,  0.0130,  0.0074,  ..., -0.0190,  0.0493, -0.0238],\n",
       "                      [ 0.0074, -0.0286, -0.0338,  ..., -0.0032, -0.0084,  0.0184],\n",
       "                      ...,\n",
       "                      [ 0.0176,  0.0052, -0.0358,  ..., -0.0099, -0.0221, -0.0058],\n",
       "                      [ 0.0627, -0.0076, -0.0405,  ..., -0.0042,  0.0107,  0.0267],\n",
       "                      [-0.0096,  0.0177, -0.0083,  ...,  0.0029, -0.0437,  0.0623]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.fc1.bias',\n",
       "              tensor([-0.0136, -0.0301,  0.0002,  ...,  0.0161, -0.0032,  0.0123],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.fc2.weight',\n",
       "              tensor([[ 0.0358, -0.0038,  0.0046,  ...,  0.0212, -0.0197,  0.0217],\n",
       "                      [ 0.0070,  0.0482,  0.0142,  ..., -0.0089,  0.0115, -0.0253],\n",
       "                      [-0.0074,  0.0074,  0.0095,  ...,  0.0301,  0.0124, -0.0027],\n",
       "                      ...,\n",
       "                      [-0.0251,  0.0107,  0.0157,  ...,  0.0255, -0.0068, -0.0018],\n",
       "                      [ 0.0090, -0.0088,  0.0314,  ...,  0.0060,  0.0085,  0.0468],\n",
       "                      [-0.0159, -0.0109, -0.0601,  ...,  0.0387, -0.0086, -0.0249]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.mlp.fc2.bias',\n",
       "              tensor([ 0.0072,  0.0025,  0.0485,  ...,  0.0339, -0.0624, -0.0143],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.input_layernorm.weight',\n",
       "              tensor([0.4541, 0.4553, 0.4238,  ..., 0.4729, 0.4585, 0.4590],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.9.input_layernorm.bias',\n",
       "              tensor([-0.0178, -0.0173,  0.1076,  ...,  0.0015, -0.0130, -0.0144],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0132,  0.0190,  0.0334,  ..., -0.0072,  0.0073,  0.0103],\n",
       "                      [-0.0059, -0.0056,  0.0080,  ..., -0.0103,  0.0116,  0.0004],\n",
       "                      [-0.0045, -0.0120,  0.0273,  ...,  0.0022,  0.0308,  0.0224],\n",
       "                      ...,\n",
       "                      [-0.0031,  0.0226,  0.0158,  ..., -0.0021,  0.0103, -0.0018],\n",
       "                      [-0.0352, -0.0003,  0.0046,  ..., -0.0181, -0.0424, -0.0096],\n",
       "                      [ 0.0025,  0.0464,  0.0210,  ..., -0.0396,  0.0140, -0.0015]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.dense.weight',\n",
       "              tensor([[-0.0143, -0.0163, -0.0343,  ...,  0.0211, -0.0042, -0.0043],\n",
       "                      [-0.0193,  0.0197,  0.0247,  ...,  0.0239,  0.0472,  0.0229],\n",
       "                      [-0.0240,  0.0320,  0.0302,  ..., -0.0067,  0.0092,  0.0013],\n",
       "                      ...,\n",
       "                      [ 0.0303, -0.0782,  0.0262,  ..., -0.0008,  0.0020,  0.0356],\n",
       "                      [-0.0081, -0.0480, -0.0249,  ...,  0.0144, -0.0015,  0.0139],\n",
       "                      [ 0.0536, -0.0083, -0.0319,  ..., -0.0026, -0.0247,  0.0323]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.self_attn.dense.bias',\n",
       "              tensor([0.0210, 0.0162, 0.0228,  ..., 0.0125, 0.0410, 0.0018],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.fc1.weight',\n",
       "              tensor([[-0.0097, -0.0380, -0.0180,  ..., -0.0195,  0.0253, -0.0159],\n",
       "                      [-0.0326, -0.0411, -0.0108,  ...,  0.0572,  0.0187, -0.0291],\n",
       "                      [-0.0608, -0.0166, -0.0017,  ..., -0.0170,  0.0031,  0.0054],\n",
       "                      ...,\n",
       "                      [ 0.0086, -0.0172, -0.0028,  ..., -0.0092,  0.0582,  0.0259],\n",
       "                      [ 0.0542,  0.0204,  0.0312,  ..., -0.0070, -0.0106, -0.0041],\n",
       "                      [ 0.0514,  0.0245, -0.0207,  ..., -0.0062, -0.0221,  0.0380]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.fc1.bias',\n",
       "              tensor([-0.0075, -0.0345,  0.0022,  ..., -0.0104, -0.0084, -0.0035],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.fc2.weight',\n",
       "              tensor([[ 0.0392,  0.0084,  0.0263,  ..., -0.0100, -0.0304, -0.0179],\n",
       "                      [ 0.0033,  0.0093,  0.0359,  ...,  0.0295, -0.0082, -0.0309],\n",
       "                      [-0.0021, -0.0369, -0.0254,  ..., -0.0016, -0.0117,  0.0031],\n",
       "                      ...,\n",
       "                      [ 0.0257, -0.0131, -0.0011,  ...,  0.0095, -0.0050,  0.0112],\n",
       "                      [-0.0215,  0.0002,  0.0219,  ..., -0.0365,  0.0097,  0.0070],\n",
       "                      [ 0.0070,  0.0094, -0.0132,  ..., -0.0009,  0.0023, -0.0150]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.mlp.fc2.bias',\n",
       "              tensor([ 0.0093, -0.0021,  0.0353,  ...,  0.0368, -0.0518, -0.0007],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.input_layernorm.weight',\n",
       "              tensor([0.4529, 0.4470, 0.4116,  ..., 0.4658, 0.4539, 0.4478],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.10.input_layernorm.bias',\n",
       "              tensor([-0.0185, -0.0145,  0.0875,  ...,  0.0041, -0.0168, -0.0168],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0291,  0.0059,  0.0065,  ...,  0.0466, -0.0250, -0.0248],\n",
       "                      [ 0.0143,  0.0345,  0.0157,  ...,  0.0108,  0.0099, -0.0230],\n",
       "                      [-0.0146, -0.0138, -0.0252,  ...,  0.0156, -0.0079, -0.0085],\n",
       "                      ...,\n",
       "                      [-0.0056, -0.0046,  0.0139,  ...,  0.0048, -0.0005, -0.0043],\n",
       "                      [-0.0561, -0.0191,  0.0042,  ...,  0.0092, -0.0257, -0.0111],\n",
       "                      [ 0.0231,  0.0049,  0.0154,  ...,  0.0204, -0.0004, -0.0148]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.dense.weight',\n",
       "              tensor([[-0.0173, -0.0290,  0.0396,  ...,  0.0003, -0.0239,  0.0197],\n",
       "                      [ 0.0477, -0.0388,  0.0018,  ...,  0.0399, -0.0111,  0.0309],\n",
       "                      [ 0.0046,  0.0023, -0.0042,  ...,  0.0198, -0.0224, -0.0076],\n",
       "                      ...,\n",
       "                      [ 0.0264, -0.0487, -0.0293,  ..., -0.0199,  0.0020,  0.0298],\n",
       "                      [-0.0055, -0.0473,  0.0385,  ..., -0.0621,  0.0711, -0.1102],\n",
       "                      [-0.0193,  0.0330,  0.0582,  ...,  0.0223, -0.0685, -0.0234]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.self_attn.dense.bias',\n",
       "              tensor([ 0.0110,  0.0179, -0.0231,  ...,  0.0182,  0.0482,  0.0119],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.fc1.weight',\n",
       "              tensor([[-0.0325, -0.0302, -0.0387,  ...,  0.0107,  0.0027,  0.0156],\n",
       "                      [ 0.0270,  0.0021, -0.0041,  ...,  0.0299, -0.0107,  0.0199],\n",
       "                      [ 0.0311,  0.0381, -0.0733,  ...,  0.0040,  0.0085, -0.0003],\n",
       "                      ...,\n",
       "                      [ 0.0290,  0.0119,  0.0139,  ...,  0.0275, -0.0060, -0.0065],\n",
       "                      [-0.0199,  0.0002, -0.0098,  ..., -0.0022,  0.0120,  0.0077],\n",
       "                      [ 0.0259,  0.0126, -0.0384,  ...,  0.0106,  0.0201, -0.0019]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.fc1.bias',\n",
       "              tensor([-0.0380, -0.0257, -0.0081,  ...,  0.0014,  0.0054, -0.0007],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.fc2.weight',\n",
       "              tensor([[-0.0094,  0.0488, -0.0207,  ...,  0.0021,  0.0161, -0.0095],\n",
       "                      [-0.0114, -0.0103,  0.0098,  ..., -0.0106,  0.0053, -0.0014],\n",
       "                      [-0.0141,  0.0216,  0.0070,  ..., -0.0061,  0.0012,  0.0158],\n",
       "                      ...,\n",
       "                      [-0.0578, -0.0319, -0.0005,  ...,  0.0068, -0.0031, -0.0018],\n",
       "                      [-0.0537, -0.0096, -0.0403,  ..., -0.0157,  0.0183, -0.0084],\n",
       "                      [-0.0343, -0.0100,  0.0225,  ..., -0.0043, -0.0081,  0.0019]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.mlp.fc2.bias',\n",
       "              tensor([ 0.0024, -0.0076,  0.0539,  ...,  0.0246, -0.0445, -0.0205],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.input_layernorm.weight',\n",
       "              tensor([0.4475, 0.4507, 0.4153,  ..., 0.4653, 0.4482, 0.4500],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.11.input_layernorm.bias',\n",
       "              tensor([-0.0179, -0.0137,  0.0818,  ...,  0.0018, -0.0211, -0.0139],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0382, -0.0418,  0.0408,  ...,  0.0058,  0.0120, -0.0038],\n",
       "                      [-0.0455,  0.0263,  0.0024,  ...,  0.0063, -0.0002, -0.0414],\n",
       "                      [-0.0145,  0.0023,  0.0070,  ..., -0.0140,  0.0387,  0.0128],\n",
       "                      ...,\n",
       "                      [-0.0317, -0.0080, -0.0010,  ..., -0.0271,  0.0162,  0.0288],\n",
       "                      [ 0.0006,  0.0104,  0.0380,  ..., -0.0040,  0.0058, -0.0244],\n",
       "                      [ 0.0268,  0.0276,  0.0330,  ...,  0.0166, -0.0189,  0.0041]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.dense.weight',\n",
       "              tensor([[-0.0078, -0.0329, -0.0566,  ..., -0.0151,  0.0468,  0.0161],\n",
       "                      [-0.0093,  0.0062, -0.0090,  ...,  0.0523,  0.0050, -0.0416],\n",
       "                      [-0.0323, -0.0030, -0.0269,  ...,  0.0045, -0.0091, -0.0133],\n",
       "                      ...,\n",
       "                      [-0.0064,  0.0397,  0.0131,  ...,  0.0184, -0.0506, -0.0267],\n",
       "                      [ 0.0289,  0.0707,  0.0300,  ..., -0.0277,  0.0472, -0.0045],\n",
       "                      [ 0.0938, -0.0280,  0.0149,  ...,  0.0161,  0.0143,  0.0061]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.self_attn.dense.bias',\n",
       "              tensor([ 0.0232,  0.0072, -0.0164,  ...,  0.0232,  0.0066,  0.0240],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.fc1.weight',\n",
       "              tensor([[-0.0314, -0.0165,  0.0498,  ..., -0.0204,  0.0166, -0.0171],\n",
       "                      [ 0.0469,  0.0629, -0.0076,  ..., -0.0233, -0.0500,  0.0223],\n",
       "                      [ 0.0081, -0.0271, -0.0036,  ...,  0.0070,  0.0253, -0.0301],\n",
       "                      ...,\n",
       "                      [ 0.0266,  0.0246,  0.0124,  ...,  0.0017, -0.0143,  0.0226],\n",
       "                      [ 0.0140, -0.0224, -0.0194,  ...,  0.0044, -0.0194, -0.0059],\n",
       "                      [ 0.0370, -0.0110, -0.0158,  ...,  0.0149,  0.0402, -0.0430]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.fc1.bias',\n",
       "              tensor([-0.0209, -0.0515,  0.0085,  ...,  0.0056,  0.0047,  0.0067],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.fc2.weight',\n",
       "              tensor([[-0.0005,  0.0258,  0.0175,  ..., -0.0410, -0.0100, -0.0255],\n",
       "                      [ 0.0061,  0.0055,  0.0274,  ..., -0.0324,  0.0464, -0.0215],\n",
       "                      [-0.0604, -0.0087,  0.0159,  ...,  0.0088,  0.0141,  0.0300],\n",
       "                      ...,\n",
       "                      [ 0.0152, -0.0039, -0.0022,  ..., -0.0137,  0.0106, -0.0026],\n",
       "                      [ 0.0302,  0.0096,  0.0399,  ...,  0.0057,  0.0062, -0.0338],\n",
       "                      [ 0.0227, -0.0110, -0.0115,  ..., -0.0138, -0.0006,  0.0039]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.mlp.fc2.bias',\n",
       "              tensor([ 0.0023, -0.0037,  0.0640,  ...,  0.0093, -0.0182, -0.0168],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.input_layernorm.weight',\n",
       "              tensor([0.4500, 0.4529, 0.4165,  ..., 0.4746, 0.4551, 0.4619],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.12.input_layernorm.bias',\n",
       "              tensor([-0.0201, -0.0125,  0.0879,  ...,  0.0022, -0.0192, -0.0152],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.query_key_value.weight',\n",
       "              tensor([[-0.0071, -0.0006,  0.0050,  ..., -0.0062, -0.0027,  0.0082],\n",
       "                      [ 0.0009, -0.0021, -0.0236,  ..., -0.0260,  0.0313, -0.0431],\n",
       "                      [-0.0252, -0.0033,  0.0318,  ..., -0.0073, -0.0076, -0.0028],\n",
       "                      ...,\n",
       "                      [ 0.0346, -0.0046, -0.0117,  ...,  0.0174, -0.0088, -0.0324],\n",
       "                      [-0.0003, -0.0287, -0.0008,  ..., -0.0270,  0.0169,  0.0178],\n",
       "                      [ 0.0227, -0.0052, -0.0131,  ...,  0.0283, -0.0185,  0.0036]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.dense.weight',\n",
       "              tensor([[-0.0104, -0.0206, -0.0515,  ...,  0.0265,  0.0259, -0.0499],\n",
       "                      [-0.0142,  0.0309,  0.0349,  ..., -0.0362,  0.0137, -0.0101],\n",
       "                      [ 0.0289,  0.0136, -0.0009,  ...,  0.0381, -0.0148,  0.0059],\n",
       "                      ...,\n",
       "                      [ 0.0256,  0.0333, -0.0229,  ..., -0.0411, -0.0236,  0.0558],\n",
       "                      [ 0.0040, -0.0165,  0.0172,  ...,  0.0046,  0.0361, -0.0142],\n",
       "                      [-0.0115, -0.0053,  0.0424,  ...,  0.0292, -0.0133,  0.0344]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.self_attn.dense.bias',\n",
       "              tensor([-0.0108, -0.0124, -0.0181,  ...,  0.0481, -0.0050, -0.0231],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.fc1.weight',\n",
       "              tensor([[-0.0484, -0.0010,  0.0290,  ..., -0.0010,  0.0269, -0.0480],\n",
       "                      [-0.0750,  0.0033, -0.0140,  ..., -0.0028, -0.0396,  0.0069],\n",
       "                      [ 0.0362,  0.0003, -0.0905,  ...,  0.0402,  0.0081, -0.0066],\n",
       "                      ...,\n",
       "                      [-0.0007,  0.0034,  0.0257,  ..., -0.0062, -0.0379, -0.0027],\n",
       "                      [ 0.0116, -0.0224, -0.0042,  ...,  0.0052,  0.0057, -0.0016],\n",
       "                      [ 0.0058,  0.0062, -0.0288,  ...,  0.0202,  0.0316,  0.0138]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.fc1.bias',\n",
       "              tensor([-0.0119, -0.0477,  0.0108,  ..., -0.0139,  0.0195,  0.0170],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.fc2.weight',\n",
       "              tensor([[-0.0135, -0.0338, -0.0189,  ...,  0.0179, -0.0068, -0.0291],\n",
       "                      [-0.0182,  0.0293, -0.0034,  ..., -0.0287,  0.0169,  0.0145],\n",
       "                      [-0.0126,  0.0322,  0.0341,  ..., -0.0207, -0.0040, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0494, -0.0335, -0.0178,  ..., -0.0094,  0.0290, -0.0158],\n",
       "                      [-0.0276, -0.0292, -0.0189,  ...,  0.0267, -0.0088, -0.0155],\n",
       "                      [ 0.0436,  0.0120,  0.0214,  ...,  0.0165, -0.0121, -0.0163]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.mlp.fc2.bias',\n",
       "              tensor([ 0.0093, -0.0205,  0.0549,  ...,  0.0239, -0.0451, -0.0077],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.input_layernorm.weight',\n",
       "              tensor([0.4570, 0.4531, 0.4268,  ..., 0.4746, 0.4619, 0.4668],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.13.input_layernorm.bias',\n",
       "              tensor([-0.0199, -0.0120,  0.0975,  ...,  0.0012, -0.0177, -0.0125],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0031,  0.0063, -0.0337,  ...,  0.0140, -0.0089,  0.0398],\n",
       "                      [ 0.0035, -0.0504,  0.0178,  ..., -0.0060, -0.0040,  0.0163],\n",
       "                      [-0.0151, -0.0063, -0.0316,  ..., -0.0137, -0.0038,  0.0009],\n",
       "                      ...,\n",
       "                      [ 0.0458, -0.0083,  0.0273,  ..., -0.0262, -0.0019, -0.0309],\n",
       "                      [-0.0059,  0.0271,  0.0278,  ..., -0.0034, -0.0160,  0.0028],\n",
       "                      [ 0.0215,  0.0282, -0.0311,  ..., -0.0117, -0.0103, -0.0176]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.dense.weight',\n",
       "              tensor([[ 0.0322,  0.0933, -0.0191,  ...,  0.0121,  0.0271, -0.0398],\n",
       "                      [-0.0146,  0.0263,  0.0542,  ...,  0.0239,  0.0031,  0.0037],\n",
       "                      [ 0.0220, -0.0155, -0.0368,  ..., -0.0246,  0.0030, -0.0439],\n",
       "                      ...,\n",
       "                      [-0.0079,  0.0125,  0.0018,  ...,  0.0335, -0.0151, -0.0122],\n",
       "                      [-0.0217, -0.0314, -0.0433,  ...,  0.0258,  0.0189,  0.0118],\n",
       "                      [ 0.0191, -0.0026,  0.0267,  ...,  0.0220, -0.0032,  0.0468]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.self_attn.dense.bias',\n",
       "              tensor([ 0.0182, -0.0003, -0.0136,  ...,  0.0406,  0.0339,  0.0035],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.fc1.weight',\n",
       "              tensor([[ 0.0557,  0.0505,  0.0184,  ...,  0.0224,  0.0042,  0.0049],\n",
       "                      [ 0.0250,  0.0660, -0.0278,  ...,  0.0165, -0.0130,  0.0807],\n",
       "                      [ 0.0453,  0.0067, -0.0616,  ..., -0.0032, -0.0169,  0.0453],\n",
       "                      ...,\n",
       "                      [-0.0192, -0.0080,  0.0085,  ...,  0.0003,  0.0534, -0.0130],\n",
       "                      [-0.0038, -0.0363,  0.0163,  ..., -0.0350, -0.0268, -0.0302],\n",
       "                      [ 0.0518,  0.0425,  0.0263,  ...,  0.0041, -0.0352,  0.0222]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.fc1.bias',\n",
       "              tensor([-0.0049, -0.0486,  0.0123,  ..., -0.0020,  0.0252, -0.0051],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.fc2.weight',\n",
       "              tensor([[-0.0072, -0.0027, -0.0362,  ...,  0.0257,  0.0053, -0.0349],\n",
       "                      [-0.0121,  0.0081,  0.0209,  ...,  0.0162, -0.0003, -0.0323],\n",
       "                      [-0.0213, -0.0179,  0.0208,  ..., -0.0304, -0.0058, -0.0292],\n",
       "                      ...,\n",
       "                      [-0.0104,  0.0013, -0.0195,  ...,  0.0200,  0.0483,  0.0302],\n",
       "                      [ 0.0097, -0.0095,  0.0105,  ..., -0.0462,  0.0250,  0.0261],\n",
       "                      [ 0.0174,  0.1018, -0.0544,  ..., -0.0262,  0.0144, -0.0341]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.mlp.fc2.bias',\n",
       "              tensor([-0.0054, -0.0096,  0.0554,  ...,  0.0271, -0.0453,  0.0170],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.input_layernorm.weight',\n",
       "              tensor([0.4475, 0.4480, 0.4207,  ..., 0.4712, 0.4575, 0.4653],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.14.input_layernorm.bias',\n",
       "              tensor([-0.0173, -0.0159,  0.1003,  ..., -0.0006, -0.0224, -0.0118],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0060, -0.0014, -0.0108,  ..., -0.0011, -0.0148,  0.0012],\n",
       "                      [ 0.0035, -0.0214, -0.0349,  ...,  0.0250, -0.0156,  0.0102],\n",
       "                      [ 0.0268, -0.0108,  0.0024,  ..., -0.0087, -0.0007,  0.0125],\n",
       "                      ...,\n",
       "                      [ 0.0118,  0.0222,  0.0023,  ...,  0.0006, -0.0104, -0.0125],\n",
       "                      [-0.0209,  0.0102, -0.0262,  ..., -0.0221, -0.0020,  0.0058],\n",
       "                      [ 0.0134,  0.0090, -0.0013,  ...,  0.0051, -0.0088, -0.0358]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.dense.weight',\n",
       "              tensor([[-0.0253, -0.0067, -0.0066,  ...,  0.0339, -0.0096, -0.0141],\n",
       "                      [-0.0305,  0.0123,  0.0119,  ...,  0.0205,  0.0298,  0.0875],\n",
       "                      [ 0.0148,  0.0021,  0.0100,  ...,  0.0687,  0.0112,  0.0628],\n",
       "                      ...,\n",
       "                      [ 0.0332,  0.0417, -0.0311,  ...,  0.0189,  0.0076,  0.0139],\n",
       "                      [-0.0086,  0.0331, -0.0229,  ..., -0.0009, -0.0045, -0.0096],\n",
       "                      [-0.0236,  0.0028, -0.0071,  ..., -0.0539,  0.0264,  0.0489]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.self_attn.dense.bias',\n",
       "              tensor([ 0.0146,  0.0167, -0.0452,  ...,  0.0113,  0.0273, -0.0203],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.fc1.weight',\n",
       "              tensor([[-0.0093,  0.0436, -0.0588,  ..., -0.0078,  0.0108, -0.0263],\n",
       "                      [-0.0343,  0.0101, -0.0005,  ..., -0.0077, -0.0320,  0.0108],\n",
       "                      [ 0.0164,  0.0142, -0.0318,  ..., -0.0123,  0.0140,  0.0071],\n",
       "                      ...,\n",
       "                      [-0.0274,  0.0422, -0.0325,  ...,  0.0334, -0.0172, -0.0042],\n",
       "                      [-0.0164,  0.0337, -0.0119,  ..., -0.0188,  0.0302,  0.0257],\n",
       "                      [ 0.0354,  0.0003, -0.0097,  ..., -0.0031,  0.0279,  0.0421]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.fc1.bias',\n",
       "              tensor([-0.0312, -0.0079, -0.0145,  ..., -0.0066, -0.0079,  0.0009],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.fc2.weight',\n",
       "              tensor([[ 0.0217,  0.0299, -0.0157,  ...,  0.0340, -0.0217, -0.0186],\n",
       "                      [-0.0322,  0.0312, -0.0170,  ..., -0.0013, -0.0046, -0.0290],\n",
       "                      [-0.0329,  0.0094, -0.0285,  ..., -0.0256, -0.0030, -0.0032],\n",
       "                      ...,\n",
       "                      [-0.0356,  0.0516,  0.0494,  ..., -0.0247, -0.0143, -0.0120],\n",
       "                      [ 0.0283,  0.0188,  0.0222,  ..., -0.0079, -0.0149, -0.0390],\n",
       "                      [ 0.0162, -0.0230, -0.0360,  ...,  0.0222,  0.0245, -0.0042]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.mlp.fc2.bias',\n",
       "              tensor([ 0.0106, -0.0278,  0.0740,  ...,  0.0080, -0.0142,  0.0239],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.input_layernorm.weight',\n",
       "              tensor([0.4451, 0.4429, 0.4114,  ..., 0.4609, 0.4492, 0.4546],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.15.input_layernorm.bias',\n",
       "              tensor([-0.0188, -0.0124,  0.0906,  ...,  0.0012, -0.0197, -0.0080],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.query_key_value.weight',\n",
       "              tensor([[-0.0143,  0.0045, -0.0009,  ...,  0.0150,  0.0307, -0.0059],\n",
       "                      [ 0.0153, -0.0171,  0.0180,  ...,  0.0181,  0.0181, -0.0022],\n",
       "                      [-0.0020,  0.0085,  0.0273,  ...,  0.0388,  0.0293, -0.0068],\n",
       "                      ...,\n",
       "                      [-0.0109,  0.0291,  0.0352,  ..., -0.0060, -0.0083, -0.0169],\n",
       "                      [-0.0033, -0.0151, -0.0222,  ...,  0.0304, -0.0083,  0.0188],\n",
       "                      [-0.0493,  0.0023, -0.0019,  ..., -0.0408, -0.0013, -0.0198]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.dense.weight',\n",
       "              tensor([[-0.0385,  0.0423, -0.0086,  ..., -0.0189,  0.0366, -0.0062],\n",
       "                      [ 0.0063, -0.0025,  0.0102,  ..., -0.0387, -0.0057,  0.0199],\n",
       "                      [ 0.0093,  0.0356, -0.0062,  ...,  0.0482,  0.0020, -0.0163],\n",
       "                      ...,\n",
       "                      [ 0.0320,  0.0489,  0.0381,  ..., -0.0125, -0.0159,  0.0036],\n",
       "                      [ 0.0102, -0.0022, -0.0153,  ..., -0.0389,  0.0414, -0.0276],\n",
       "                      [ 0.0124, -0.0275,  0.0441,  ...,  0.0253,  0.0243,  0.0029]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.self_attn.dense.bias',\n",
       "              tensor([-0.0173,  0.0281, -0.0245,  ...,  0.0456,  0.0075, -0.0281],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.fc1.weight',\n",
       "              tensor([[ 0.0136, -0.0075, -0.0199,  ..., -0.0247,  0.0187, -0.0162],\n",
       "                      [-0.0864,  0.0246,  0.0023,  ...,  0.0269, -0.0515, -0.0170],\n",
       "                      [ 0.0642,  0.0035, -0.0235,  ..., -0.0051, -0.0569,  0.0200],\n",
       "                      ...,\n",
       "                      [-0.0446, -0.0311, -0.0158,  ...,  0.0238, -0.0208,  0.0107],\n",
       "                      [-0.0072, -0.0047, -0.0327,  ...,  0.0411,  0.0293, -0.0018],\n",
       "                      [ 0.0195, -0.0235, -0.0016,  ..., -0.0226, -0.0024,  0.0139]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.fc1.bias',\n",
       "              tensor([-0.0406,  0.0183, -0.0406,  ..., -0.0016,  0.0145, -0.0037],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.fc2.weight',\n",
       "              tensor([[ 0.0017,  0.0501,  0.0100,  ...,  0.0396,  0.0460, -0.0161],\n",
       "                      [-0.0225, -0.0368,  0.0140,  ...,  0.0384, -0.0196, -0.0460],\n",
       "                      [ 0.0437, -0.0104, -0.0067,  ..., -0.0002,  0.0201,  0.0180],\n",
       "                      ...,\n",
       "                      [-0.0368, -0.0280,  0.0029,  ..., -0.0416, -0.0255,  0.0399],\n",
       "                      [ 0.0064,  0.0549, -0.0043,  ...,  0.0104, -0.0166,  0.0056],\n",
       "                      [ 0.0202,  0.0111,  0.0343,  ..., -0.0432,  0.0410,  0.0220]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.mlp.fc2.bias',\n",
       "              tensor([ 0.0109, -0.0118,  0.0604,  ..., -0.0211, -0.0114,  0.0089],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.input_layernorm.weight',\n",
       "              tensor([0.4514, 0.4502, 0.4150,  ..., 0.4685, 0.4590, 0.4685],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.16.input_layernorm.bias',\n",
       "              tensor([-0.0210, -0.0183,  0.1039,  ..., -0.0012, -0.0176, -0.0060],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.query_key_value.weight',\n",
       "              tensor([[-0.0185,  0.0037,  0.0022,  ..., -0.0071,  0.0035,  0.0354],\n",
       "                      [-0.0250, -0.0471,  0.0159,  ..., -0.0319,  0.0086, -0.0008],\n",
       "                      [ 0.0115,  0.0030, -0.0189,  ...,  0.0112,  0.0069,  0.0153],\n",
       "                      ...,\n",
       "                      [ 0.0070, -0.0004,  0.0076,  ...,  0.0261,  0.0098,  0.0233],\n",
       "                      [ 0.0261,  0.0121, -0.0266,  ..., -0.0178,  0.0335,  0.0240],\n",
       "                      [-0.0343,  0.0223, -0.0129,  ...,  0.0094,  0.0003,  0.0393]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.dense.weight',\n",
       "              tensor([[-0.0156,  0.0284,  0.0212,  ...,  0.0475, -0.0919,  0.0158],\n",
       "                      [ 0.0108, -0.0090, -0.0032,  ..., -0.0261,  0.0302, -0.0057],\n",
       "                      [ 0.0113, -0.0478, -0.1019,  ..., -0.0228, -0.0599, -0.0078],\n",
       "                      ...,\n",
       "                      [-0.0243, -0.0156, -0.0044,  ...,  0.0290,  0.0365,  0.0470],\n",
       "                      [ 0.0189,  0.0021,  0.0337,  ..., -0.0981, -0.0647, -0.0338],\n",
       "                      [ 0.0317,  0.0117, -0.0096,  ..., -0.0357,  0.0438, -0.0009]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.self_attn.dense.bias',\n",
       "              tensor([ 0.0070,  0.0275, -0.0242,  ...,  0.0582, -0.0170,  0.0365],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.fc1.weight',\n",
       "              tensor([[ 0.0467,  0.0025,  0.0070,  ...,  0.0085, -0.0324,  0.0058],\n",
       "                      [-0.0058, -0.0057, -0.0192,  ...,  0.0102, -0.0063,  0.0054],\n",
       "                      [-0.0294, -0.0387, -0.0258,  ..., -0.0051,  0.0153, -0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0215,  0.0107, -0.0146,  ..., -0.0015, -0.0441, -0.0370],\n",
       "                      [-0.0290, -0.0343,  0.0117,  ..., -0.0015, -0.0203, -0.0087],\n",
       "                      [ 0.0218, -0.0128,  0.0175,  ...,  0.0356,  0.0131, -0.0175]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.fc1.bias',\n",
       "              tensor([-0.0379, -0.0257, -0.0060,  ..., -0.0401, -0.0081, -0.0018],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.fc2.weight',\n",
       "              tensor([[ 3.0243e-02, -1.2833e-02,  6.6161e-05,  ...,  3.0106e-02,\n",
       "                        2.1343e-03,  7.0877e-03],\n",
       "                      [-2.4452e-03, -3.5889e-02,  1.2016e-02,  ...,  2.6459e-02,\n",
       "                       -4.6448e-02,  2.1191e-03],\n",
       "                      [-2.8259e-02,  2.2446e-02,  8.7967e-03,  ...,  2.5558e-02,\n",
       "                       -1.6586e-02, -1.4252e-02],\n",
       "                      ...,\n",
       "                      [-3.4485e-02,  4.1016e-02,  4.0894e-03,  ...,  7.2021e-03,\n",
       "                        9.3765e-03, -1.4389e-02],\n",
       "                      [-4.0588e-02, -7.3891e-03,  1.1787e-02,  ...,  3.4698e-02,\n",
       "                       -1.4557e-02, -8.4991e-03],\n",
       "                      [ 1.0605e-02, -4.4403e-03,  5.1392e-02,  ...,  1.8188e-02,\n",
       "                        6.9389e-03,  3.2013e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.17.mlp.fc2.bias',\n",
       "              tensor([-0.0043, -0.0052,  0.0795,  ..., -0.0308, -0.0108,  0.0011],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.input_layernorm.weight',\n",
       "              tensor([0.4309, 0.4399, 0.4221,  ..., 0.4539, 0.4475, 0.4526],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.17.input_layernorm.bias',\n",
       "              tensor([-0.0176, -0.0139,  0.1109,  ..., -0.0072, -0.0156, -0.0072],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.query_key_value.weight',\n",
       "              tensor([[-1.0742e-02, -3.3600e-02,  1.4381e-02,  ...,  1.7691e-03,\n",
       "                        3.2867e-02, -1.3916e-02],\n",
       "                      [ 7.9269e-03,  2.1271e-02, -1.6632e-02,  ..., -1.9608e-02,\n",
       "                       -1.9348e-02, -4.0985e-02],\n",
       "                      [ 2.7908e-02,  2.6184e-02, -6.5193e-03,  ...,  2.2873e-02,\n",
       "                       -8.9722e-03, -2.5749e-03],\n",
       "                      ...,\n",
       "                      [ 1.5465e-02,  1.2512e-02, -8.8024e-04,  ...,  8.1863e-03,\n",
       "                        4.9667e-03, -2.4629e-04],\n",
       "                      [-3.7323e-02, -6.9923e-03, -3.8910e-02,  ..., -5.0545e-04,\n",
       "                       -4.3396e-02,  3.1311e-02],\n",
       "                      [ 4.7264e-03,  2.0180e-03,  4.8523e-03,  ...,  8.8751e-05,\n",
       "                       -2.6855e-02, -3.5820e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.dense.weight',\n",
       "              tensor([[ 0.0063,  0.0754,  0.0379,  ..., -0.0104, -0.0301,  0.0602],\n",
       "                      [-0.0484, -0.0479, -0.0275,  ..., -0.0573,  0.0310,  0.0602],\n",
       "                      [ 0.0209,  0.0449,  0.0145,  ..., -0.0344, -0.0262,  0.0185],\n",
       "                      ...,\n",
       "                      [ 0.0088,  0.0576, -0.0168,  ..., -0.0181,  0.0300,  0.0095],\n",
       "                      [ 0.0393,  0.0224, -0.0266,  ...,  0.0275, -0.0238, -0.0037],\n",
       "                      [-0.0227, -0.0177,  0.0337,  ..., -0.0345,  0.0173,  0.0183]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.self_attn.dense.bias',\n",
       "              tensor([ 0.0152,  0.0083, -0.0514,  ...,  0.0526, -0.0085,  0.0039],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.fc1.weight',\n",
       "              tensor([[ 0.0892,  0.0576, -0.0513,  ..., -0.0400, -0.0086, -0.0343],\n",
       "                      [ 0.0568,  0.0536,  0.0203,  ..., -0.0132,  0.0189,  0.0226],\n",
       "                      [ 0.0389,  0.0471,  0.0373,  ..., -0.0334,  0.0077,  0.0408],\n",
       "                      ...,\n",
       "                      [-0.0403,  0.0002, -0.0135,  ...,  0.0108, -0.0135, -0.0097],\n",
       "                      [-0.0081, -0.0413,  0.0019,  ..., -0.0057, -0.0040, -0.0077],\n",
       "                      [ 0.0108, -0.0173, -0.0054,  ...,  0.0011, -0.0054,  0.0434]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.fc1.bias',\n",
       "              tensor([-0.0325, -0.0349, -0.0073,  ..., -0.0068,  0.0066, -0.0116],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.fc2.weight',\n",
       "              tensor([[ 0.0887,  0.0148, -0.0080,  ...,  0.0230,  0.0173, -0.0292],\n",
       "                      [ 0.0552, -0.0216, -0.0366,  ...,  0.0029,  0.0031, -0.0024],\n",
       "                      [-0.0040,  0.0453, -0.0272,  ...,  0.0145,  0.0050, -0.0302],\n",
       "                      ...,\n",
       "                      [-0.0065,  0.0478,  0.0329,  ..., -0.0129,  0.0119,  0.0032],\n",
       "                      [ 0.0136, -0.0055,  0.0040,  ...,  0.0082,  0.0244,  0.0005],\n",
       "                      [-0.0278,  0.0162, -0.0238,  ...,  0.0066,  0.0018, -0.0094]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.mlp.fc2.bias',\n",
       "              tensor([-0.0111, -0.0141,  0.0723,  ..., -0.0114, -0.0070, -0.0062],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.input_layernorm.weight',\n",
       "              tensor([0.4453, 0.4487, 0.4443,  ..., 0.4570, 0.4463, 0.4629],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.18.input_layernorm.bias',\n",
       "              tensor([-0.0303, -0.0246,  0.1056,  ..., -0.0147, -0.0162, -0.0124],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0111,  0.0184, -0.0472,  ...,  0.0171,  0.0066,  0.0200],\n",
       "                      [-0.0132, -0.0149, -0.0047,  ..., -0.0060, -0.0022,  0.0179],\n",
       "                      [ 0.0533, -0.0041, -0.0014,  ..., -0.0088, -0.0104, -0.0305],\n",
       "                      ...,\n",
       "                      [-0.0404,  0.0379, -0.0046,  ..., -0.0246, -0.0181,  0.0191],\n",
       "                      [ 0.0167, -0.0154,  0.0053,  ...,  0.0188,  0.0297,  0.0242],\n",
       "                      [-0.0087, -0.0116,  0.0216,  ..., -0.0058, -0.0106, -0.0162]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.dense.weight',\n",
       "              tensor([[ 0.0355,  0.0627,  0.0235,  ..., -0.0348,  0.0146, -0.0095],\n",
       "                      [ 0.0104, -0.0076, -0.0065,  ..., -0.0023,  0.0269,  0.0057],\n",
       "                      [-0.0376, -0.0144, -0.0258,  ...,  0.0665, -0.0254,  0.0542],\n",
       "                      ...,\n",
       "                      [-0.0507, -0.0071, -0.0070,  ...,  0.0686,  0.0017, -0.0081],\n",
       "                      [ 0.0086, -0.0324,  0.0343,  ...,  0.0080,  0.0273,  0.0431],\n",
       "                      [ 0.0482, -0.0612,  0.0282,  ...,  0.0287,  0.0088,  0.0291]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.self_attn.dense.bias',\n",
       "              tensor([ 0.0092, -0.0057, -0.0505,  ...,  0.0225, -0.0073,  0.0055],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.fc1.weight',\n",
       "              tensor([[-0.0763, -0.0073, -0.0122,  ...,  0.0479,  0.0175,  0.0505],\n",
       "                      [-0.0443,  0.0018, -0.0090,  ..., -0.0302, -0.0087, -0.0017],\n",
       "                      [-0.0221,  0.0696,  0.0065,  ...,  0.0038,  0.0302,  0.0129],\n",
       "                      ...,\n",
       "                      [-0.0107, -0.0085, -0.0029,  ...,  0.0299,  0.0008, -0.0171],\n",
       "                      [ 0.0071,  0.0179, -0.0134,  ...,  0.0042, -0.0148, -0.0155],\n",
       "                      [-0.0149, -0.0030, -0.0594,  ..., -0.0181, -0.0322,  0.0199]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.fc1.bias',\n",
       "              tensor([-0.0364, -0.0307, -0.0108,  ...,  0.0126,  0.0052, -0.0009],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.fc2.weight',\n",
       "              tensor([[-0.0703,  0.0019,  0.0161,  ..., -0.0195, -0.0020, -0.0083],\n",
       "                      [ 0.0239, -0.0087, -0.0656,  ...,  0.0018, -0.0270, -0.0179],\n",
       "                      [ 0.0195, -0.0080, -0.0111,  ..., -0.0065,  0.0034,  0.0327],\n",
       "                      ...,\n",
       "                      [ 0.0818,  0.0135,  0.0117,  ...,  0.0172, -0.0071, -0.0175],\n",
       "                      [ 0.0861,  0.0224, -0.0556,  ..., -0.0031,  0.0286,  0.0172],\n",
       "                      [ 0.0295,  0.0298,  0.0212,  ...,  0.0374,  0.0455, -0.0198]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.mlp.fc2.bias',\n",
       "              tensor([-0.0023, -0.0091,  0.0575,  ...,  0.0049, -0.0065, -0.0035],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.input_layernorm.weight',\n",
       "              tensor([0.4551, 0.4578, 0.4358,  ..., 0.4641, 0.4622, 0.4746],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.19.input_layernorm.bias',\n",
       "              tensor([-0.0215, -0.0224,  0.1087,  ..., -0.0102, -0.0132, -0.0117],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.query_key_value.weight',\n",
       "              tensor([[ 2.5162e-02,  3.7498e-03,  2.9583e-03,  ...,  4.3182e-03,\n",
       "                       -1.4359e-02,  5.8594e-03],\n",
       "                      [ 2.4155e-02, -1.6317e-03, -1.6907e-02,  ...,  1.0162e-02,\n",
       "                       -7.9117e-03, -1.6403e-02],\n",
       "                      [ 4.5891e-03,  1.1536e-02, -1.4748e-02,  ...,  1.8967e-02,\n",
       "                        1.0391e-02, -7.5579e-05],\n",
       "                      ...,\n",
       "                      [-1.3664e-02, -1.0597e-02, -4.9067e-04,  ..., -1.7792e-02,\n",
       "                       -4.3823e-02,  7.6790e-03],\n",
       "                      [-2.0635e-04, -3.9276e-02,  1.7258e-02,  ...,  2.9770e-02,\n",
       "                        8.9874e-03,  6.4240e-03],\n",
       "                      [-8.9035e-03, -1.1993e-02,  1.1490e-02,  ..., -2.8229e-02,\n",
       "                        1.3458e-02, -6.1836e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.dense.weight',\n",
       "              tensor([[-0.0107,  0.0068,  0.0249,  ..., -0.0411,  0.0547,  0.0045],\n",
       "                      [ 0.0296,  0.0632,  0.0022,  ..., -0.0244, -0.0155, -0.0514],\n",
       "                      [-0.0206,  0.0045, -0.0192,  ..., -0.0034, -0.0335,  0.0091],\n",
       "                      ...,\n",
       "                      [ 0.0246,  0.0070,  0.0521,  ..., -0.0173,  0.0039,  0.0469],\n",
       "                      [ 0.0135,  0.0088,  0.0103,  ..., -0.0430,  0.0367,  0.0262],\n",
       "                      [ 0.0187,  0.0198,  0.0069,  ...,  0.0475, -0.0298,  0.0043]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.self_attn.dense.bias',\n",
       "              tensor([ 0.0254,  0.0094, -0.0605,  ..., -0.0178, -0.0482,  0.0677],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.fc1.weight',\n",
       "              tensor([[ 0.0385,  0.0222,  0.0151,  ...,  0.0227,  0.0282,  0.0084],\n",
       "                      [-0.0186, -0.0013,  0.0062,  ...,  0.0209,  0.0286,  0.0145],\n",
       "                      [-0.0271, -0.0149,  0.0075,  ...,  0.0266, -0.0012, -0.0007],\n",
       "                      ...,\n",
       "                      [ 0.0008, -0.0518, -0.0195,  ...,  0.0076, -0.0598,  0.0081],\n",
       "                      [ 0.0182,  0.0103,  0.0226,  ..., -0.0293, -0.0283, -0.0026],\n",
       "                      [-0.0181, -0.0130,  0.0147,  ...,  0.0035, -0.0022, -0.0042]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.fc1.bias',\n",
       "              tensor([-0.0384, -0.0315, -0.0075,  ..., -0.0044, -0.0151, -0.0134],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.fc2.weight',\n",
       "              tensor([[ 0.0425, -0.0257,  0.0175,  ...,  0.0199, -0.0057,  0.0393],\n",
       "                      [ 0.0300,  0.0027, -0.0459,  ...,  0.0117, -0.0078, -0.0195],\n",
       "                      [-0.0038, -0.0098, -0.0578,  ...,  0.0343, -0.0218,  0.0008],\n",
       "                      ...,\n",
       "                      [ 0.0161, -0.0094,  0.0044,  ..., -0.0008,  0.0056, -0.0163],\n",
       "                      [-0.0004, -0.0256,  0.0089,  ...,  0.0182,  0.0568, -0.0133],\n",
       "                      [-0.0659,  0.0070, -0.0284,  ..., -0.0356,  0.0028,  0.0100]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.mlp.fc2.bias',\n",
       "              tensor([-0.0078, -0.0244,  0.0550,  ..., -0.0007, -0.0086, -0.0117],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.input_layernorm.weight',\n",
       "              tensor([0.4578, 0.4634, 0.4514,  ..., 0.4524, 0.4519, 0.4722],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.20.input_layernorm.bias',\n",
       "              tensor([-0.0246, -0.0273,  0.1160,  ..., -0.0076, -0.0181, -0.0170],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.query_key_value.weight',\n",
       "              tensor([[-0.0090,  0.0286,  0.0224,  ..., -0.0312, -0.0231, -0.0129],\n",
       "                      [ 0.0284, -0.0154,  0.0043,  ..., -0.0064,  0.0215, -0.0121],\n",
       "                      [-0.0097,  0.0152,  0.0041,  ..., -0.0120, -0.0047,  0.0058],\n",
       "                      ...,\n",
       "                      [-0.0075, -0.0178, -0.0161,  ...,  0.0226, -0.0050,  0.0091],\n",
       "                      [-0.0438,  0.0045,  0.0094,  ..., -0.0216,  0.0383,  0.0080],\n",
       "                      [-0.0013,  0.0070,  0.0008,  ...,  0.0213, -0.0136,  0.0177]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.dense.weight',\n",
       "              tensor([[-0.0259, -0.0500, -0.0081,  ...,  0.0410,  0.0038, -0.0108],\n",
       "                      [-0.0061,  0.0038, -0.0134,  ..., -0.0330, -0.0271,  0.0372],\n",
       "                      [ 0.0002,  0.0010, -0.0355,  ...,  0.0540,  0.0724,  0.0507],\n",
       "                      ...,\n",
       "                      [-0.0227, -0.0294, -0.0578,  ...,  0.0059,  0.0034,  0.0037],\n",
       "                      [-0.0180,  0.0083, -0.0086,  ..., -0.0545, -0.0278,  0.0155],\n",
       "                      [ 0.0185, -0.0318,  0.0003,  ..., -0.0189, -0.0271,  0.0172]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.self_attn.dense.bias',\n",
       "              tensor([-0.0272,  0.0106, -0.0667,  ...,  0.0182, -0.0192,  0.0601],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.fc1.weight',\n",
       "              tensor([[ 2.7069e-02, -2.4490e-02, -4.9835e-02,  ...,  2.4689e-02,\n",
       "                       -8.0933e-02, -2.5894e-02],\n",
       "                      [ 5.1483e-02,  5.0323e-02, -5.4291e-02,  ..., -3.2135e-02,\n",
       "                       -8.6746e-03, -4.4159e-02],\n",
       "                      [ 6.5804e-03, -1.9470e-02, -2.2781e-02,  ...,  6.8604e-02,\n",
       "                        6.0333e-02,  6.3171e-02],\n",
       "                      ...,\n",
       "                      [-2.5024e-03,  4.9171e-03, -1.6470e-03,  ..., -2.2858e-02,\n",
       "                        1.0193e-02,  5.9395e-03],\n",
       "                      [-3.0609e-02,  2.1622e-02,  7.7367e-05,  ..., -2.2141e-02,\n",
       "                       -7.9651e-02,  1.3176e-02],\n",
       "                      [ 2.3232e-03,  1.5930e-02,  1.9455e-02,  ...,  9.1858e-03,\n",
       "                       -7.3891e-03,  9.4833e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.fc1.bias',\n",
       "              tensor([-0.0621, -0.0251, -0.0283,  ..., -0.0166, -0.0580, -0.0498],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.fc2.weight',\n",
       "              tensor([[-0.0032, -0.0121, -0.0300,  ...,  0.0144,  0.0204,  0.0107],\n",
       "                      [-0.0342,  0.0479, -0.0497,  ...,  0.0512, -0.0180, -0.0017],\n",
       "                      [ 0.0207, -0.0103,  0.0654,  ..., -0.0165,  0.0021,  0.0128],\n",
       "                      ...,\n",
       "                      [ 0.0075, -0.0132, -0.0385,  ...,  0.0403,  0.0436, -0.0099],\n",
       "                      [-0.0654, -0.0023, -0.0420,  ...,  0.0427,  0.0579,  0.0199],\n",
       "                      [-0.0082, -0.0090,  0.0717,  ..., -0.0291, -0.0369,  0.0266]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.mlp.fc2.bias',\n",
       "              tensor([ 0.0194, -0.0203,  0.0432,  ...,  0.0070, -0.0119, -0.0181],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.input_layernorm.weight',\n",
       "              tensor([0.4722, 0.4807, 0.4565,  ..., 0.4763, 0.4885, 0.5015],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.21.input_layernorm.bias',\n",
       "              tensor([-0.0236, -0.0270,  0.1196,  ..., -0.0084, -0.0152, -0.0146],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0060,  0.0277,  0.0007,  ...,  0.0030, -0.0132,  0.0349],\n",
       "                      [ 0.0180,  0.0286,  0.0105,  ..., -0.0162,  0.0126, -0.0009],\n",
       "                      [ 0.0071, -0.0039,  0.0215,  ...,  0.0433,  0.0152, -0.0065],\n",
       "                      ...,\n",
       "                      [ 0.0005, -0.0163,  0.0457,  ..., -0.0253, -0.0237, -0.0366],\n",
       "                      [-0.0008, -0.0182,  0.0207,  ...,  0.0321, -0.0139,  0.0165],\n",
       "                      [ 0.0113, -0.0013,  0.0135,  ...,  0.0309, -0.0208, -0.0023]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.dense.weight',\n",
       "              tensor([[-0.0131, -0.0113,  0.0031,  ..., -0.0069, -0.0102,  0.0178],\n",
       "                      [ 0.0295, -0.0180, -0.0280,  ...,  0.0174, -0.0015,  0.0132],\n",
       "                      [-0.0048,  0.0508,  0.0076,  ..., -0.0157,  0.0296, -0.0239],\n",
       "                      ...,\n",
       "                      [-0.0051, -0.0453,  0.0225,  ...,  0.0187, -0.0516, -0.0057],\n",
       "                      [ 0.0186, -0.0141,  0.0286,  ..., -0.0155,  0.0071,  0.0344],\n",
       "                      [ 0.0260,  0.0008,  0.0029,  ..., -0.0002,  0.0423,  0.0341]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.self_attn.dense.bias',\n",
       "              tensor([ 0.0040, -0.0107, -0.0611,  ...,  0.0219,  0.0132,  0.0422],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.fc1.weight',\n",
       "              tensor([[-1.2245e-02, -4.5891e-03, -3.7933e-02,  ..., -3.4698e-02,\n",
       "                       -4.0359e-03,  6.0616e-03],\n",
       "                      [ 1.4191e-02, -2.2842e-02, -6.7505e-02,  ...,  4.6570e-02,\n",
       "                       -1.2344e-02, -1.9028e-02],\n",
       "                      [ 4.5853e-03, -1.5140e-05, -3.2043e-02,  ..., -1.2718e-02,\n",
       "                        5.3955e-02, -2.3972e-02],\n",
       "                      ...,\n",
       "                      [ 4.7852e-02, -2.7222e-02,  2.2934e-02,  ...,  6.5613e-02,\n",
       "                        2.0218e-02, -3.4912e-02],\n",
       "                      [-1.7761e-02, -1.1187e-03, -6.7017e-02,  ...,  4.5419e-04,\n",
       "                        3.2101e-03,  2.9465e-02],\n",
       "                      [-1.2579e-03, -1.5297e-02, -3.5492e-02,  ..., -1.7090e-02,\n",
       "                        2.8744e-03,  1.6953e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.fc1.bias',\n",
       "              tensor([-0.0396, -0.0310, -0.0202,  ...,  0.0098, -0.0145,  0.0016],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.fc2.weight',\n",
       "              tensor([[-0.0344, -0.0263, -0.0256,  ...,  0.0019, -0.0301, -0.0206],\n",
       "                      [-0.0206,  0.0622,  0.0086,  ...,  0.0174,  0.0241, -0.0132],\n",
       "                      [-0.0053, -0.0120,  0.0265,  ..., -0.0170,  0.0272, -0.0351],\n",
       "                      ...,\n",
       "                      [-0.0122, -0.0195,  0.0121,  ..., -0.0221,  0.0123,  0.0216],\n",
       "                      [ 0.0317, -0.0690,  0.0114,  ..., -0.0218,  0.0155,  0.0280],\n",
       "                      [ 0.0333,  0.0113,  0.0176,  ...,  0.0385,  0.0424, -0.0040]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.mlp.fc2.bias',\n",
       "              tensor([ 0.0179, -0.0172,  0.0493,  ...,  0.0059, -0.0098, -0.0451],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.input_layernorm.weight',\n",
       "              tensor([0.4797, 0.4946, 0.4717,  ..., 0.4785, 0.4912, 0.5156],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.22.input_layernorm.bias',\n",
       "              tensor([-0.0220, -0.0267,  0.1263,  ..., -0.0115, -0.0215, -0.0168],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0141, -0.0056, -0.0179,  ..., -0.0210,  0.0305, -0.0186],\n",
       "                      [-0.0208, -0.0117, -0.0291,  ...,  0.0210, -0.0024, -0.0074],\n",
       "                      [-0.0594,  0.0066,  0.0129,  ...,  0.0024,  0.0210, -0.0077],\n",
       "                      ...,\n",
       "                      [ 0.0093, -0.0275,  0.0104,  ..., -0.0027, -0.0347,  0.0267],\n",
       "                      [ 0.0285,  0.0052, -0.0058,  ..., -0.0285, -0.0100,  0.0202],\n",
       "                      [-0.0247, -0.0184,  0.0457,  ..., -0.0035,  0.0239,  0.0088]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.dense.weight',\n",
       "              tensor([[ 0.0199,  0.0272,  0.0627,  ..., -0.0058,  0.0566,  0.0018],\n",
       "                      [-0.0469, -0.0083, -0.0174,  ...,  0.0312,  0.0155,  0.0421],\n",
       "                      [-0.0261,  0.0029, -0.0186,  ..., -0.0420, -0.0250,  0.0395],\n",
       "                      ...,\n",
       "                      [-0.0154, -0.0263,  0.0275,  ..., -0.0129,  0.0018,  0.0753],\n",
       "                      [ 0.0216,  0.0003, -0.0151,  ..., -0.0182, -0.0231, -0.0244],\n",
       "                      [ 0.0347,  0.0422,  0.0035,  ...,  0.0211, -0.0305,  0.0337]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.self_attn.dense.bias',\n",
       "              tensor([-0.0219,  0.0543, -0.0291,  ...,  0.0451, -0.0251,  0.0208],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.fc1.weight',\n",
       "              tensor([[-0.0362,  0.0038,  0.0273,  ...,  0.0512, -0.0211,  0.0216],\n",
       "                      [-0.0311, -0.0288, -0.0004,  ...,  0.0305,  0.0045,  0.0490],\n",
       "                      [ 0.0016, -0.0026, -0.0100,  ...,  0.0029,  0.0071, -0.0189],\n",
       "                      ...,\n",
       "                      [ 0.0014,  0.0689,  0.0283,  ...,  0.0086,  0.0147, -0.0010],\n",
       "                      [ 0.0263, -0.0498, -0.0114,  ..., -0.0289,  0.0087,  0.0215],\n",
       "                      [ 0.0226, -0.0038, -0.0625,  ..., -0.0206,  0.0282,  0.0530]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.fc1.bias',\n",
       "              tensor([-0.0220, -0.0258, -0.0254,  ...,  0.0008,  0.0025, -0.0040],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.fc2.weight',\n",
       "              tensor([[ 0.0129,  0.0217,  0.0092,  ...,  0.0099, -0.0199, -0.0193],\n",
       "                      [ 0.0131, -0.0041, -0.0156,  ..., -0.0275, -0.0069,  0.0206],\n",
       "                      [ 0.0189, -0.0210,  0.0159,  ...,  0.0043, -0.0172,  0.0042],\n",
       "                      ...,\n",
       "                      [-0.0274,  0.0555, -0.0119,  ..., -0.0468, -0.0472,  0.0191],\n",
       "                      [-0.0073, -0.0287,  0.0227,  ..., -0.0162, -0.0340, -0.0338],\n",
       "                      [ 0.0242,  0.0430, -0.0079,  ...,  0.0041,  0.0042,  0.0115]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.mlp.fc2.bias',\n",
       "              tensor([-0.0127, -0.0108,  0.0503,  ...,  0.0015, -0.0052, -0.0223],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.input_layernorm.weight',\n",
       "              tensor([0.4897, 0.4961, 0.4919,  ..., 0.4736, 0.4971, 0.5107],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.23.input_layernorm.bias',\n",
       "              tensor([-0.0270, -0.0334,  0.1161,  ..., -0.0165, -0.0232, -0.0206],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.query_key_value.weight',\n",
       "              tensor([[ 2.9724e-02, -8.9798e-03, -9.6464e-04,  ...,  4.5738e-03,\n",
       "                       -3.3203e-02, -3.0746e-02],\n",
       "                      [ 2.1534e-03, -3.9253e-03,  1.0803e-02,  ...,  2.5955e-02,\n",
       "                        2.1454e-02, -2.0447e-02],\n",
       "                      [-1.8295e-02, -1.4999e-02,  3.0289e-02,  ...,  1.6098e-02,\n",
       "                       -4.6204e-02, -5.5015e-05],\n",
       "                      ...,\n",
       "                      [ 1.3756e-02, -1.8204e-02,  1.3931e-02,  ...,  2.6306e-02,\n",
       "                       -1.1818e-02,  1.9501e-02],\n",
       "                      [ 5.4665e-03,  5.2023e-04,  3.7313e-04,  ...,  1.5976e-02,\n",
       "                        1.8280e-02, -4.6997e-03],\n",
       "                      [-1.4671e-02,  8.0872e-04,  7.0095e-04,  ...,  1.1665e-02,\n",
       "                       -5.2567e-03,  4.7264e-03]], dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.dense.weight',\n",
       "              tensor([[-0.0372, -0.0150, -0.0244,  ...,  0.0236,  0.0070,  0.0066],\n",
       "                      [-0.0266,  0.0467,  0.0246,  ..., -0.0226, -0.0163, -0.0169],\n",
       "                      [-0.0602, -0.0039, -0.0182,  ...,  0.0543,  0.0224,  0.0058],\n",
       "                      ...,\n",
       "                      [-0.0457,  0.0402,  0.0213,  ..., -0.0386, -0.0240,  0.0717],\n",
       "                      [-0.0527,  0.0886, -0.0053,  ...,  0.0240, -0.0670, -0.0102],\n",
       "                      [ 0.0216, -0.0192,  0.0765,  ..., -0.0403, -0.0320,  0.0232]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.self_attn.dense.bias',\n",
       "              tensor([-0.0251,  0.0080, -0.0458,  ...,  0.0392,  0.0123, -0.0061],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.fc1.weight',\n",
       "              tensor([[-1.2238e-02, -3.5954e-03, -6.0005e-03,  ..., -3.1525e-02,\n",
       "                        3.6469e-02,  4.9362e-03],\n",
       "                      [-1.5411e-02, -8.2016e-03, -2.1912e-02,  ..., -1.9913e-02,\n",
       "                       -5.8899e-03,  2.4529e-03],\n",
       "                      [-1.2383e-02, -2.0065e-02, -2.7390e-02,  ...,  6.8092e-03,\n",
       "                        2.9297e-02,  2.8551e-05],\n",
       "                      ...,\n",
       "                      [-4.4922e-02,  1.8433e-02, -3.2959e-02,  ...,  1.5240e-03,\n",
       "                        2.3926e-02,  5.6343e-03],\n",
       "                      [ 1.1047e-02,  3.3203e-02,  6.0516e-02,  ..., -1.3535e-02,\n",
       "                        3.5889e-02, -9.2316e-03],\n",
       "                      [-1.3132e-03, -1.1597e-02, -8.1711e-03,  ...,  1.9417e-03,\n",
       "                       -5.9326e-02,  7.6416e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.fc1.bias',\n",
       "              tensor([-0.0183, -0.0136, -0.0282,  ..., -0.0114, -0.0182, -0.0282],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.fc2.weight',\n",
       "              tensor([[-0.0574,  0.0071, -0.0228,  ...,  0.0414, -0.0043, -0.0008],\n",
       "                      [ 0.0381, -0.0108, -0.0459,  ..., -0.0550, -0.0040, -0.0205],\n",
       "                      [-0.0170, -0.0360,  0.0370,  ..., -0.0113,  0.0172,  0.0019],\n",
       "                      ...,\n",
       "                      [-0.0141,  0.0202,  0.0041,  ..., -0.0136, -0.0111, -0.0057],\n",
       "                      [ 0.0182,  0.0116, -0.0278,  ..., -0.0192, -0.0391,  0.0249],\n",
       "                      [-0.0341, -0.0173, -0.0039,  ..., -0.0227,  0.0103,  0.0025]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.mlp.fc2.bias',\n",
       "              tensor([ 0.0174, -0.0074,  0.0462,  ...,  0.0115, -0.0104, -0.0369],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.input_layernorm.weight',\n",
       "              tensor([0.5112, 0.5200, 0.4910,  ..., 0.4983, 0.5273, 0.5435],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.24.input_layernorm.bias',\n",
       "              tensor([-0.0174, -0.0238,  0.1125,  ..., -0.0140, -0.0161, -0.0125],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.query_key_value.weight',\n",
       "              tensor([[-0.0224, -0.0245, -0.0049,  ..., -0.0520, -0.0262,  0.0134],\n",
       "                      [-0.0009, -0.0266, -0.0457,  ...,  0.0034, -0.0083, -0.0084],\n",
       "                      [-0.0432,  0.0228, -0.0264,  ..., -0.0037, -0.0128,  0.0237],\n",
       "                      ...,\n",
       "                      [-0.0173, -0.0278,  0.0081,  ...,  0.0128,  0.0089,  0.0124],\n",
       "                      [-0.0380, -0.0326, -0.0462,  ...,  0.0255, -0.0108,  0.0087],\n",
       "                      [-0.0042,  0.0083, -0.0022,  ...,  0.0299,  0.0229, -0.0037]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.dense.weight',\n",
       "              tensor([[-0.0125,  0.0208, -0.0118,  ..., -0.0144,  0.0014, -0.0889],\n",
       "                      [-0.0085,  0.0327, -0.0061,  ..., -0.0614,  0.0216, -0.0225],\n",
       "                      [ 0.0068,  0.0217,  0.0105,  ..., -0.0043, -0.0305, -0.0137],\n",
       "                      ...,\n",
       "                      [ 0.0007, -0.0063, -0.0183,  ...,  0.0054,  0.0286,  0.0153],\n",
       "                      [ 0.0139, -0.0217,  0.0013,  ..., -0.0692, -0.0239, -0.0330],\n",
       "                      [-0.0233, -0.0043, -0.0421,  ..., -0.0425,  0.0417, -0.0278]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.self_attn.dense.bias',\n",
       "              tensor([-0.0240, -0.0002, -0.0646,  ...,  0.0443, -0.0088, -0.0147],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.fc1.weight',\n",
       "              tensor([[ 0.0229, -0.0006, -0.0050,  ..., -0.0068, -0.0342,  0.0052],\n",
       "                      [-0.0294,  0.0275, -0.0597,  ...,  0.0424, -0.0277, -0.0117],\n",
       "                      [ 0.0457, -0.0248,  0.0370,  ...,  0.0025, -0.0022, -0.0033],\n",
       "                      ...,\n",
       "                      [-0.0112,  0.0274,  0.0479,  ...,  0.0078,  0.0227,  0.0286],\n",
       "                      [-0.0179, -0.0317,  0.0205,  ...,  0.0129, -0.0070,  0.0131],\n",
       "                      [-0.0226,  0.0119, -0.0649,  ..., -0.0272,  0.0121,  0.0055]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.fc1.bias',\n",
       "              tensor([-0.0275, -0.0127,  0.0005,  ..., -0.0219,  0.0028,  0.0043],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.fc2.weight',\n",
       "              tensor([[-0.0096, -0.0693, -0.0083,  ...,  0.0157,  0.0299,  0.0119],\n",
       "                      [-0.0193, -0.0234,  0.0053,  ...,  0.0005, -0.0172,  0.0114],\n",
       "                      [-0.0054,  0.0132, -0.0542,  ...,  0.0244, -0.0486,  0.0179],\n",
       "                      ...,\n",
       "                      [ 0.0522,  0.0112,  0.0360,  ..., -0.0272,  0.0146,  0.0347],\n",
       "                      [-0.0248,  0.0284,  0.0024,  ..., -0.0300,  0.0452,  0.0205],\n",
       "                      [ 0.1090,  0.0334,  0.0259,  ..., -0.0279,  0.0062,  0.0074]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.mlp.fc2.bias',\n",
       "              tensor([ 0.0094,  0.0013,  0.0413,  ..., -0.0048, -0.0298, -0.0243],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.input_layernorm.weight',\n",
       "              tensor([0.5264, 0.5371, 0.5205,  ..., 0.4990, 0.5410, 0.5654],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.25.input_layernorm.bias',\n",
       "              tensor([-0.0242, -0.0344,  0.1235,  ..., -0.0200, -0.0275, -0.0226],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0033,  0.0028, -0.0201,  ...,  0.0177, -0.0074, -0.0077],\n",
       "                      [-0.0141, -0.0109, -0.0184,  ...,  0.0010, -0.0137,  0.0071],\n",
       "                      [-0.0307,  0.0005,  0.0065,  ..., -0.0060,  0.0094, -0.0095],\n",
       "                      ...,\n",
       "                      [ 0.0015, -0.0197, -0.0054,  ...,  0.0033, -0.0242, -0.0287],\n",
       "                      [ 0.0011, -0.0329, -0.0031,  ..., -0.0046,  0.0086, -0.0259],\n",
       "                      [ 0.0233, -0.0078,  0.0012,  ..., -0.0277,  0.0280, -0.0101]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.dense.weight',\n",
       "              tensor([[-0.0294,  0.0087,  0.0349,  ..., -0.0413, -0.0006,  0.0052],\n",
       "                      [-0.0025,  0.0247,  0.0064,  ..., -0.0096,  0.0517,  0.0385],\n",
       "                      [-0.0394,  0.0193, -0.0069,  ..., -0.0216, -0.0247,  0.0427],\n",
       "                      ...,\n",
       "                      [ 0.0505, -0.0189,  0.0159,  ...,  0.0453,  0.0230,  0.0290],\n",
       "                      [-0.0306,  0.0041, -0.0085,  ...,  0.0902,  0.0037, -0.0006],\n",
       "                      [-0.0534, -0.0031, -0.0246,  ..., -0.0318, -0.0355, -0.0262]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.self_attn.dense.bias',\n",
       "              tensor([ 0.0190, -0.0038, -0.0720,  ...,  0.0460,  0.0177, -0.0032],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.fc1.weight',\n",
       "              tensor([[-0.0439,  0.0187,  0.0040,  ..., -0.0144, -0.0550,  0.0727],\n",
       "                      [-0.0172, -0.0117, -0.0600,  ...,  0.0159, -0.0216,  0.0079],\n",
       "                      [-0.0078,  0.0340,  0.0008,  ..., -0.0206,  0.0040, -0.0034],\n",
       "                      ...,\n",
       "                      [-0.0114, -0.0032,  0.0012,  ..., -0.0041,  0.0101,  0.0208],\n",
       "                      [ 0.0673,  0.0258,  0.0024,  ..., -0.0250,  0.0162,  0.0278],\n",
       "                      [ 0.0367,  0.0191, -0.0144,  ..., -0.0600,  0.0282, -0.0334]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.fc1.bias',\n",
       "              tensor([-0.0359, -0.0172,  0.0053,  ..., -0.0074,  0.0099, -0.0054],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.fc2.weight',\n",
       "              tensor([[-0.0206,  0.0336, -0.0087,  ..., -0.0065, -0.0366, -0.0212],\n",
       "                      [ 0.0068,  0.0369, -0.0308,  ...,  0.0854, -0.0167, -0.0438],\n",
       "                      [-0.0242, -0.0398,  0.0168,  ..., -0.0144, -0.0055, -0.0349],\n",
       "                      ...,\n",
       "                      [ 0.0687,  0.0525,  0.0430,  ..., -0.0194,  0.0500,  0.0097],\n",
       "                      [-0.1074, -0.0263, -0.0312,  ..., -0.0276, -0.0166, -0.0486],\n",
       "                      [ 0.0409, -0.0209, -0.0057,  ...,  0.0311, -0.0309,  0.0074]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.mlp.fc2.bias',\n",
       "              tensor([ 0.0044, -0.0035,  0.0583,  ...,  0.0017,  0.0029, -0.0083],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.input_layernorm.weight',\n",
       "              tensor([0.5151, 0.5312, 0.5444,  ..., 0.4846, 0.5444, 0.5640],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.26.input_layernorm.bias',\n",
       "              tensor([-0.0662, -0.0515,  0.1157,  ..., -0.0722, -0.1097, -0.1206],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0081,  0.0405, -0.0012,  ...,  0.0068, -0.0241, -0.0164],\n",
       "                      [-0.0086, -0.0115, -0.0174,  ...,  0.0020,  0.0180,  0.0021],\n",
       "                      [ 0.0178, -0.0009,  0.0262,  ..., -0.0066,  0.0225, -0.0116],\n",
       "                      ...,\n",
       "                      [ 0.0225, -0.0079, -0.0257,  ..., -0.0215,  0.0061,  0.0087],\n",
       "                      [ 0.0034,  0.0038, -0.0153,  ...,  0.0358, -0.0014,  0.0102],\n",
       "                      [-0.0079,  0.0191, -0.0176,  ...,  0.0232, -0.0105,  0.0127]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.dense.weight',\n",
       "              tensor([[-0.0174,  0.0209, -0.0076,  ..., -0.0006, -0.0204,  0.0572],\n",
       "                      [ 0.0375,  0.0564,  0.0197,  ...,  0.0077, -0.0317,  0.0043],\n",
       "                      [ 0.0271, -0.0035, -0.0418,  ..., -0.0314,  0.0275, -0.0333],\n",
       "                      ...,\n",
       "                      [-0.0403,  0.0652, -0.0267,  ...,  0.0824,  0.0233,  0.0285],\n",
       "                      [-0.0214, -0.0095, -0.0492,  ...,  0.0500,  0.0024, -0.0038],\n",
       "                      [ 0.0284,  0.0352, -0.0092,  ..., -0.0461,  0.0393, -0.0235]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.self_attn.dense.bias',\n",
       "              tensor([-0.0036,  0.0023, -0.0674,  ...,  0.0191,  0.0141, -0.0195],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.fc1.weight',\n",
       "              tensor([[ 0.0237, -0.0313, -0.0393,  ..., -0.0371, -0.0153,  0.0124],\n",
       "                      [ 0.0264, -0.0044, -0.0324,  ..., -0.0350,  0.0205, -0.0223],\n",
       "                      [-0.0150,  0.0048, -0.0242,  ..., -0.0291, -0.0655, -0.0117],\n",
       "                      ...,\n",
       "                      [-0.0508, -0.0274, -0.0365,  ...,  0.0026, -0.0149, -0.0053],\n",
       "                      [-0.0067, -0.0092, -0.0266,  ...,  0.0306,  0.0115, -0.0019],\n",
       "                      [ 0.0157, -0.0047,  0.0041,  ..., -0.0131,  0.0357,  0.0224]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.fc1.bias',\n",
       "              tensor([-0.0168, -0.0161, -0.0093,  ..., -0.0259, -0.0116, -0.0055],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.fc2.weight',\n",
       "              tensor([[-0.0063,  0.0208, -0.0186,  ...,  0.0016,  0.0347,  0.0169],\n",
       "                      [ 0.0422,  0.0053,  0.0068,  ..., -0.0114,  0.0187, -0.0687],\n",
       "                      [ 0.0064,  0.0111, -0.0217,  ...,  0.0294, -0.0188, -0.0258],\n",
       "                      ...,\n",
       "                      [ 0.0212, -0.0063,  0.0681,  ...,  0.0296,  0.0477,  0.0293],\n",
       "                      [ 0.0343,  0.0200, -0.0256,  ...,  0.0101, -0.0083,  0.0182],\n",
       "                      [ 0.0146,  0.0125,  0.0288,  ...,  0.0188, -0.0267,  0.0402]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.mlp.fc2.bias',\n",
       "              tensor([ 0.0061,  0.0027,  0.0692,  ...,  0.0158,  0.0001, -0.0463],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.input_layernorm.weight',\n",
       "              tensor([0.5469, 0.5557, 0.5503,  ..., 0.4814, 0.5601, 0.5630],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.27.input_layernorm.bias',\n",
       "              tensor([-0.0294, -0.0356,  0.1141,  ..., -0.0225, -0.0189, -0.0191],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0271, -0.0038,  0.0139,  ...,  0.0137, -0.0142, -0.0062],\n",
       "                      [-0.0104, -0.0213,  0.0140,  ...,  0.0111, -0.0211,  0.0229],\n",
       "                      [ 0.0215,  0.0437,  0.0002,  ..., -0.0103,  0.0085,  0.0094],\n",
       "                      ...,\n",
       "                      [ 0.0036,  0.0087, -0.0002,  ...,  0.0251, -0.0317, -0.0119],\n",
       "                      [ 0.0233, -0.0149,  0.0188,  ..., -0.0020, -0.0172, -0.0130],\n",
       "                      [-0.0170, -0.0169,  0.0073,  ..., -0.0223,  0.0500, -0.0061]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.dense.weight',\n",
       "              tensor([[ 0.0229, -0.0269, -0.0414,  ..., -0.0219, -0.0231,  0.0406],\n",
       "                      [-0.0228,  0.0080,  0.0798,  ..., -0.0345, -0.0020,  0.0324],\n",
       "                      [-0.0259,  0.0228, -0.0011,  ..., -0.0054, -0.0002,  0.0021],\n",
       "                      ...,\n",
       "                      [-0.0309, -0.0162, -0.0152,  ..., -0.0229, -0.0133,  0.1040],\n",
       "                      [ 0.0269, -0.0119,  0.0510,  ..., -0.0264,  0.0013, -0.0301],\n",
       "                      [ 0.0398, -0.0378,  0.0366,  ..., -0.0652, -0.0022, -0.0217]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.self_attn.dense.bias',\n",
       "              tensor([ 1.0078e-02, -6.9916e-05, -9.8938e-02,  ...,  4.0588e-02,\n",
       "                      -1.1673e-02,  1.2074e-03], dtype=torch.float16)),\n",
       "             ('model.layers.28.mlp.fc1.weight',\n",
       "              tensor([[ 0.0569, -0.0088, -0.0117,  ...,  0.0418,  0.0018, -0.0015],\n",
       "                      [-0.0053, -0.0044, -0.0014,  ...,  0.0310, -0.0442,  0.0211],\n",
       "                      [-0.0325,  0.0586,  0.0163,  ...,  0.0119, -0.0260, -0.0126],\n",
       "                      ...,\n",
       "                      [-0.0179, -0.0495,  0.0263,  ..., -0.0391, -0.0456,  0.0195],\n",
       "                      [-0.0138, -0.0241, -0.0115,  ..., -0.0484, -0.0305,  0.0150],\n",
       "                      [-0.0457,  0.0068, -0.0345,  ..., -0.0706, -0.0049,  0.0389]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.mlp.fc1.bias',\n",
       "              tensor([-2.3193e-02, -3.4760e-02, -1.7319e-02,  ..., -1.6663e-02,\n",
       "                      -7.5877e-05, -3.5034e-02], dtype=torch.float16)),\n",
       "             ('model.layers.28.mlp.fc2.weight',\n",
       "              tensor([[ 0.0276, -0.0192, -0.0121,  ...,  0.0163, -0.0174, -0.0525],\n",
       "                      [ 0.0229,  0.0020,  0.0107,  ...,  0.0409, -0.0141,  0.0606],\n",
       "                      [ 0.0161,  0.0048,  0.0069,  ...,  0.0060,  0.0235,  0.0096],\n",
       "                      ...,\n",
       "                      [-0.0687, -0.0227, -0.0115,  ...,  0.0562, -0.0132, -0.0340],\n",
       "                      [ 0.0670,  0.0116, -0.0235,  ...,  0.0423, -0.0404,  0.0076],\n",
       "                      [-0.0131, -0.0136,  0.0299,  ..., -0.0051,  0.0305,  0.0068]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.mlp.fc2.bias',\n",
       "              tensor([ 0.0195, -0.0298,  0.1044,  ...,  0.0280,  0.0282, -0.0365],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.input_layernorm.weight',\n",
       "              tensor([0.5537, 0.5615, 0.5767,  ..., 0.4548, 0.5659, 0.5591],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.28.input_layernorm.bias',\n",
       "              tensor([-0.0189, -0.0367,  0.1530,  ..., -0.0134, -0.0060, -0.0136],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0179, -0.0180, -0.0012,  ..., -0.0002, -0.0213,  0.0111],\n",
       "                      [ 0.0018, -0.0044, -0.0145,  ...,  0.0167,  0.0115,  0.0018],\n",
       "                      [-0.0157, -0.0303, -0.0141,  ..., -0.0127, -0.0146, -0.0160],\n",
       "                      ...,\n",
       "                      [ 0.0272,  0.0254, -0.0186,  ..., -0.0186, -0.0126,  0.0206],\n",
       "                      [-0.0223, -0.0501, -0.0051,  ..., -0.0008, -0.0431, -0.0012],\n",
       "                      [-0.0132,  0.0114, -0.0075,  ...,  0.0371, -0.0141, -0.0167]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.dense.weight',\n",
       "              tensor([[-0.0345, -0.0639, -0.0070,  ..., -0.0090, -0.0172, -0.0261],\n",
       "                      [ 0.0103,  0.0162, -0.0003,  ...,  0.0105,  0.0204,  0.0510],\n",
       "                      [ 0.0063,  0.0378, -0.0050,  ..., -0.0151,  0.0086,  0.0211],\n",
       "                      ...,\n",
       "                      [-0.0038, -0.0643, -0.0189,  ..., -0.0024, -0.0765, -0.0254],\n",
       "                      [-0.0432, -0.0269,  0.0546,  ..., -0.0131, -0.0093,  0.0041],\n",
       "                      [-0.0154, -0.0186, -0.0042,  ..., -0.0007, -0.0173,  0.0192]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.self_attn.dense.bias',\n",
       "              tensor([ 0.0092, -0.0228, -0.0704,  ...,  0.0194, -0.0532, -0.0037],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.mlp.fc1.weight',\n",
       "              tensor([[ 0.0117, -0.0567, -0.0193,  ...,  0.0176,  0.0312,  0.0445],\n",
       "                      [-0.0220, -0.0805,  0.0071,  ...,  0.0081,  0.0282,  0.0308],\n",
       "                      [-0.0024,  0.0326, -0.0049,  ...,  0.0052,  0.0103,  0.0068],\n",
       "                      ...,\n",
       "                      [-0.0552,  0.0044, -0.0049,  ...,  0.0075, -0.0185, -0.0148],\n",
       "                      [ 0.0090,  0.0552, -0.0261,  ..., -0.0155,  0.0136, -0.0097],\n",
       "                      [-0.0442, -0.0176, -0.0124,  ..., -0.0130, -0.0268, -0.0453]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.mlp.fc1.bias',\n",
       "              tensor([-0.0095, -0.0101, -0.0110,  ..., -0.0061,  0.0060, -0.0043],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.mlp.fc2.weight',\n",
       "              tensor([[-0.0089, -0.0049,  0.0100,  ...,  0.0153,  0.0264,  0.0529],\n",
       "                      [-0.0041,  0.0591, -0.0072,  ..., -0.0280, -0.0632,  0.0107],\n",
       "                      [ 0.0269,  0.0070, -0.0302,  ...,  0.0035,  0.0410, -0.0295],\n",
       "                      ...,\n",
       "                      [-0.0202, -0.0877, -0.0290,  ..., -0.0009, -0.0242, -0.0204],\n",
       "                      [ 0.0030,  0.0550,  0.0116,  ...,  0.0231, -0.0568,  0.0173],\n",
       "                      [ 0.0050, -0.0084, -0.0018,  ..., -0.0072, -0.0452, -0.0236]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.mlp.fc2.bias',\n",
       "              tensor([ 0.0322,  0.0006,  0.0816,  ..., -0.0012,  0.0018, -0.0859],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.input_layernorm.weight',\n",
       "              tensor([0.5171, 0.4604, 0.5869,  ..., 1.2354, 0.5566, 0.6953],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.29.input_layernorm.bias',\n",
       "              tensor([-0.5830, -0.7422, -0.3333,  ..., -0.4463, -0.5610, -0.5410],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.query_key_value.weight',\n",
       "              tensor([[-7.3051e-04, -1.1375e-02,  1.8585e-02,  ...,  2.2827e-02,\n",
       "                        1.5526e-02,  3.0853e-02],\n",
       "                      [-1.0948e-02, -3.7789e-04,  2.0142e-02,  ...,  1.6356e-03,\n",
       "                        1.8509e-02, -2.5284e-02],\n",
       "                      [ 4.0344e-02,  3.3356e-02, -1.5076e-02,  ..., -1.4534e-02,\n",
       "                        1.5236e-02,  1.4793e-02],\n",
       "                      ...,\n",
       "                      [-1.3506e-04, -8.0566e-03,  4.1107e-02,  ...,  1.4763e-02,\n",
       "                        1.1360e-02, -4.5288e-02],\n",
       "                      [ 5.6114e-03,  3.1372e-02,  1.2894e-02,  ..., -1.3580e-02,\n",
       "                       -5.2571e-05, -3.2104e-02],\n",
       "                      [ 6.1989e-05,  3.7980e-04, -2.1881e-02,  ...,  7.9575e-03,\n",
       "                        4.8340e-02, -2.3438e-02]], dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.dense.weight',\n",
       "              tensor([[ 0.0211, -0.0038,  0.0162,  ..., -0.0005,  0.0116,  0.0099],\n",
       "                      [-0.0111,  0.0039, -0.0285,  ..., -0.0184, -0.0134, -0.0205],\n",
       "                      [-0.0028,  0.0366,  0.0042,  ...,  0.0055, -0.0125,  0.0254],\n",
       "                      ...,\n",
       "                      [-0.0015, -0.0151,  0.0089,  ...,  0.0034, -0.0687, -0.0463],\n",
       "                      [-0.0146,  0.0082, -0.0142,  ..., -0.0038, -0.0067,  0.0255],\n",
       "                      [ 0.0212, -0.0325, -0.0026,  ...,  0.0070, -0.0133,  0.0099]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.self_attn.dense.bias',\n",
       "              tensor([-0.0007, -0.0060, -0.0281,  ...,  0.0326, -0.0261,  0.0015],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.mlp.fc1.weight',\n",
       "              tensor([[ 0.0652, -0.0258, -0.0278,  ..., -0.0052,  0.0308, -0.0095],\n",
       "                      [-0.0523, -0.0300,  0.0227,  ...,  0.0139,  0.0078,  0.0058],\n",
       "                      [-0.0228, -0.0143, -0.0321,  ..., -0.0046,  0.0115, -0.0121],\n",
       "                      ...,\n",
       "                      [ 0.0350, -0.0071,  0.0371,  ..., -0.0020,  0.0339,  0.0225],\n",
       "                      [-0.0437,  0.0081, -0.0205,  ..., -0.0129,  0.0054,  0.0120],\n",
       "                      [-0.0036, -0.0159, -0.0026,  ..., -0.0211, -0.0083,  0.0128]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.mlp.fc1.bias',\n",
       "              tensor([-0.0041, -0.0025, -0.0058,  ...,  0.0047,  0.0010,  0.0008],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.mlp.fc2.weight',\n",
       "              tensor([[-0.0240,  0.0597,  0.0398,  ..., -0.0717,  0.0172, -0.0242],\n",
       "                      [-0.0149,  0.0197, -0.0657,  ...,  0.0134,  0.0112,  0.0334],\n",
       "                      [ 0.0206,  0.0190, -0.0232,  ..., -0.0177,  0.0155, -0.0641],\n",
       "                      ...,\n",
       "                      [-0.0348, -0.0967, -0.0206,  ...,  0.0283, -0.0143, -0.0281],\n",
       "                      [-0.0490, -0.0558,  0.0017,  ...,  0.0178, -0.0163,  0.0398],\n",
       "                      [-0.0096,  0.0133, -0.0162,  ...,  0.0283, -0.0757, -0.0625]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.mlp.fc2.bias',\n",
       "              tensor([ 0.0250, -0.0088,  0.0503,  ...,  0.0091,  0.0423, -0.0527],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.input_layernorm.weight',\n",
       "              tensor([0.5039, 0.4917, 0.5337,  ..., 1.0439, 0.4871, 0.5776],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.30.input_layernorm.bias',\n",
       "              tensor([-0.1458, -0.1716, -0.0774,  ..., -0.2529, -0.1372, -0.1197],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.query_key_value.weight',\n",
       "              tensor([[ 0.0029,  0.0105,  0.0377,  ..., -0.0192, -0.0079,  0.0239],\n",
       "                      [-0.0172,  0.0072, -0.0041,  ...,  0.0022,  0.0191, -0.0078],\n",
       "                      [ 0.0029,  0.0238,  0.0013,  ...,  0.0022,  0.0024, -0.0060],\n",
       "                      ...,\n",
       "                      [ 0.0137, -0.0068, -0.0133,  ...,  0.0048, -0.0171,  0.0265],\n",
       "                      [-0.0159,  0.0079, -0.0340,  ...,  0.0062,  0.0296,  0.0012],\n",
       "                      [-0.0226,  0.0122,  0.0028,  ..., -0.0075,  0.0167,  0.0171]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.query_key_value.bias',\n",
       "              tensor([0., 0., 0.,  ..., 0., 0., 0.], dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.dense.weight',\n",
       "              tensor([[-0.0097,  0.0008,  0.0088,  ..., -0.0186, -0.0112,  0.0226],\n",
       "                      [-0.0119, -0.0012,  0.0019,  ..., -0.0188, -0.0213, -0.0042],\n",
       "                      [-0.0095, -0.0304,  0.0102,  ..., -0.0149,  0.0051,  0.0159],\n",
       "                      ...,\n",
       "                      [ 0.0251,  0.0119, -0.0174,  ..., -0.0041, -0.0237,  0.0158],\n",
       "                      [ 0.0149,  0.0131,  0.0024,  ..., -0.0024,  0.0196, -0.0037],\n",
       "                      [ 0.0002, -0.0195,  0.0143,  ..., -0.0047, -0.0104,  0.0023]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.self_attn.dense.bias',\n",
       "              tensor([ 0.0032,  0.0005, -0.0264,  ...,  0.0139, -0.0339, -0.0049],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.mlp.fc1.weight',\n",
       "              tensor([[-0.0062,  0.0341, -0.0020,  ..., -0.0110, -0.0137, -0.0158],\n",
       "                      [-0.0196,  0.0705, -0.0504,  ...,  0.0016, -0.0251, -0.0573],\n",
       "                      [-0.0016,  0.0673,  0.0316,  ..., -0.0064, -0.0288, -0.0021],\n",
       "                      ...,\n",
       "                      [ 0.0069,  0.0278, -0.0008,  ..., -0.0421,  0.0684, -0.0375],\n",
       "                      [-0.0017,  0.0251,  0.0377,  ..., -0.0186, -0.0029,  0.0142],\n",
       "                      [ 0.0399, -0.0240, -0.0155,  ..., -0.0457, -0.0343, -0.0368]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.mlp.fc1.bias',\n",
       "              tensor([ 0.0171, -0.0107, -0.0218,  ...,  0.0016,  0.0017,  0.0028],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.mlp.fc2.weight',\n",
       "              tensor([[ 0.0005,  0.0091, -0.0022,  ..., -0.0175,  0.0123,  0.0075],\n",
       "                      [ 0.0366,  0.0070, -0.0120,  ...,  0.0115,  0.0049,  0.0275],\n",
       "                      [-0.0143,  0.0311, -0.0094,  ..., -0.0027, -0.0157, -0.0320],\n",
       "                      ...,\n",
       "                      [ 0.0946, -0.0398, -0.0820,  ...,  0.0572,  0.0152, -0.0062],\n",
       "                      [ 0.0170, -0.0041, -0.0041,  ..., -0.0161,  0.0249, -0.0059],\n",
       "                      [ 0.0809, -0.0588, -0.0051,  ...,  0.0285, -0.0617, -0.0073]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.mlp.fc2.bias',\n",
       "              tensor([ 0.0009, -0.0168,  0.0076,  ..., -0.0131,  0.0164, -0.0091],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.input_layernorm.weight',\n",
       "              tensor([0.4692, 0.4309, 0.4402,  ..., 0.4702, 0.4026, 0.5635],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.layers.31.input_layernorm.bias',\n",
       "              tensor([-0.1868, -0.2676, -0.1121,  ..., -0.2856, -0.0979, -0.2125],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.final_layernorm.weight',\n",
       "              tensor([1.3301, 1.4346, 1.2207,  ..., 0.7881, 1.1143, 1.1895],\n",
       "                     dtype=torch.float16)),\n",
       "             ('model.final_layernorm.bias',\n",
       "              tensor([-0.0117,  0.0499, -0.0923,  ...,  0.1569, -0.0720, -0.0215],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.0.weight',\n",
       "              tensor([[-0.0091,  0.0476, -0.0224,  ...,  0.1141,  0.0269,  0.0955],\n",
       "                      [ 0.0043, -0.0111,  0.0010,  ...,  0.0086,  0.0096, -0.0257],\n",
       "                      [-0.0246, -0.0616, -0.0167,  ..., -0.0148,  0.0091, -0.0036],\n",
       "                      ...,\n",
       "                      [ 0.0154, -0.0112, -0.0068,  ...,  0.0024, -0.0443, -0.0212],\n",
       "                      [ 0.0182,  0.0123,  0.0005,  ..., -0.0179, -0.0129,  0.0033],\n",
       "                      [-0.0524, -0.0018,  0.0344,  ...,  0.0123, -0.0380,  0.0224]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.0.bias',\n",
       "              tensor([ 0.0082, -0.0062, -0.0138,  ..., -0.0065, -0.0008,  0.0225],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.2.weight',\n",
       "              tensor([[ 0.0027,  0.0024, -0.0101,  ..., -0.0001, -0.0033, -0.0022],\n",
       "                      [ 0.0043,  0.0060, -0.0104,  ..., -0.0039, -0.0052, -0.0101]],\n",
       "                     dtype=torch.float16)),\n",
       "             ('score.2.bias',\n",
       "              tensor([ 0.0061, -0.0088], dtype=torch.float16))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_state_dict = torch.load(\"/home/it/environments/Genety/models/phi/2024-05-08_phi.pth\", mmap=True)\n",
    "loaded_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "59f73bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(loaded_state_dict, assign=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "981bf5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, f\"./models/phi/{today}_phi_full.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06a3a7a-de22-4c52-b161-9285ff1475a8",
   "metadata": {},
   "source": [
    "# Hyperparameter tunning with Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4817ec7-7abd-4e00-83b7-1a0fa8fef034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime, date\n",
    "\n",
    "import ray\n",
    "import ray.train.torch\n",
    "\n",
    "from ray import train, tune\n",
    "from ray.train import RunConfig, ScalingConfig, Checkpoint\n",
    "from ray.train.torch import TorchTrainer\n",
    "\n",
    "from ray.tune.schedulers import ASHAScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "05e661f3-d65a-4350-9a2b-fb2b78f74a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-04-08 17:22:06,152\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.train.torch.get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "38fa186d-4971-48b2-8048-ba14511e893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# function passed to the DataLoader to process a batch of data as indicated\n",
    "max_tokens = 50\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Get label and text\n",
    "    y, x = list(zip(*batch))\n",
    "\n",
    "    # Create list with indices from tokeniser\n",
    "    encoded_x = tokenizer(x, padding=True, truncation=True)\n",
    "    encoded_x.input_ids = torch.tensor(encoded_x.input_ids)\n",
    "    encoded_x.attention_mask = torch.tensor(encoded_x.attention_mask)\n",
    "    \n",
    "    return encoded_x, torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0a4e940-d605-4d22-9e47-031524d2d379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(model, optimizer, train_loader, max_norm):  \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.train()\n",
    "    \n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 5\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for idx, (encoded_x, label) in enumerate(train_loader):           \n",
    "        label = label.to(device)\n",
    "        \n",
    "        \n",
    "        encoded_x.input_ids = encoded_x.input_ids.to(device)\n",
    "        encoded_x.attention_mask = encoded_x.attention_mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_ids=encoded_x.input_ids, attention_mask=encoded_x.attention_mask)\n",
    "        predicted_label = outputs.logits\n",
    "        loss = criterion(predicted_label, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        total_loss += loss.item()\n",
    "         \n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "        \n",
    "\n",
    "def eval_func(model, data_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "    \n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (encoded_x, label) in enumerate(data_loader):\n",
    "            label = label.to(device)\n",
    "            \n",
    "            encoded_x.input_ids = encoded_x.input_ids.to(device)\n",
    "            encoded_x.attention_mask = encoded_x.attention_mask.to(device)\n",
    "            \n",
    "            outputs = model(input_ids=encoded_x.input_ids, attention_mask=encoded_x.attention_mask)\n",
    "            predicted_label = outputs.logits\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count, loss.item() / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9f889b5-bad0-47b0-973a-76c9e9085b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "\n",
    "today = date.today().isoformat()\n",
    "model_name = \"roberta_base\"\n",
    "checkpoint_path = f\"./models/{model_name}\"\n",
    "num_class = 2\n",
    "\n",
    "def train_search(config):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # print(device)\n",
    "    # print(config)\n",
    "    # config_params = config[\"params\"]\n",
    "    config_params = config\n",
    "    # print(config_params)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    model = torch.hub.load(\n",
    "        \"huggingface/pytorch-transformers\",\n",
    "        \"modelForSequenceClassification\",\n",
    "        \"roberta-base\"\n",
    "    )\n",
    "    \n",
    "    for i, parameter in enumerate(model.parameters()):\n",
    "        parameter.requires_grad = False\n",
    "    \n",
    "    model.classifier.out_proj = nn.Linear(in_features=768, out_features=num_class)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    \n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        split_train_, batch_size=config_params[\"batch_size\"], shuffle=True, collate_fn=collate_batch\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        split_valid_, batch_size=config_params[\"batch_size\"], shuffle=True, collate_fn=collate_batch\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_dataset, batch_size=config_params[\"batch_size\"], shuffle=True, collate_fn=collate_batch\n",
    "    )\n",
    "    \n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=config_params[\"lr\"])\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=config_params[\"lr\"], momentum=config_params[\"momentum\"])\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=config_params[\"step_size\"], gamma=config_params[\"lr_gamma\"])\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(1, config_params[\"epochs\"] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        accu_train, loss_train = train_func(model, optimizer, train_dataloader, config_params[\"max_norm\"])\n",
    "        accu_val, loss_val = eval_func(model, valid_dataloader)\n",
    "        \n",
    "        # Always let the scheduler take a step because it will be optimized by Hyperopt\n",
    "        scheduler.step()\n",
    "            \n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            checkpoint = None\n",
    "            if epoch % 5 == 0:\n",
    "                # This saves the model to the trial directory\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(temp_checkpoint_dir, \"model.pth\")\n",
    "                )\n",
    "                checkpoint = Checkpoint.from_directory(temp_checkpoint_dir)\n",
    "\n",
    "            # Send the current training result back to Ray Tune\n",
    "            train.report({\n",
    "                \"loss_train\": loss_train,\n",
    "                \"loss_val\": loss_val,\n",
    "                \"accuracy_train\": accu_train,\n",
    "                \"accuracy_val\": accu_val,\n",
    "            }, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e58ba18-0c1e-4b9f-871d-32d26e9572f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "542d7596-2724-494d-b195-5c31d34e6f77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-04-08 17:39:56</td></tr>\n",
       "<tr><td>Running for: </td><td>00:17:50.01        </td></tr>\n",
       "<tr><td>Memory:      </td><td>4.9/30.6 GiB       </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=26<br>Bracket: Iter 64.000: None | Iter 16.000: 0.8130841121495327 | Iter 4.000: 0.6915887850467289 | Iter 1.000: 0.514018691588785<br>Logical resource usage: 8.0/16 CPUs, 0.5/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status    </th><th>loc              </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  epochs</th><th style=\"text-align: right;\">       lr</th><th style=\"text-align: right;\">  lr_gamma</th><th style=\"text-align: right;\">  max_norm</th><th style=\"text-align: right;\">  momentum</th><th style=\"text-align: right;\">  step_size</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  loss_train</th><th style=\"text-align: right;\">   loss_val</th><th style=\"text-align: right;\">  accuracy_train</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_search_95c528eb</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.077966 </td><td style=\"text-align: right;\"> 0.0211905</td><td style=\"text-align: right;\"> 0.825864 </td><td style=\"text-align: right;\"> 0.791408 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        17.9878 </td><td style=\"text-align: right;\">   0.0463574</td><td style=\"text-align: right;\">0.00203249 </td><td style=\"text-align: right;\">        0.539972</td></tr>\n",
       "<tr><td>train_search_9017ddd2</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.238933 </td><td style=\"text-align: right;\"> 0.684672 </td><td style=\"text-align: right;\"> 0.725902 </td><td style=\"text-align: right;\"> 0.527381 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">       251.104  </td><td style=\"text-align: right;\">   0.0177453</td><td style=\"text-align: right;\">0.000966206</td><td style=\"text-align: right;\">        0.709677</td></tr>\n",
       "<tr><td>train_search_f9e7a3a2</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.244985 </td><td style=\"text-align: right;\"> 0.452759 </td><td style=\"text-align: right;\"> 0.806634 </td><td style=\"text-align: right;\"> 0.75996  </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">       242.753  </td><td style=\"text-align: right;\">   0.018428 </td><td style=\"text-align: right;\">0.00110557 </td><td style=\"text-align: right;\">        0.700795</td></tr>\n",
       "<tr><td>train_search_793de5d9</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.145438 </td><td style=\"text-align: right;\"> 0.886026 </td><td style=\"text-align: right;\"> 0.0290485</td><td style=\"text-align: right;\"> 0.609793 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        29.1748 </td><td style=\"text-align: right;\">   0.0214495</td><td style=\"text-align: right;\">0.00127282 </td><td style=\"text-align: right;\">        0.561477</td></tr>\n",
       "<tr><td>train_search_3812ad0f</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">2.07567  </td><td style=\"text-align: right;\"> 0.248548 </td><td style=\"text-align: right;\"> 0.0909424</td><td style=\"text-align: right;\"> 0.424915 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.4176 </td><td style=\"text-align: right;\">   0.0522997</td><td style=\"text-align: right;\">0.000836132</td><td style=\"text-align: right;\">        0.526882</td></tr>\n",
       "<tr><td>train_search_7608a81a</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.224056 </td><td style=\"text-align: right;\"> 0.58139  </td><td style=\"text-align: right;\"> 0.834711 </td><td style=\"text-align: right;\"> 0.222954 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        16.4752 </td><td style=\"text-align: right;\">   0.0488402</td><td style=\"text-align: right;\">0.00134252 </td><td style=\"text-align: right;\">        0.541374</td></tr>\n",
       "<tr><td>train_search_1448e4b1</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.0574658</td><td style=\"text-align: right;\"> 0.384255 </td><td style=\"text-align: right;\"> 0.848441 </td><td style=\"text-align: right;\"> 0.442182 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       147.269  </td><td style=\"text-align: right;\">   0.0384443</td><td style=\"text-align: right;\">0.0012176  </td><td style=\"text-align: right;\">        0.674614</td></tr>\n",
       "<tr><td>train_search_b997c710</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.107936 </td><td style=\"text-align: right;\"> 0.4831   </td><td style=\"text-align: right;\"> 0.23481  </td><td style=\"text-align: right;\"> 0.0739218</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        11.6814 </td><td style=\"text-align: right;\">   0.0439614</td><td style=\"text-align: right;\">0.00130615 </td><td style=\"text-align: right;\">        0.508649</td></tr>\n",
       "<tr><td>train_search_3dd2e38d</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.277214 </td><td style=\"text-align: right;\"> 0.189365 </td><td style=\"text-align: right;\"> 0.457607 </td><td style=\"text-align: right;\"> 0.261112 </td><td style=\"text-align: right;\">          8</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.2719 </td><td style=\"text-align: right;\">   0.0476185</td><td style=\"text-align: right;\">0.00140894 </td><td style=\"text-align: right;\">        0.528752</td></tr>\n",
       "<tr><td>train_search_9825589d</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">1.78789  </td><td style=\"text-align: right;\"> 0.121518 </td><td style=\"text-align: right;\"> 0.499747 </td><td style=\"text-align: right;\"> 0.279879 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         6.90509</td><td style=\"text-align: right;\">   0.0773781</td><td style=\"text-align: right;\">0.00953058 </td><td style=\"text-align: right;\">        0.507714</td></tr>\n",
       "<tr><td>train_search_d54285cb</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.391334 </td><td style=\"text-align: right;\"> 0.448622 </td><td style=\"text-align: right;\"> 0.0229866</td><td style=\"text-align: right;\"> 0.0420599</td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       131.461  </td><td style=\"text-align: right;\">   0.0413611</td><td style=\"text-align: right;\">0.00122005 </td><td style=\"text-align: right;\">        0.638149</td></tr>\n",
       "<tr><td>train_search_931d5035</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">2.20273  </td><td style=\"text-align: right;\"> 0.0229893</td><td style=\"text-align: right;\"> 0.332659 </td><td style=\"text-align: right;\"> 0.100081 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.7413 </td><td style=\"text-align: right;\">   0.050249 </td><td style=\"text-align: right;\">0.00155184 </td><td style=\"text-align: right;\">        0.494624</td></tr>\n",
       "<tr><td>train_search_f4fe7505</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">2.07334  </td><td style=\"text-align: right;\"> 0.248639 </td><td style=\"text-align: right;\"> 0.663452 </td><td style=\"text-align: right;\"> 0.281592 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        13.6404 </td><td style=\"text-align: right;\">   0.187664 </td><td style=\"text-align: right;\">0.0101255  </td><td style=\"text-align: right;\">        0.521739</td></tr>\n",
       "<tr><td>train_search_5e9d4e6c</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.200506 </td><td style=\"text-align: right;\"> 0.254284 </td><td style=\"text-align: right;\"> 0.551236 </td><td style=\"text-align: right;\"> 0.345064 </td><td style=\"text-align: right;\">          9</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        36.7901 </td><td style=\"text-align: right;\">   0.0221209</td><td style=\"text-align: right;\">0.00115557 </td><td style=\"text-align: right;\">        0.564282</td></tr>\n",
       "<tr><td>train_search_4f482769</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.0761317</td><td style=\"text-align: right;\"> 0.380775 </td><td style=\"text-align: right;\"> 0.212084 </td><td style=\"text-align: right;\"> 0.717154 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        35.4922 </td><td style=\"text-align: right;\">   0.0210212</td><td style=\"text-align: right;\">0.00117059 </td><td style=\"text-align: right;\">        0.609163</td></tr>\n",
       "<tr><td>train_search_6a463c4a</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.941604 </td><td style=\"text-align: right;\"> 0.15324  </td><td style=\"text-align: right;\"> 0.619542 </td><td style=\"text-align: right;\"> 0.518237 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.4655 </td><td style=\"text-align: right;\">   0.0499936</td><td style=\"text-align: right;\">0.00533328 </td><td style=\"text-align: right;\">        0.525012</td></tr>\n",
       "<tr><td>train_search_399f0e4d</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.168928 </td><td style=\"text-align: right;\"> 0.527597 </td><td style=\"text-align: right;\"> 0.167131 </td><td style=\"text-align: right;\"> 0.282257 </td><td style=\"text-align: right;\">          3</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        10.3899 </td><td style=\"text-align: right;\">   0.0218255</td><td style=\"text-align: right;\">0.00135855 </td><td style=\"text-align: right;\">        0.536699</td></tr>\n",
       "<tr><td>train_search_5aab29ec</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.550553 </td><td style=\"text-align: right;\"> 0.882113 </td><td style=\"text-align: right;\"> 0.588993 </td><td style=\"text-align: right;\"> 0.507441 </td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       165.927  </td><td style=\"text-align: right;\">   0.0484788</td><td style=\"text-align: right;\">0.00203961 </td><td style=\"text-align: right;\">        0.645629</td></tr>\n",
       "<tr><td>train_search_96cd7c5d</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.309501 </td><td style=\"text-align: right;\"> 0.887438 </td><td style=\"text-align: right;\"> 0.139021 </td><td style=\"text-align: right;\"> 0.807736 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">       286.885  </td><td style=\"text-align: right;\">   0.035235 </td><td style=\"text-align: right;\">0.000589981</td><td style=\"text-align: right;\">        0.723703</td></tr>\n",
       "<tr><td>train_search_b1a6830a</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.405311 </td><td style=\"text-align: right;\"> 0.872813 </td><td style=\"text-align: right;\"> 0.101692 </td><td style=\"text-align: right;\"> 0.364236 </td><td style=\"text-align: right;\">          1</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        44.0999 </td><td style=\"text-align: right;\">   0.0416269</td><td style=\"text-align: right;\">0.00135799 </td><td style=\"text-align: right;\">        0.616176</td></tr>\n",
       "<tr><td>train_search_12a79460</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.70239  </td><td style=\"text-align: right;\"> 0.706379 </td><td style=\"text-align: right;\"> 0.723952 </td><td style=\"text-align: right;\"> 0.659075 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         8.86181</td><td style=\"text-align: right;\">   0.0416047</td><td style=\"text-align: right;\">0.0020057  </td><td style=\"text-align: right;\">        0.508181</td></tr>\n",
       "<tr><td>train_search_b79717e7</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">1.15366  </td><td style=\"text-align: right;\"> 0.706536 </td><td style=\"text-align: right;\"> 0.758395 </td><td style=\"text-align: right;\"> 0.882864 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       109.144  </td><td style=\"text-align: right;\">   0.0236426</td><td style=\"text-align: right;\">0.00140243 </td><td style=\"text-align: right;\">        0.675549</td></tr>\n",
       "<tr><td>train_search_83526b65</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.115968 </td><td style=\"text-align: right;\"> 0.707253 </td><td style=\"text-align: right;\"> 0.899703 </td><td style=\"text-align: right;\"> 0.607019 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        14.4247 </td><td style=\"text-align: right;\">   0.0225935</td><td style=\"text-align: right;\">0.00117435 </td><td style=\"text-align: right;\">        0.534362</td></tr>\n",
       "<tr><td>train_search_54619cd9</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.307381 </td><td style=\"text-align: right;\"> 0.775344 </td><td style=\"text-align: right;\"> 0.381822 </td><td style=\"text-align: right;\"> 0.894636 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        44.2242 </td><td style=\"text-align: right;\">   0.0585746</td><td style=\"text-align: right;\">0.00143013 </td><td style=\"text-align: right;\">        0.610098</td></tr>\n",
       "<tr><td>train_search_b476d64a</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.55438  </td><td style=\"text-align: right;\"> 0.794642 </td><td style=\"text-align: right;\"> 0.372051 </td><td style=\"text-align: right;\"> 0.832016 </td><td style=\"text-align: right;\">          5</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        26.9463 </td><td style=\"text-align: right;\">   0.0295877</td><td style=\"text-align: right;\">0.00162667 </td><td style=\"text-align: right;\">        0.556802</td></tr>\n",
       "<tr><td>train_search_b8413d13</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">1.21615  </td><td style=\"text-align: right;\"> 0.611325 </td><td style=\"text-align: right;\"> 0.317022 </td><td style=\"text-align: right;\"> 0.884775 </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">        34.5665 </td><td style=\"text-align: right;\">   0.0840256</td><td style=\"text-align: right;\">0.00202294 </td><td style=\"text-align: right;\">        0.606826</td></tr>\n",
       "<tr><td>train_search_66046fc0</td><td>TERMINATED</td><td>10.1.1.204:139485</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">1.23468  </td><td style=\"text-align: right;\"> 0.771111 </td><td style=\"text-align: right;\"> 0.726042 </td><td style=\"text-align: right;\"> 0.899698 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">       208.249  </td><td style=\"text-align: right;\">   0.0824354</td><td style=\"text-align: right;\">0.00113586 </td><td style=\"text-align: right;\">        0.672744</td></tr>\n",
       "<tr><td>train_search_0bae8482</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.766116 </td><td style=\"text-align: right;\"> 0.825232 </td><td style=\"text-align: right;\"> 0.279531 </td><td style=\"text-align: right;\"> 0.703699 </td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">       115.348  </td><td style=\"text-align: right;\">   0.0210406</td><td style=\"text-align: right;\">0.000884646</td><td style=\"text-align: right;\">        0.667602</td></tr>\n",
       "<tr><td>train_search_5ef26cf4</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">1.44272  </td><td style=\"text-align: right;\"> 0.639568 </td><td style=\"text-align: right;\"> 0.514073 </td><td style=\"text-align: right;\"> 0.82329  </td><td style=\"text-align: right;\">          2</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        12.5069 </td><td style=\"text-align: right;\">   0.137661 </td><td style=\"text-align: right;\">0.000673803</td><td style=\"text-align: right;\">        0.522207</td></tr>\n",
       "<tr><td>train_search_373fbfc5</td><td>TERMINATED</td><td>10.1.1.204:139395</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      30</td><td style=\"text-align: right;\">0.455163 </td><td style=\"text-align: right;\"> 0.841951 </td><td style=\"text-align: right;\"> 0.427847 </td><td style=\"text-align: right;\"> 0.79644  </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">        15.8291 </td><td style=\"text-align: right;\">   0.0247064</td><td style=\"text-align: right;\">0.001664   </td><td style=\"text-align: right;\">        0.540907</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_9017ddd2_2_batch_size=32,epochs=30,lr=0.2389,lr_gamma=0.6847,max_norm=0.7259,momentum=0.5274,step_size=5_2024-04-08_17-22-08/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_f9e7a3a2_3_batch_size=32,epochs=30,lr=0.2450,lr_gamma=0.4528,max_norm=0.8066,momentum=0.7600,step_size=2_2024-04-08_17-22-10/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_9017ddd2_2_batch_size=32,epochs=30,lr=0.2389,lr_gamma=0.6847,max_norm=0.7259,momentum=0.5274,step_size=5_2024-04-08_17-22-08/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_f9e7a3a2_3_batch_size=32,epochs=30,lr=0.2450,lr_gamma=0.4528,max_norm=0.8066,momentum=0.7600,step_size=2_2024-04-08_17-22-10/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_9017ddd2_2_batch_size=32,epochs=30,lr=0.2389,lr_gamma=0.6847,max_norm=0.7259,momentum=0.5274,step_size=5_2024-04-08_17-22-08/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_f9e7a3a2_3_batch_size=32,epochs=30,lr=0.2450,lr_gamma=0.4528,max_norm=0.8066,momentum=0.7600,step_size=2_2024-04-08_17-22-10/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_9017ddd2_2_batch_size=32,epochs=30,lr=0.2389,lr_gamma=0.6847,max_norm=0.7259,momentum=0.5274,step_size=5_2024-04-08_17-22-08/checkpoint_000003)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_f9e7a3a2_3_batch_size=32,epochs=30,lr=0.2450,lr_gamma=0.4528,max_norm=0.8066,momentum=0.7600,step_size=2_2024-04-08_17-22-10/checkpoint_000003)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_9017ddd2_2_batch_size=32,epochs=30,lr=0.2389,lr_gamma=0.6847,max_norm=0.7259,momentum=0.5274,step_size=5_2024-04-08_17-22-08/checkpoint_000004)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_f9e7a3a2_3_batch_size=32,epochs=30,lr=0.2450,lr_gamma=0.4528,max_norm=0.8066,momentum=0.7600,step_size=2_2024-04-08_17-22-10/checkpoint_000004)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_9017ddd2_2_batch_size=32,epochs=30,lr=0.2389,lr_gamma=0.6847,max_norm=0.7259,momentum=0.5274,step_size=5_2024-04-08_17-22-08/checkpoint_000005)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_f9e7a3a2_3_batch_size=32,epochs=30,lr=0.2450,lr_gamma=0.4528,max_norm=0.8066,momentum=0.7600,step_size=2_2024-04-08_17-22-10/checkpoint_000005)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_1448e4b1_7_batch_size=16,epochs=30,lr=0.0575,lr_gamma=0.3843,max_norm=0.8484,momentum=0.4422,step_size=7_2024-04-08_17-26-46/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_1448e4b1_7_batch_size=16,epochs=30,lr=0.0575,lr_gamma=0.3843,max_norm=0.8484,momentum=0.4422,step_size=7_2024-04-08_17-26-46/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_d54285cb_11_batch_size=16,epochs=30,lr=0.3913,lr_gamma=0.4486,max_norm=0.0230,momentum=0.0421,step_size=9_2024-04-08_17-27-28/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_1448e4b1_7_batch_size=16,epochs=30,lr=0.0575,lr_gamma=0.3843,max_norm=0.8484,momentum=0.4422,step_size=7_2024-04-08_17-26-46/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_d54285cb_11_batch_size=16,epochs=30,lr=0.3913,lr_gamma=0.4486,max_norm=0.0230,momentum=0.0421,step_size=9_2024-04-08_17-27-28/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_d54285cb_11_batch_size=16,epochs=30,lr=0.3913,lr_gamma=0.4486,max_norm=0.0230,momentum=0.0421,step_size=9_2024-04-08_17-27-28/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_5aab29ec_18_batch_size=16,epochs=30,lr=0.5506,lr_gamma=0.8821,max_norm=0.5890,momentum=0.5074,step_size=7_2024-04-08_17-30-22/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_5aab29ec_18_batch_size=16,epochs=30,lr=0.5506,lr_gamma=0.8821,max_norm=0.5890,momentum=0.5074,step_size=7_2024-04-08_17-30-22/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_5aab29ec_18_batch_size=16,epochs=30,lr=0.5506,lr_gamma=0.8821,max_norm=0.5890,momentum=0.5074,step_size=7_2024-04-08_17-30-22/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000003)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000004)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_b79717e7_22_batch_size=32,epochs=30,lr=1.1537,lr_gamma=0.7065,max_norm=0.7584,momentum=0.8829,step_size=2_2024-04-08_17-34-03/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000005)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_b79717e7_22_batch_size=32,epochs=30,lr=1.1537,lr_gamma=0.7065,max_norm=0.7584,momentum=0.8829,step_size=2_2024-04-08_17-34-03/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_b79717e7_22_batch_size=32,epochs=30,lr=1.1537,lr_gamma=0.7065,max_norm=0.7584,momentum=0.8829,step_size=2_2024-04-08_17-34-03/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_66046fc0_27_batch_size=16,epochs=30,lr=1.2347,lr_gamma=0.7711,max_norm=0.7260,momentum=0.8997,step_size=4_2024-04-08_17-36-19/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_0bae8482_28_batch_size=32,epochs=30,lr=0.7661,lr_gamma=0.8252,max_norm=0.2795,momentum=0.7037,step_size=4_2024-04-08_17-36-28/checkpoint_000000)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_0bae8482_28_batch_size=32,epochs=30,lr=0.7661,lr_gamma=0.8252,max_norm=0.2795,momentum=0.7037,step_size=4_2024-04-08_17-36-28/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_66046fc0_27_batch_size=16,epochs=30,lr=1.2347,lr_gamma=0.7711,max_norm=0.7260,momentum=0.8997,step_size=4_2024-04-08_17-36-19/checkpoint_000001)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_0bae8482_28_batch_size=32,epochs=30,lr=0.7661,lr_gamma=0.8252,max_norm=0.2795,momentum=0.7037,step_size=4_2024-04-08_17-36-28/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_66046fc0_27_batch_size=16,epochs=30,lr=1.2347,lr_gamma=0.7711,max_norm=0.7260,momentum=0.8997,step_size=4_2024-04-08_17-36-19/checkpoint_000002)\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "\u001b[36m(train_search pid=139395)\u001b[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_66046fc0_27_batch_size=16,epochs=30,lr=1.2347,lr_gamma=0.7711,max_norm=0.7260,momentum=0.8997,step_size=4_2024-04-08_17-36-19/checkpoint_000003)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_66046fc0_27_batch_size=16,epochs=30,lr=1.2347,lr_gamma=0.7711,max_norm=0.7260,momentum=0.8997,step_size=4_2024-04-08_17-36-19/checkpoint_000004)\n",
      "\u001b[36m(train_search pid=139485)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_66046fc0_27_batch_size=16,epochs=30,lr=1.2347,lr_gamma=0.7711,max_norm=0.7260,momentum=0.8997,step_size=4_2024-04-08_17-36-19/checkpoint_000005)\n",
      "2024-04-08 17:39:56,865\tINFO tune.py:1042 -- Total run time: 1070.12 seconds (1070.01 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from hyperopt import hp\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "exp_name = model_name\n",
    "\n",
    "space = {\n",
    "    \"lr\": hp.loguniform(\"lr\", -3, 1),\n",
    "    \"momentum\": hp.uniform(\"momentum\", 0.01, 0.9),\n",
    "    \"epochs\": hp.choice(\"epochs\", [30]),\n",
    "    \"batch_size\": hp.choice(\"batch_size\", [16, 32]),\n",
    "    \"step_size\": hp.randint(\"step_size\", 1, 10),\n",
    "    \"lr_gamma\": hp.uniform(\"lr_gamma\", 0.01, 0.9),\n",
    "    \"max_norm\": hp.uniform(\"max_norm\", 0.01, 0.9),\n",
    "}\n",
    "\n",
    "hyperopt_search = HyperOptSearch(space, metric=\"accuracy_val\", mode=\"max\")\n",
    "\n",
    "# Uncomment this to enable distributed execution\n",
    "# `ray.init(address=\"auto\")`\n",
    "\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_search, resources={\"cpu\":8, \"gpu\":0.5}),\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=30,\n",
    "        scheduler=ASHAScheduler(metric=\"accuracy_val\", mode=\"max\"), # Early stopping\n",
    "        search_alg=hyperopt_search, # Hyperopt library for Hyper-parameter Optimization\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=exp_name,\n",
    "        checkpoint_config=train.CheckpointConfig(\n",
    "            checkpoint_score_attribute=\"accuracy_val\",\n",
    "            num_to_keep=2,\n",
    "            # checkpoint_at_end=True\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef201770-1da9-4602-b449-d01a99b054ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>,\n",
       " <AxesSubplot:>]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfs = {result.path: result.metrics_dataframe for result in results}\n",
    "[d[\"accuracy_val\"].plot() for d in dfs.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baa263b2-2ced-46db-9d6a-8059280910c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'epochs': 30,\n",
       " 'lr': 0.30950058877069947,\n",
       " 'lr_gamma': 0.8874384348303483,\n",
       " 'max_norm': 0.13902099797968576,\n",
       " 'momentum': 0.8077356837534213,\n",
       " 'step_size': 4}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result = results.get_best_result(\"accuracy_val\", \"max\")\n",
    "best_result.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "797fe3d4-9b44-49df-8626-5110d348ef54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss_train</th>\n",
       "      <th>loss_val</th>\n",
       "      <th>accuracy_train</th>\n",
       "      <th>accuracy_val</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>checkpoint_dir_name</th>\n",
       "      <th>done</th>\n",
       "      <th>training_iteration</th>\n",
       "      <th>trial_id</th>\n",
       "      <th>date</th>\n",
       "      <th>...</th>\n",
       "      <th>time_since_restore</th>\n",
       "      <th>iterations_since_restore</th>\n",
       "      <th>config/batch_size</th>\n",
       "      <th>config/epochs</th>\n",
       "      <th>config/lr</th>\n",
       "      <th>config/lr_gamma</th>\n",
       "      <th>config/max_norm</th>\n",
       "      <th>config/momentum</th>\n",
       "      <th>config/step_size</th>\n",
       "      <th>should_checkpoint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.046683</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.538569</td>\n",
       "      <td>0.502804</td>\n",
       "      <td>1712622638</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-30-38</td>\n",
       "      <td>...</td>\n",
       "      <td>5.293167</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.043864</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.596073</td>\n",
       "      <td>0.570093</td>\n",
       "      <td>1712622648</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-30-48</td>\n",
       "      <td>...</td>\n",
       "      <td>15.655403</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.041782</td>\n",
       "      <td>0.001874</td>\n",
       "      <td>0.623656</td>\n",
       "      <td>0.502804</td>\n",
       "      <td>1712622659</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-30-59</td>\n",
       "      <td>...</td>\n",
       "      <td>25.976075</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.040105</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.639084</td>\n",
       "      <td>0.771963</td>\n",
       "      <td>1712622669</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-31-09</td>\n",
       "      <td>...</td>\n",
       "      <td>36.305968</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.042993</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.624591</td>\n",
       "      <td>0.583178</td>\n",
       "      <td>1712622680</td>\n",
       "      <td>checkpoint_000000</td>\n",
       "      <td>False</td>\n",
       "      <td>5</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-31-20</td>\n",
       "      <td>...</td>\n",
       "      <td>47.360196</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.040554</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.655446</td>\n",
       "      <td>0.704673</td>\n",
       "      <td>1712622688</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>6</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-31-28</td>\n",
       "      <td>...</td>\n",
       "      <td>55.661068</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.039787</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.671809</td>\n",
       "      <td>0.502804</td>\n",
       "      <td>1712622699</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>7</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-31-39</td>\n",
       "      <td>...</td>\n",
       "      <td>66.020274</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.042306</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.652641</td>\n",
       "      <td>0.727103</td>\n",
       "      <td>1712622709</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>8</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-31-49</td>\n",
       "      <td>...</td>\n",
       "      <td>76.308627</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.038799</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.673212</td>\n",
       "      <td>0.629907</td>\n",
       "      <td>1712622719</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>9</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-31-59</td>\n",
       "      <td>...</td>\n",
       "      <td>86.660007</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.039226</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.657317</td>\n",
       "      <td>0.717757</td>\n",
       "      <td>1712622730</td>\n",
       "      <td>checkpoint_000001</td>\n",
       "      <td>False</td>\n",
       "      <td>10</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-32-10</td>\n",
       "      <td>...</td>\n",
       "      <td>97.827054</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.038173</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.678822</td>\n",
       "      <td>0.573832</td>\n",
       "      <td>1712622739</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>11</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-32-19</td>\n",
       "      <td>...</td>\n",
       "      <td>106.087132</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.039373</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.660589</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>1712622749</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>12</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-32-29</td>\n",
       "      <td>...</td>\n",
       "      <td>116.373694</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.038183</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>0.688640</td>\n",
       "      <td>0.687850</td>\n",
       "      <td>1712622759</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-32-39</td>\n",
       "      <td>...</td>\n",
       "      <td>126.661963</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.038604</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.667134</td>\n",
       "      <td>0.514019</td>\n",
       "      <td>1712622770</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>14</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-32-50</td>\n",
       "      <td>...</td>\n",
       "      <td>137.060414</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.038509</td>\n",
       "      <td>0.000851</td>\n",
       "      <td>0.671809</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1712622781</td>\n",
       "      <td>checkpoint_000002</td>\n",
       "      <td>False</td>\n",
       "      <td>15</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-01</td>\n",
       "      <td>...</td>\n",
       "      <td>148.202280</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.037574</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.699860</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>1712622789</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>16</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-09</td>\n",
       "      <td>...</td>\n",
       "      <td>156.369919</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.037871</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.689107</td>\n",
       "      <td>0.792523</td>\n",
       "      <td>1712622799</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>17</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-19</td>\n",
       "      <td>...</td>\n",
       "      <td>166.141110</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.700795</td>\n",
       "      <td>0.571963</td>\n",
       "      <td>1712622802</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>18</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-22</td>\n",
       "      <td>...</td>\n",
       "      <td>168.936548</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.036502</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.718560</td>\n",
       "      <td>0.734579</td>\n",
       "      <td>1712622806</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>19</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-26</td>\n",
       "      <td>...</td>\n",
       "      <td>173.327392</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.036845</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.689575</td>\n",
       "      <td>0.730841</td>\n",
       "      <td>1712622817</td>\n",
       "      <td>checkpoint_000003</td>\n",
       "      <td>False</td>\n",
       "      <td>20</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-37</td>\n",
       "      <td>...</td>\n",
       "      <td>184.375078</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.036211</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>0.837383</td>\n",
       "      <td>1712622828</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>21</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-48</td>\n",
       "      <td>...</td>\n",
       "      <td>194.865065</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.036206</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.703132</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>1712622838</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>22</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-33-58</td>\n",
       "      <td>...</td>\n",
       "      <td>205.127545</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.036753</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.705002</td>\n",
       "      <td>0.760748</td>\n",
       "      <td>1712622844</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-34-04</td>\n",
       "      <td>...</td>\n",
       "      <td>211.533205</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.037238</td>\n",
       "      <td>0.000476</td>\n",
       "      <td>0.703600</td>\n",
       "      <td>0.732710</td>\n",
       "      <td>1712622853</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>24</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-34-13</td>\n",
       "      <td>...</td>\n",
       "      <td>219.791026</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.037459</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.705470</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1712622856</td>\n",
       "      <td>checkpoint_000004</td>\n",
       "      <td>False</td>\n",
       "      <td>25</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-34-16</td>\n",
       "      <td>...</td>\n",
       "      <td>223.383752</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.037102</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.688640</td>\n",
       "      <td>0.682243</td>\n",
       "      <td>1712622869</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>26</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-34-29</td>\n",
       "      <td>...</td>\n",
       "      <td>236.404531</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.036151</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.713885</td>\n",
       "      <td>0.749533</td>\n",
       "      <td>1712622882</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>27</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-34-42</td>\n",
       "      <td>...</td>\n",
       "      <td>249.646624</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.037124</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.705002</td>\n",
       "      <td>0.798131</td>\n",
       "      <td>1712622893</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>28</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-34-53</td>\n",
       "      <td>...</td>\n",
       "      <td>259.972487</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.035581</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.712482</td>\n",
       "      <td>0.719626</td>\n",
       "      <td>1712622906</td>\n",
       "      <td>None</td>\n",
       "      <td>False</td>\n",
       "      <td>29</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-35-06</td>\n",
       "      <td>...</td>\n",
       "      <td>273.116376</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.035235</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.723703</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>1712622919</td>\n",
       "      <td>checkpoint_000005</td>\n",
       "      <td>False</td>\n",
       "      <td>30</td>\n",
       "      <td>96cd7c5d</td>\n",
       "      <td>2024-04-08_17-35-20</td>\n",
       "      <td>...</td>\n",
       "      <td>286.885493</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>0.309501</td>\n",
       "      <td>0.887438</td>\n",
       "      <td>0.139021</td>\n",
       "      <td>0.807736</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    loss_train  loss_val  accuracy_train  accuracy_val   timestamp  \\\n",
       "0     0.046683  0.001157        0.538569      0.502804  1712622638   \n",
       "1     0.043864  0.001106        0.596073      0.570093  1712622648   \n",
       "2     0.041782  0.001874        0.623656      0.502804  1712622659   \n",
       "3     0.040105  0.001218        0.639084      0.771963  1712622669   \n",
       "4     0.042993  0.001235        0.624591      0.583178  1712622680   \n",
       "5     0.040554  0.000806        0.655446      0.704673  1712622688   \n",
       "6     0.039787  0.001721        0.671809      0.502804  1712622699   \n",
       "7     0.042306  0.001034        0.652641      0.727103  1712622709   \n",
       "8     0.038799  0.001186        0.673212      0.629907  1712622719   \n",
       "9     0.039226  0.000968        0.657317      0.717757  1712622730   \n",
       "10    0.038173  0.000955        0.678822      0.573832  1712622739   \n",
       "11    0.039373  0.000937        0.660589      0.747664  1712622749   \n",
       "12    0.038183  0.001282        0.688640      0.687850  1712622759   \n",
       "13    0.038604  0.000842        0.667134      0.514019  1712622770   \n",
       "14    0.038509  0.000851        0.671809      0.800000  1712622781   \n",
       "15    0.037574  0.001124        0.699860      0.822430  1712622789   \n",
       "16    0.037871  0.000844        0.689107      0.792523  1712622799   \n",
       "17    0.037249  0.000945        0.700795      0.571963  1712622802   \n",
       "18    0.036502  0.001044        0.718560      0.734579  1712622806   \n",
       "19    0.036845  0.000847        0.689575      0.730841  1712622817   \n",
       "20    0.036211  0.000671        0.703600      0.837383  1712622828   \n",
       "21    0.036206  0.001002        0.703132      0.803738  1712622838   \n",
       "22    0.036753  0.000823        0.705002      0.760748  1712622844   \n",
       "23    0.037238  0.000476        0.703600      0.732710  1712622853   \n",
       "24    0.037459  0.000699        0.705470      0.800000  1712622856   \n",
       "25    0.037102  0.001003        0.688640      0.682243  1712622869   \n",
       "26    0.036151  0.000966        0.713885      0.749533  1712622882   \n",
       "27    0.037124  0.000865        0.705002      0.798131  1712622893   \n",
       "28    0.035581  0.001032        0.712482      0.719626  1712622906   \n",
       "29    0.035235  0.000590        0.723703      0.822430  1712622919   \n",
       "\n",
       "   checkpoint_dir_name   done  training_iteration  trial_id  \\\n",
       "0                 None  False                   1  96cd7c5d   \n",
       "1                 None  False                   2  96cd7c5d   \n",
       "2                 None  False                   3  96cd7c5d   \n",
       "3                 None  False                   4  96cd7c5d   \n",
       "4    checkpoint_000000  False                   5  96cd7c5d   \n",
       "5                 None  False                   6  96cd7c5d   \n",
       "6                 None  False                   7  96cd7c5d   \n",
       "7                 None  False                   8  96cd7c5d   \n",
       "8                 None  False                   9  96cd7c5d   \n",
       "9    checkpoint_000001  False                  10  96cd7c5d   \n",
       "10                None  False                  11  96cd7c5d   \n",
       "11                None  False                  12  96cd7c5d   \n",
       "12                None  False                  13  96cd7c5d   \n",
       "13                None  False                  14  96cd7c5d   \n",
       "14   checkpoint_000002  False                  15  96cd7c5d   \n",
       "15                None  False                  16  96cd7c5d   \n",
       "16                None  False                  17  96cd7c5d   \n",
       "17                None  False                  18  96cd7c5d   \n",
       "18                None  False                  19  96cd7c5d   \n",
       "19   checkpoint_000003  False                  20  96cd7c5d   \n",
       "20                None  False                  21  96cd7c5d   \n",
       "21                None  False                  22  96cd7c5d   \n",
       "22                None  False                  23  96cd7c5d   \n",
       "23                None  False                  24  96cd7c5d   \n",
       "24   checkpoint_000004  False                  25  96cd7c5d   \n",
       "25                None  False                  26  96cd7c5d   \n",
       "26                None  False                  27  96cd7c5d   \n",
       "27                None  False                  28  96cd7c5d   \n",
       "28                None  False                  29  96cd7c5d   \n",
       "29   checkpoint_000005  False                  30  96cd7c5d   \n",
       "\n",
       "                   date  ...  time_since_restore  iterations_since_restore  \\\n",
       "0   2024-04-08_17-30-38  ...            5.293167                         1   \n",
       "1   2024-04-08_17-30-48  ...           15.655403                         2   \n",
       "2   2024-04-08_17-30-59  ...           25.976075                         3   \n",
       "3   2024-04-08_17-31-09  ...           36.305968                         4   \n",
       "4   2024-04-08_17-31-20  ...           47.360196                         5   \n",
       "5   2024-04-08_17-31-28  ...           55.661068                         6   \n",
       "6   2024-04-08_17-31-39  ...           66.020274                         7   \n",
       "7   2024-04-08_17-31-49  ...           76.308627                         8   \n",
       "8   2024-04-08_17-31-59  ...           86.660007                         9   \n",
       "9   2024-04-08_17-32-10  ...           97.827054                        10   \n",
       "10  2024-04-08_17-32-19  ...          106.087132                        11   \n",
       "11  2024-04-08_17-32-29  ...          116.373694                        12   \n",
       "12  2024-04-08_17-32-39  ...          126.661963                        13   \n",
       "13  2024-04-08_17-32-50  ...          137.060414                        14   \n",
       "14  2024-04-08_17-33-01  ...          148.202280                        15   \n",
       "15  2024-04-08_17-33-09  ...          156.369919                        16   \n",
       "16  2024-04-08_17-33-19  ...          166.141110                        17   \n",
       "17  2024-04-08_17-33-22  ...          168.936548                        18   \n",
       "18  2024-04-08_17-33-26  ...          173.327392                        19   \n",
       "19  2024-04-08_17-33-37  ...          184.375078                        20   \n",
       "20  2024-04-08_17-33-48  ...          194.865065                        21   \n",
       "21  2024-04-08_17-33-58  ...          205.127545                        22   \n",
       "22  2024-04-08_17-34-04  ...          211.533205                        23   \n",
       "23  2024-04-08_17-34-13  ...          219.791026                        24   \n",
       "24  2024-04-08_17-34-16  ...          223.383752                        25   \n",
       "25  2024-04-08_17-34-29  ...          236.404531                        26   \n",
       "26  2024-04-08_17-34-42  ...          249.646624                        27   \n",
       "27  2024-04-08_17-34-53  ...          259.972487                        28   \n",
       "28  2024-04-08_17-35-06  ...          273.116376                        29   \n",
       "29  2024-04-08_17-35-20  ...          286.885493                        30   \n",
       "\n",
       "    config/batch_size config/epochs config/lr  config/lr_gamma  \\\n",
       "0                  16            30  0.309501         0.887438   \n",
       "1                  16            30  0.309501         0.887438   \n",
       "2                  16            30  0.309501         0.887438   \n",
       "3                  16            30  0.309501         0.887438   \n",
       "4                  16            30  0.309501         0.887438   \n",
       "5                  16            30  0.309501         0.887438   \n",
       "6                  16            30  0.309501         0.887438   \n",
       "7                  16            30  0.309501         0.887438   \n",
       "8                  16            30  0.309501         0.887438   \n",
       "9                  16            30  0.309501         0.887438   \n",
       "10                 16            30  0.309501         0.887438   \n",
       "11                 16            30  0.309501         0.887438   \n",
       "12                 16            30  0.309501         0.887438   \n",
       "13                 16            30  0.309501         0.887438   \n",
       "14                 16            30  0.309501         0.887438   \n",
       "15                 16            30  0.309501         0.887438   \n",
       "16                 16            30  0.309501         0.887438   \n",
       "17                 16            30  0.309501         0.887438   \n",
       "18                 16            30  0.309501         0.887438   \n",
       "19                 16            30  0.309501         0.887438   \n",
       "20                 16            30  0.309501         0.887438   \n",
       "21                 16            30  0.309501         0.887438   \n",
       "22                 16            30  0.309501         0.887438   \n",
       "23                 16            30  0.309501         0.887438   \n",
       "24                 16            30  0.309501         0.887438   \n",
       "25                 16            30  0.309501         0.887438   \n",
       "26                 16            30  0.309501         0.887438   \n",
       "27                 16            30  0.309501         0.887438   \n",
       "28                 16            30  0.309501         0.887438   \n",
       "29                 16            30  0.309501         0.887438   \n",
       "\n",
       "    config/max_norm  config/momentum  config/step_size  should_checkpoint  \n",
       "0          0.139021         0.807736                 4                NaN  \n",
       "1          0.139021         0.807736                 4                NaN  \n",
       "2          0.139021         0.807736                 4                NaN  \n",
       "3          0.139021         0.807736                 4                NaN  \n",
       "4          0.139021         0.807736                 4               True  \n",
       "5          0.139021         0.807736                 4                NaN  \n",
       "6          0.139021         0.807736                 4                NaN  \n",
       "7          0.139021         0.807736                 4                NaN  \n",
       "8          0.139021         0.807736                 4                NaN  \n",
       "9          0.139021         0.807736                 4               True  \n",
       "10         0.139021         0.807736                 4                NaN  \n",
       "11         0.139021         0.807736                 4                NaN  \n",
       "12         0.139021         0.807736                 4                NaN  \n",
       "13         0.139021         0.807736                 4                NaN  \n",
       "14         0.139021         0.807736                 4               True  \n",
       "15         0.139021         0.807736                 4                NaN  \n",
       "16         0.139021         0.807736                 4                NaN  \n",
       "17         0.139021         0.807736                 4                NaN  \n",
       "18         0.139021         0.807736                 4                NaN  \n",
       "19         0.139021         0.807736                 4               True  \n",
       "20         0.139021         0.807736                 4                NaN  \n",
       "21         0.139021         0.807736                 4                NaN  \n",
       "22         0.139021         0.807736                 4                NaN  \n",
       "23         0.139021         0.807736                 4                NaN  \n",
       "24         0.139021         0.807736                 4               True  \n",
       "25         0.139021         0.807736                 4                NaN  \n",
       "26         0.139021         0.807736                 4                NaN  \n",
       "27         0.139021         0.807736                 4                NaN  \n",
       "28         0.139021         0.807736                 4                NaN  \n",
       "29         0.139021         0.807736                 4               True  \n",
       "\n",
       "[30 rows x 25 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_result.metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0494c26-dbff-424c-8938-386410d896dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = best_result.metrics_dataframe\n",
    "\n",
    "plt.plot(df['accuracy_train'], label='accuracy_train')\n",
    "plt.plot(df['accuracy_val'], label='accuracy_val')\n",
    "\n",
    "plt.legend(title='')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b38c7602-c38f-48b6-b6a0-b77f984de7fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000004),\n",
       "  {'loss_train': 0.037459485870257,\n",
       "   'loss_val': 0.0006986077143767169,\n",
       "   'accuracy_train': 0.7054698457223001,\n",
       "   'accuracy_val': 0.8,\n",
       "   'timestamp': 1712622856,\n",
       "   'checkpoint_dir_name': 'checkpoint_000004',\n",
       "   'should_checkpoint': True,\n",
       "   'done': False,\n",
       "   'training_iteration': 25,\n",
       "   'trial_id': '96cd7c5d',\n",
       "   'date': '2024-04-08_17-34-16',\n",
       "   'time_this_iter_s': 3.592725992202759,\n",
       "   'time_total_s': 223.38375163078308,\n",
       "   'pid': 139395,\n",
       "   'hostname': 'coeus',\n",
       "   'node_ip': '10.1.1.204',\n",
       "   'config': {'batch_size': 16,\n",
       "    'epochs': 30,\n",
       "    'lr': 0.30950058877069947,\n",
       "    'lr_gamma': 0.8874384348303483,\n",
       "    'max_norm': 0.13902099797968576,\n",
       "    'momentum': 0.8077356837534213,\n",
       "    'step_size': 4},\n",
       "   'time_since_restore': 223.38375163078308,\n",
       "   'iterations_since_restore': 25}),\n",
       " (Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000005),\n",
       "  {'loss_train': 0.03523503265519073,\n",
       "   'loss_val': 0.0005899809231267911,\n",
       "   'accuracy_train': 0.7237026647966339,\n",
       "   'accuracy_val': 0.822429906542056,\n",
       "   'timestamp': 1712622919,\n",
       "   'checkpoint_dir_name': 'checkpoint_000005',\n",
       "   'should_checkpoint': True,\n",
       "   'done': False,\n",
       "   'training_iteration': 30,\n",
       "   'trial_id': '96cd7c5d',\n",
       "   'date': '2024-04-08_17-35-20',\n",
       "   'time_this_iter_s': 13.769116401672363,\n",
       "   'time_total_s': 286.8854925632477,\n",
       "   'pid': 139395,\n",
       "   'hostname': 'coeus',\n",
       "   'node_ip': '10.1.1.204',\n",
       "   'config': {'batch_size': 16,\n",
       "    'epochs': 30,\n",
       "    'lr': 0.30950058877069947,\n",
       "    'lr_gamma': 0.8874384348303483,\n",
       "    'max_norm': 0.13902099797968576,\n",
       "    'momentum': 0.8077356837534213,\n",
       "    'step_size': 4},\n",
       "   'time_since_restore': 286.8854925632477,\n",
       "   'iterations_since_restore': 30})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoints = best_result.best_checkpoints\n",
    "best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af3301b9-a9f0-4407-8f9e-4e69233c3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = best_result.get_best_checkpoint(\"accuracy_val\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57588117-ea63-4fef-a9f5-27957275e80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Checkpoint(filesystem=local, path=/home/it/ray_results/roberta_base/train_search_96cd7c5d_19_batch_size=16,epochs=30,lr=0.3095,lr_gamma=0.8874,max_norm=0.1390,momentum=0.8077,step_size=4_2024-04-08_17-30-33/checkpoint_000005),\n",
       " {'loss_train': 0.03523503265519073,\n",
       "  'loss_val': 0.0005899809231267911,\n",
       "  'accuracy_train': 0.7237026647966339,\n",
       "  'accuracy_val': 0.822429906542056,\n",
       "  'timestamp': 1712622919,\n",
       "  'checkpoint_dir_name': 'checkpoint_000005',\n",
       "  'should_checkpoint': True,\n",
       "  'done': False,\n",
       "  'training_iteration': 30,\n",
       "  'trial_id': '96cd7c5d',\n",
       "  'date': '2024-04-08_17-35-20',\n",
       "  'time_this_iter_s': 13.769116401672363,\n",
       "  'time_total_s': 286.8854925632477,\n",
       "  'pid': 139395,\n",
       "  'hostname': 'coeus',\n",
       "  'node_ip': '10.1.1.204',\n",
       "  'config': {'batch_size': 16,\n",
       "   'epochs': 30,\n",
       "   'lr': 0.30950058877069947,\n",
       "   'lr_gamma': 0.8874384348303483,\n",
       "   'max_norm': 0.13902099797968576,\n",
       "   'momentum': 0.8077356837534213,\n",
       "   'step_size': 4},\n",
       "  'time_since_restore': 286.8854925632477,\n",
       "  'iterations_since_restore': 30})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(x for x in best_checkpoints if x[0].path == best_checkpoint.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0e33b354-a0e9-4763-a48e-0b9c114ddae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/it/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load(os.path.join(best_checkpoint.path, \"model.pth\"))\n",
    "\n",
    "num_class = 2\n",
    "model = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\",\n",
    "    \"modelForSequenceClassification\",\n",
    "    \"roberta-base\"\n",
    ")\n",
    "model.classifier.out_proj = nn.Linear(in_features=768, out_features=num_class)\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bad227ff-333b-4914-b5ca-4ca3f98a3f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "uq_path = today + \"_\" + \"_\".join(best_result.path.split(\"=\")[0].split(\"/\")[-2:])\n",
    "save_path = os.path.join(f'./models/{model_name}', uq_path + \"_model.pt\")\n",
    "torch.save(model, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8a91a-5457-41a9-8581-5c89f9582b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Genety (conda)",
   "language": "python",
   "name": "genety-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
